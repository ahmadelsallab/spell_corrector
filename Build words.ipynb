{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "import tarfile\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        sents = row.split(delimiter)\n",
    "        if (len(sents) < 2):\n",
    "            continue\n",
    "        input_text = sents[prediction_index]\n",
    "        gt_texts.append(sents[gt_index])\n",
    "    return input_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_terms(json_file):\n",
    "    texts = []\n",
    "    with open(json_file) as f:\n",
    "        med_terms_dict = json.load(f)\n",
    "    texts += list(med_terms_dict.keys())\n",
    "    texts += list(med_terms_dict.values())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accidents_terms(file_name):\n",
    "\n",
    "    f = open(file_name, encoding='utf8')\n",
    "    line = 0  \n",
    "    texts = []\n",
    "    try:\n",
    "        for r in f:\n",
    "            for term in r.split('|'):\n",
    "                    texts += term.replace('\\\"', '')\n",
    "    except:\n",
    "        print('finished')\n",
    "\n",
    "                \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\窶能\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\笳十\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\窶能\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\笳十\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []        \n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../dat/'\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_22.txt', 'field_class_23.txt', 'field_class_24.txt', 'field_class_25.txt', 'field_class_26.txt', 'field_class_27.txt', 'field_class_28.txt', 'field_class_29.txt', 'field_class_30.txt', 'field_class_31.txt', 'field_class_32.txt', 'field_class_33.txt', 'field_class_34.txt', 'NL-14622714.txt', 'NL-14627449.txt', 'NL-14628986.txt', 'NL-14631911.txt', 'NL-14640007.txt']\n",
    "files_list = ['field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt']\n",
    "\n",
    "for file_name in files_list:\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    _, gt = load_data_with_gt(tess_correction_data)\n",
    "    texts += gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HW terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hw_correction_data = os.path.join(data_path, 'handwritten_output.txt')\n",
    "_, gt = load_data_with_gt(hw_correction_data, delimiter='|', gt_index=0, prediction_index=1)\n",
    "texts += gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load clean claims forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "file_name = os.path.join(data_path, 'claims.txt')\n",
    "#texts += load_raw_data(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Medical Terms dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = os.path.join(data_path, 'abbrevs.json')\n",
    "texts += load_medical_terms(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Medical Instruction dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path, 'medical_instructions.txt')\n",
    "texts += load_raw_data(file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load accident terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = os.path.join(data_path, 'AccidentsL.txt')\n",
    "texts += load_accidents_terms(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load procedures and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path, 'procedures_tests.txt')\n",
    "texts += load_raw_data(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1081609\n",
      "Claim Folder Contents\n",
      " \n",
      "\n",
      "Claimant Name:\n",
      " \n",
      "\n",
      "Claim Number:\n",
      " \n",
      "\n",
      "Unauthorized access is strictly probihited\n",
      " \n",
      "\n",
      "Print Date:\n",
      " \n",
      "\n",
      "Claim Type: VB Accident - Accident Injury\n",
      " \n",
      "\n",
      "Who The Reported Event Happened To: Employee/Policyholder\n",
      " \n",
      "\n",
      "Policyholder/Owner Information\n",
      " \n",
      "\n",
      "First Name:\n",
      " \n",
      "\n",
      "Middle Name/Initial:\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(texts))\n",
    "for i in range(10):\n",
    "    print(texts[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\ntexts += 'Gender\\n'\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "texts += 'Gender\\n'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('med.txt', 'w') as f:\n",
    "    for text in texts:\n",
    "        f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words/\r\n",
      "words/en_US_GB_CA_mixed.txt\r\n",
      "words/big_orig.txt\r\n",
      "words/._big.txt\r\n",
      "words/big.txt\r\n",
      "words/en_US_GB_CA_lower.txt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf autocorrect/words.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_big_orig = open('words/big_orig.txt', 'r')\n",
    "f_big_orig = open('med.txt', 'r')\n",
    "f_med = open('med.txt', 'r')\n",
    "f_big = open('words/big.txt', 'w')\n",
    "for line in f_big_orig:\n",
    "    f_big.write(line + '\\n')\n",
    "for line in f_med:\n",
    "    f_big.write(line + '\\n')\n",
    "    \n",
    "\n",
    "f_big_orig.close()\n",
    "f_big.close()\n",
    "f_med.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words/\r\n",
      "words/en_US_GB_CA_mixed.txt\r\n",
      "words/big_orig.txt\r\n",
      "words/._big.txt\r\n",
      "words/big.txt\r\n",
      "words/en_US_GB_CA_lower.txt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf autocorrect/words.tar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf words/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
