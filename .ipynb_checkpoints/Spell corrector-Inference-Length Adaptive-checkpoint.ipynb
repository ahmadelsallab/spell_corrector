{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, num_samples, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []   \n",
    "    \n",
    "    #for row in open(file_name, encoding='utf8'):\n",
    "    for row in open(file_name):\n",
    "        if cnt < num_samples :            \n",
    "            input_text = row           \n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len:\n",
    "                cnt += 1                \n",
    "                input_texts.append(input_text)\n",
    "    return input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    \n",
    "    if(len(input_texts) > max_encoder_seq_length):\n",
    "        input_texts = input_texts[:max_encoder_seq_length]\n",
    "    \n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        for t, char in enumerate(input_text[:max_encoder_seq_length]):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "                \n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, max_decoder_seq_length, vocab_to_int, int_to_vocab):\n",
    "    \n",
    "    #print(max_decoder_seq_length)\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#']\n",
    "    #special_chars = []\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        \n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            #print('End', sampled_char, 'Len ', len(decoded_sentence), 'Max len ', max_decoder_seq_length)\n",
    "            sampled_char = ''\n",
    "        \n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        \n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "        #decoded_sentence += sampled_char\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    \n",
    "    # Word level spell correct\n",
    "    '''\n",
    "    corrected_decoded_sentence = ''\n",
    "    for w in decoded_sentence.split(' '):\n",
    "        corrected_decoded_sentence += spell(w) + ' '\n",
    "    decoded_sentence = corrected_decoded_sentence\n",
    "    '''\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_spell_correct(decoded_sentence):\n",
    "    if(decoded_sentence == ''):\n",
    "        return ''\n",
    "    corrected_decoded_sentence = ''\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#', '☒', '■', '☐', '□', '☑', '@']\n",
    "    for w in decoded_sentence.split(' '):\n",
    "        #print(w)\n",
    "        if((len(re.findall(r'\\d+', w))==0) and not (w in special_chars)):\n",
    "            corrected_decoded_sentence += spell(w) + ' '\n",
    "        else:\n",
    "            corrected_decoded_sentence += w + ' '\n",
    "    return corrected_decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence, vocab):\n",
    "    s = ''\n",
    "    prev_char = ''\n",
    "    for c in sentence.strip():\n",
    "        if c not in vocab or (c == ' ' and prev_char == ' '):\n",
    "            s += ''\n",
    "        else:\n",
    "            s += c\n",
    "        prev_char = c\n",
    "            \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_lengths = [50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = {}\n",
    "model_file = {}\n",
    "encoder_model_file = {}\n",
    "decoder_model_file = {}\n",
    "model = {}\n",
    "encoder_model = {}\n",
    "decoder_model = {}\n",
    "vocab = {}\n",
    "vocab_to_int = {}\n",
    "int_to_vocab = {}\n",
    "max_sent_len = {}\n",
    "min_sent_len = {}\n",
    "num_decoder_tokens = {}\n",
    "num_encoder_tokens = {}\n",
    "max_encoder_seq_length = {}\n",
    "max_decoder_seq_length = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in max_sent_lengths:\n",
    "    vocab_file[i] = 'vocab-{}.npz'.format(i)\n",
    "    model_file[i] = 'best_model-{}.hdf5'.format(i)\n",
    "    encoder_model_file[i] = 'encoder_model-{}.hdf5'.format(i)\n",
    "    decoder_model_file[i] = 'decoder_model-{}.hdf5'.format(i)\n",
    "    \n",
    "    vocab = np.load(file=vocab_file[i])\n",
    "    vocab_to_int[i] = vocab['vocab_to_int'].item()\n",
    "    int_to_vocab[i] = vocab['int_to_vocab'].item()\n",
    "    max_sent_len[i] = vocab['max_sent_len']\n",
    "    min_sent_len[i] = vocab['min_sent_len']\n",
    "    input_characters = sorted(list(vocab_to_int))\n",
    "    num_decoder_tokens[i] = num_encoder_tokens[i] = len(input_characters) #int(encoder_model.layers[0].input.shape[2])\n",
    "    max_encoder_seq_length[i] = max_decoder_seq_length[i] = max_sent_len[i] - 1#max([len(txt) for txt in input_texts])\n",
    "    \n",
    "    model[i] = load_model(model_file[i])\n",
    "    encoder_model[i] = load_model(encoder_model_file[i])\n",
    "    decoder_model[i] = load_model(decoder_model_file[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "#tess_correction_data = os.path.join(data_path, 'test_data.txt')\n",
    "#input_texts = load_data(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "OCR_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "#input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)\n",
    "input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len=10000, min_sent_len=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1951\n",
      "Me dieal Provider Roles: Treating  \n",
      " \tMedical Provider Roles: Treating\n",
      "\n",
      "\n",
      "Provider First Name: Christine  \n",
      " \tProvider First Name: Christine\n",
      "\n",
      "\n",
      "Provider Last Name: Nolen, MD  \n",
      " \tProvider Last Name: Nolen, MD\n",
      "\n",
      "\n",
      "Address Line 1 : 7 25 American Avenue  \n",
      " \tAddress Line 1 : 725 American Avenue\n",
      "\n",
      "\n",
      "City. W’aukesha  \n",
      " \tCity: Waukesha\n",
      "\n",
      "\n",
      "StatefProvinee: ‘WI  \n",
      " \tState/Province: WI\n",
      "\n",
      "\n",
      "Postal Code: 5 31 88  \n",
      " \tPostal Code: 53188\n",
      "\n",
      "\n",
      "Country\". US  \n",
      " \tCountry:  US\n",
      "\n",
      "\n",
      "Business Telephone: (2 62) 92 8- 1000  \n",
      " \tBusiness Telephone: (262) 928- 1000\n",
      "\n",
      "\n",
      "Date ot‘Pirst Visit: 1 2/01f20 17  \n",
      " \tDate of First Visit: 12/01/2017\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_texts_ = []\\nfor sent in input_texts:\\n    sent_ = ''\\n    for word in sent.split(' '):\\n        sent_ += spell(word) + ' '\\n    input_texts_.append(sent_)\\ninput_texts = input_texts_\\ninput_texts_ = []\\n# Sample data\\nprint(len(input_texts))\\nfor i in range(10):\\n    print(input_texts[i], '\\n', target_texts[i])\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spell correct before inference\n",
    "'''\n",
    "input_texts_ = []\n",
    "for sent in input_texts:\n",
    "    sent_ = ''\n",
    "    for word in sent.split(' '):\n",
    "        sent_ += spell(word) + ' '\n",
    "    input_texts_.append(sent_)\n",
    "input_texts = input_texts_\n",
    "input_texts_ = []\n",
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Lenght =  50\n",
      "Input sentence: Me dieal Provider Roles: Treating\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Char Decoded sentence: Medical Provider Roles:Treating\n",
      "Word Decoded sentence: Me deal Provider Roles Treating \n",
      "-Lenght =  50\n",
      "Input sentence: Provider First Name: Christine\n",
      "GT sentence: Provider First Name: Christine\n",
      "Char Decoded sentence: Provider First Name: Christine\n",
      "Word Decoded sentence: Provider First Name Christine \n",
      "-Lenght =  50\n",
      "Input sentence: Provider Last Name: Nolen, MD\n",
      "GT sentence: Provider Last Name: Nolen, MD\n",
      "Char Decoded sentence: Provider Last Name: Norle, MD\n",
      "Word Decoded sentence: Provider Last Name Dolens MD \n",
      "-Lenght =  50\n",
      "Input sentence: Address Line 1 : 7 25 American Avenue\n",
      "GT sentence: Address Line 1 : 725 American Avenue\n",
      "Char Decoded sentence: Address Line 1:7A25ent Admentine Ave\n",
      "Word Decoded sentence: Address Line 1 : 7 25 American Avenue \n",
      "-Lenght =  50\n",
      "Input sentence: City. W’aukesha\n",
      "GT sentence: City: Waukesha\n",
      "Char Decoded sentence: City.States\n",
      "Word Decoded sentence: City Waukesha \n",
      "-Lenght =  50\n",
      "Input sentence: StatefProvinee: ‘WI\n",
      "GT sentence: State/Province: WI\n",
      "Char Decoded sentence: StateProvine:En\n",
      "Word Decoded sentence: StatefProvinee: DWI \n",
      "-Lenght =  50\n",
      "Input sentence: Postal Code: 5 31 88\n",
      "GT sentence: Postal Code: 53188\n",
      "Char Decoded sentence: Postal Code: 5 3188\n",
      "Word Decoded sentence: Postal Codes 5 31 88 \n",
      "-Lenght =  50\n",
      "Input sentence: Country\". US\n",
      "GT sentence: Country:  US\n",
      "Char Decoded sentence: Country\".S\n",
      "Word Decoded sentence: Country US \n",
      "-Lenght =  50\n",
      "Input sentence: Business Telephone: (2 62) 92 8- 1000\n",
      "GT sentence: Business Telephone: (262) 928- 1000\n",
      "Char Decoded sentence: Business Telephone: (262)928-1000\n",
      "Word Decoded sentence: Business Telephone (2 62) 92 8- 1000 \n",
      "-Lenght =  50\n",
      "Input sentence: Date ot‘Pirst Visit: 1 2/01f20 17\n",
      "GT sentence: Date of First Visit: 12/01/2017\n",
      "Char Decoded sentence: Date to Pisit Visit: 12/012017q\n",
      "Word Decoded sentence: Date ot‘Pirst Visit 1 2/01f20 17 \n",
      "-Lenght =  50\n",
      "Input sentence: Medical Protitler Information — Hospitalization\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Char Decoded sentence: Medical Provider Information —Pospitaliz Dation\n",
      "Word Decoded sentence: Medical Profiteer Information — Hospitalization \n",
      "-Lenght =  50\n",
      "Input sentence: Hospital Name: W'aukesha Memorial Hospital\n",
      "GT sentence: Hospital Name: Waukesha Memorial Hospital\n",
      "Char Decoded sentence: ospital Name: Faumesta Memprisal orstall m\n",
      "Word Decoded sentence: Hospital Name Waukesha Memorial Hospital \n",
      "-Lenght =  50\n",
      "Input sentence: Address Line 1 : 7\" 25 Arnerie an Drive\n",
      "GT sentence: Address Line 1 : 725 American Drive\n",
      "Char Decoded sentence: Address Line 1:7\" 25 Araine an Drive\n",
      "Word Decoded sentence: Address Line 1 : 7\" 25 Arteries an Drive \n",
      "-Lenght =  50\n",
      "Input sentence: City. ‘Waukesha\n",
      "GT sentence: City: Waukesha\n",
      "Char Decoded sentence: City.Stauk ualse\n",
      "Word Decoded sentence: City Waukesha \n",
      "-Lenght =  50\n",
      "Input sentence: StatefProﬁnoe: W'I\n",
      "GT sentence: State/Province: WI\n",
      "Char Decoded sentence: StateProvinc:\n",
      "Word Decoded sentence: StatefProﬁnoe: Wei \n",
      "-Lenght =  50\n",
      "Input sentence: Postal Code: 5 31 88\n",
      "GT sentence: Postal Code: 53188\n",
      "Char Decoded sentence: Postal Code: 5 3188\n",
      "Word Decoded sentence: Postal Codes 5 31 88 \n",
      "-Lenght =  50\n",
      "Input sentence: Country. US\n",
      "GT sentence: Country: US\n",
      "Char Decoded sentence: Country.UuS\n",
      "Word Decoded sentence: Country US \n",
      "-Lenght =  50\n",
      "Input sentence: Claim Type: VB Accident - Accidental Injury\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Char Decoded sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Word Decoded sentence: Claim Type VB Accident - Accidental Injury \n",
      "-Lenght =  100\n",
      "Input sentence: Who The Reporled ETEIII Happened To: ErrployeefPolicyholders Child\n",
      "GT sentence: Who The Reported Event Happened To: Employee/Policyholder's child\n",
      "Char Decoded sentence: Who The Reported ETTHEIIT Happened :o EmployeePolicyholder Child\n",
      "Word Decoded sentence: Who The Reported ETEIII Happened To ErrployeefPolicyholders Child \n",
      "-Lenght =  50\n",
      "Input sentence: Policyhold El':\"0“1l€l' In form ariorl\n",
      "GT sentence: Policyholder/Owner Information\n",
      "Char Decoded sentence: Policyholder :\"0s1ring Information\n",
      "Word Decoded sentence: Policyholder El':\"0“1l€l' In form prior \n",
      "-Lenght =  50\n",
      "Input sentence: First Name:\n",
      "GT sentence: First Name:\n",
      "Char Decoded sentence: First Name:\n",
      "Word Decoded sentence: First Name \n",
      "-Lenght =  50\n",
      "Input sentence: Middle Narmflnitial:\n",
      "GT sentence: Middle Name/Initial:\n",
      "Char Decoded sentence: Middle NameInitial:\n",
      "Word Decoded sentence: Middle Narmflnitial: \n",
      "-Lenght =  50\n",
      "Input sentence: Last Name:\n",
      "GT sentence: Last Name:\n",
      "Char Decoded sentence: Last Name:\n",
      "Word Decoded sentence: Last Name \n",
      "-Lenght =  50\n",
      "Input sentence: Social 8 ecurity Number:\n",
      "GT sentence: Social Security Number:\n",
      "Char Decoded sentence: Social 8 ecurity Numbe:\n",
      "Word Decoded sentence: Social 8 security Number \n",
      "-Lenght =  50\n",
      "Input sentence: Birth Date:\n",
      "GT sentence: Birth Date:\n",
      "Char Decoded sentence: Birth Date:\n",
      "Word Decoded sentence: Birth Date \n",
      "-Lenght =  50\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "Char Decoded sentence: Gender:\n",
      "Word Decoded sentence: Gender \n",
      "-Lenght =  50\n",
      "Input sentence: Language Preference:\n",
      "GT sentence: Language Preference:\n",
      "Char Decoded sentence: Language Preference:\n",
      "Word Decoded sentence: Language Preference \n",
      "-Lenght =  50\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Char Decoded sentence: Address Line 1:\n",
      "Word Decoded sentence: Address Line 1: \n",
      "-Lenght =  50\n",
      "Input sentence: CW-\n",
      "GT sentence: City:\n",
      "Char Decoded sentence: Co-CA\n",
      "Word Decoded sentence: CWC \n",
      "-Lenght =  50\n",
      "Input sentence: StatefProvince :\n",
      "GT sentence: State/Province:\n",
      "Char Decoded sentence: StateProvince:\n",
      "Word Decoded sentence: StatefProvince : \n",
      "-Lenght =  50\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "Char Decoded sentence: Postal Code:\n",
      "Word Decoded sentence: Postal Codes \n",
      "-Lenght =  50\n",
      "Input sentence: Country\n",
      "GT sentence: Country:\n",
      "Char Decoded sentence: Country\n",
      "Word Decoded sentence: Country \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ec4367d43179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(max_decoder_seq_length[len_range])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#print(max_decoder_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mcorrected_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_spell_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-Lenght = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ba4192e10587>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq, encoder_model, decoder_model, num_decoder_tokens, max_decoder_seq_length, vocab_to_int, int_to_vocab)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(target_seq)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         output_tokens, attention, h, c  = decoder_model.predict(\n\u001b[0;32m---> 24\u001b[0;31m             [target_seq, encoder_outputs] + states_value)\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print(attention.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mattention_density\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoded_sentences = []\n",
    "corrected_sentences = []\n",
    "\n",
    "#for seq_index in range(len(input_texts)):\n",
    "results = open('RESULTS.md', 'w')\n",
    "results.write('|OCR sentence|GT sentence|Char decoded sentence|Word decoded sentence|Sentence length (chars)|\\n')\n",
    "results.write('---------------|-----------|----------------|----------------|----------------|\\n')\n",
    "     \n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    #print(input_text)\n",
    "    # Find the input length range to choose the proper model to use\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    \n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "    \n",
    "    \n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "    \n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    print('GT sentence:', target_text.strip())\n",
    "    print('Char Decoded sentence:', decoded_sentence)   \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    results.write(' | ' + input_text + ' | ' + target_text.strip() + ' | ' + decoded_sentence + ' | ' + corrected_sentence + ' | ' + str(len_range) + ' | \\n')\n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    corrected_sentences.append(corrected_sentence)\n",
    "results.close()    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: SUBJECTIVE: This is a S-year-old +@W his left great toe with the handleh lacration.\n",
      "Word Decoded sentence: SUBJECTIVES This is a S-year-old New his left great toe with the handled laceration \n",
      "\n",
      "\n",
      "Input sentence: Thera was no handlebarthe lacration.\n",
      "Word Decoded sentence: Thera was no handlebarthe laceration \n",
      "\n",
      "\n",
      "Input sentence: Patiet last tet is needing this for school at this\n",
      "Word Decoded sentence: Patient last tet is needing this for school at this \n",
      "\n",
      "\n",
      "Input sentence: OBJECTIVE : The temp is 99.8, the f tha blood pressure 99/64, O2 sat 94 8/10 at this time.\n",
      "Word Decoded sentence: OBJECTIVE : The temp is 99.8, the f tha blood pressure 99/64, O2 sat 94 8/10 at this time \n",
      "\n",
      "\n",
      "Input sentence: Left great toe the dorsl surface, extending ta th active hemorrhage at this time.\n",
      "Word Decoded sentence: Left great toe the dorsal surface extending ta th active hemorrhage at this time \n",
      "\n",
      "\n",
      "Input sentence: Th anaathetized with a cotton ball sat Left this in place for 20 minutes.\n",
      "Word Decoded sentence: Th anaathetized with a cotton ball sat Left this in place for 20 minutes \n",
      "\n",
      "\n",
      "Input sentence: with Betadine again and injected th he tolerated very well.\n",
      "Word Decoded sentence: with Betadine again and injected th he tolerated very well \n",
      "\n",
      "\n",
      "Input sentence: The wound sutures. Antibiotic eintment and g\n",
      "Word Decoded sentence: The wound sutures Antibiotic eintment and g \n",
      "\n",
      "\n",
      "Input sentence: Patient tolerated very well. Pat\n",
      "Word Decoded sentence: Patient tolerated very well Pat \n",
      "\n",
      "\n",
      "Input sentence: IMPRESSION: Lacration te left grs\n",
      "Word Decoded sentence: IMPRESSION Laceration te left grs \n",
      "\n",
      "\n",
      "Input sentence: PLAN: Patent is to do dressing ch advised as far as checking tha waurn it with soap and water.\n",
      "Word Decoded sentence: PLAN Patent is to do dressing ch advised as far as checking tha warn it with soap and water \n",
      "\n",
      "\n",
      "Input sentence: Sutures oy have any problems prior te that tim ona teaspoon three times a day rer Ibuprofan far pain, discomfort. Cg\n",
      "Word Decoded sentence: Sutures oy have any problems prior te that tim ona teaspoon three times a day rer Ibuprofan far pain discomfort Cg \n",
      "\n",
      "\n",
      "Input sentence: hite male who accidently dropped a bike onto ar end hitting the left great toe,causing a guard to the end of the bike, which caused anus shot is more that three years ago and time.\n",
      "Word Decoded sentence: Hite male who accidently dropped a bike onto ar end hitting the left great toe,causing a guard to the end of the bike which caused anus shot is more that three years ago and time \n",
      "\n",
      "\n",
      "Input sentence: nlse ef 105 and regular, resprations 286,% on room air.\n",
      "Word Decoded sentence: nlse ef 105 and regular respirations 286,% on room air \n",
      "\n",
      "\n",
      "Input sentence: Patient rates hia pain at — there i15 noted a 3-om laceration across a lateral aspect of tha toe.\n",
      "Word Decoded sentence: Patient rates hia pain at — there i15 noted a 3-om laceration across a lateral aspect of tha toe \n",
      "\n",
      "\n",
      "Input sentence: There is no e toa ir cleansed with Betadine.\n",
      "Word Decoded sentence: There is no e toa ir cleansed with Betadine. \n",
      "\n",
      "\n",
      "Input sentence: It is then urated with 5 cu of 2% Hylocaine plain.\n",
      "Word Decoded sentence: It is then urated with 5 cu of 2% Hylocaine plain \n",
      "\n",
      "\n",
      "Input sentence: We then cleansed ae toa with 3 cc of 2% Xylacaina plain\n",
      "Word Decoded sentence: We then cleansed ae toa with 3 cc of 2% Xylacaina plain \n",
      "\n",
      "\n",
      "Input sentence: which was then clesed with five 5-0 Prolene ressure dressing was then applied to the tos\n",
      "Word Decoded sentence: which was then closed with five 5-0 Prolene ressure dressing was then applied to the tos \n",
      "\n",
      "\n",
      "Input sentence: paient is given DPT 0.5 ee intramucular (IM).at toe.\n",
      "Word Decoded sentence: patient is given DPT 0.5 ee intramuscular (IM).at toe \n",
      "\n",
      "\n",
      "Input sentence: Kefylex 250 mg per 5 ml, the next seven days.\n",
      "Word Decoded sentence: Keflex 250 mg per 5 ml the next seven days \n",
      "\n",
      "\n",
      "Input sentence: He may use Tylenol or 11 if any problems.\n",
      "Word Decoded sentence: He may use Tylenol or 11 if any problems \n",
      "\n",
      "\n",
      "Input sentence: Unum Life Insurance Company of America 2211\n",
      "Word Decoded sentence: Unum Life Insurance Company of America 2211 \n",
      "\n",
      "\n",
      "Input sentence: Congress Street Portland, Maine 04122\n",
      "Word Decoded sentence: Congress Street Portland Maine 04122 \n",
      "\n",
      "\n",
      "Input sentence: APPLICATION FOR GROUP CRITICAL LLNESS INSURANCE\n",
      "Word Decoded sentence: APPLICATION FOR GROUP CRITICAL ILLNESS INSURANCE \n",
      "\n",
      "\n",
      "Input sentence: I Evidence of Insurability\n",
      "Word Decoded sentence: I Evidence of Insurability \n",
      "\n",
      "\n",
      "Input sentence: \n",
      "Word Decoded sentence: a \n",
      "\n",
      "\n",
      "Input sentence: Application Type: @ New Enrollee Change to\n",
      "Word Decoded sentence: Application Type a New Enrollee Change to \n",
      "\n",
      "\n",
      "Input sentence: Existing Coverage Reinstatement Internal\n",
      "Word Decoded sentence: Existing Coverage Reinstatement Internal \n",
      "\n",
      "\n",
      "Input sentence: Replacement Late Applicant Rehire SECTION 1:\n",
      "Word Decoded sentence: Replacement Late Applicant Rehire SECTION 1: \n",
      "\n",
      "\n",
      "Input sentence: Employee(Applicant) Information Always\n",
      "Word Decoded sentence: Employee(Applicant) Information Always \n",
      "\n",
      "\n",
      "Input sentence: Complete Employee Name(First, Middle, Last)\n",
      "Word Decoded sentence: Complete Employee Name(First, Middle Last \n",
      "\n",
      "\n",
      "Input sentence: Social Security Number Nikolas J Jones\n",
      "Word Decoded sentence: Social Security Number Nikolas J Jones \n",
      "\n",
      "\n",
      "Input sentence: 123 - 456 - 7890 Home Address(Street/ PO Box)\n",
      "Word Decoded sentence: 123 - 456 - 7890 Home Address(Street/ PO Box \n",
      "\n",
      "\n",
      "Input sentence: Gender 1634 Stewert St F M City Date of Birth\n",
      "Word Decoded sentence: Gender 1634 Stewer St F M City Date of Birth \n",
      "\n",
      "\n",
      "Input sentence: (mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip\n",
      "Word Decoded sentence: mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip \n",
      "\n",
      "\n",
      "Input sentence: Code Home Phone # Washington 98101 854-555-1212\n",
      "Word Decoded sentence: Code Home Phone # Washington 98101 854-555-1212 \n",
      "\n",
      "\n",
      "Input sentence: Are you Actively at Work? Employee ID / Payroll #\n",
      "Word Decoded sentence: Are you Actively at Work Employee ID / Payroll # \n",
      "\n",
      "\n",
      "Input sentence: Yes No55624 a.Are you a U.S.Citizen or\n",
      "Word Decoded sentence: Yes No55624 aware you a U.S.Citizen or \n",
      "\n",
      "\n",
      "Input sentence: Canadian Citizen working in the U.S.? b.Are you\n",
      "Word Decoded sentence: Canadian Citizen working in the U.S.? bare you \n",
      "\n",
      "\n",
      "Input sentence: legally authorized to work in Yes No(If No\n",
      "Word Decoded sentence: legally authorized to work in Yes Notify No \n",
      "\n",
      "\n",
      "Input sentence: reply to part b) the U.S.? Yes No Employer\n",
      "Word Decoded sentence: reply to part by the U.S.? Yes No Employer \n",
      "\n",
      "\n",
      "Input sentence: Name Group Number Date of Hire(mm/ dd / yyyy)\n",
      "Word Decoded sentence: Name Group Number Date of Hire(mm/ dd / yyyy) \n",
      "\n",
      "\n",
      "Input sentence: Facebook 11 - 555566 11 / 30 / 2016 Occupation\n",
      "Word Decoded sentence: Facebook 11 - 555566 11 / 30 / 2016 Occupation \n",
      "\n",
      "\n",
      "Input sentence: Eligibility Class Software Engineer 7 Scheduled\n",
      "Word Decoded sentence: Eligibility Class Software Engineer 7 Scheduled \n",
      "\n",
      "\n",
      "Input sentence: Number of Work Hours per Week Work Phone # 35\n",
      "Word Decoded sentence: Number of Work Hours per Week Work Phone # 35 \n",
      "\n",
      "\n",
      "Input sentence: 854-555-6622 SECTION 2: Spouse Information\n",
      "Word Decoded sentence: 854-555-6622 SECTION 2: Spouse Information \n",
      "\n",
      "\n",
      "Input sentence: Complete Only if applying for Spouse coverage Name\n",
      "Word Decoded sentence: Complete Only if applying for Spouse coverage Name \n",
      "\n",
      "\n",
      "Input sentence: (First, Middle, Last) Social Security Number\n",
      "Word Decoded sentence: First Middle Last Social Security Number \n",
      "\n",
      "\n",
      "Input sentence: Gender Date of Birth(mm / dd / yyyy) Does the\n",
      "Word Decoded sentence: Gender Date of Birth(mm / dd / yyyy) Does the \n",
      "\n",
      "\n",
      "Input sentence: 1019 - 07 - AZ 1\n",
      "Word Decoded sentence: 1019 - 07 - AZ 1 \n",
      "\n",
      "\n",
      "Input sentence: if claint is for a child, please state your relationship 10 the child\n",
      "Word Decoded sentence: if clint is for a child please state your relationship 10 the child \n",
      "\n",
      "\n",
      "Input sentence: date of accident 3d _ time of accident ram. 0 p.m.\n",
      "Word Decoded sentence: date of accident 3d a time of accident rams 0 pm \n",
      "\n",
      "\n",
      "Input sentence: have you slopped working? (of yes [1 no if yes, what was the last day that you worked? (mm/ddryy)_| —3 | —{% cnslamegs bil =\n",
      "Word Decoded sentence: have you slopped working of yes [1 no if yes what was the last day that you worked (mm/ddryy)_| —3 a —{% cnslamegs Bil a \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = ['SUBJECTIVE: This is a S-year-old +@W his left great toe with the handleh lacration.',\n",
    "               'Thera was no handlebarthe lacration.',\n",
    "               'Patiet last tet is needing this for school at this',\n",
    "               'OBJECTIVE : The temp is 99.8, the f tha blood pressure 99/64, O2 sat 94 8/10 at this time.',\n",
    "               'Left great toe the dorsl surface, extending ta th active hemorrhage at this time.',\n",
    "               'Th anaathetized with a cotton ball sat Left this in place for 20 minutes.',\n",
    "               'with Betadine again and injected th he tolerated very well.',\n",
    "               'The wound sutures. Antibiotic eintment and g',\n",
    "               'Patient tolerated very well. Pat',\n",
    "               'IMPRESSION: Lacration te left grs',\n",
    "               'PLAN: Patent is to do dressing ch advised as far as checking tha waurn it with soap and water.',\n",
    "               'Sutures oy have any problems prior te that tim ona teaspoon three times a day rer Ibuprofan far pain, discomfort. Cg',\n",
    "               'hite male who accidently dropped a bike onto ar end hitting the left great toe,'\n",
    "               'causing a guard to the end of the bike, which caused anus shot is more that three years ago and time.',\n",
    "               'nlse ef 105 and regular, resprations 286,% on room air.',\n",
    "               'Patient rates hia pain at — there i15 noted a 3-om laceration across a lateral aspect of tha toe.',\n",
    "               'There is no e toa ir cleansed with Betadine.',\n",
    "               'It is then urated with 5 cu of 2% Hylocaine plain.',\n",
    "               'We then cleansed ae toa with 3 cc of 2% Xylacaina plain',\n",
    "               'which was then clesed with five 5-0 Prolene ressure dressing was then applied to the tos',\n",
    "               'paient is given DPT 0.5 ee intramucular (IM).at toe.',\n",
    "               'Kefylex 250 mg per 5 ml, the next seven days.',\n",
    "               'He may use Tylenol or 11 if any problems.',\n",
    "               'Unum Life Insurance Company of America 2211',               \n",
    "               'Congress Street Portland, Maine 04122',\n",
    "               'APPLICATION FOR GROUP CRITICAL LLNESS INSURANCE',\n",
    "               'I Evidence of Insurability',\n",
    "               '',\n",
    "               'Application Type: @ New Enrollee Change to',\n",
    "               'Existing Coverage  Reinstatement  Internal',\n",
    "               'Replacement  Late Applicant  Rehire SECTION 1:',\n",
    "               'Employee(Applicant) Information  Always',\n",
    "               'Complete Employee Name(First, Middle, Last)',\n",
    "               'Social Security Number Nikolas J Jones',\n",
    "               '123 - 456 - 7890 Home Address(Street/ PO Box)',\n",
    "               'Gender 1634 Stewert St  F  M City Date of Birth',\n",
    "               '(mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip',\n",
    "               'Code Home Phone # Washington 98101 854-555-1212',\n",
    "               'Are you Actively at Work? Employee ID / Payroll #',\n",
    "               ' Yes  No55624 a.Are you a U.S.Citizen or',\n",
    "               'Canadian Citizen working in the U.S.? b.Are you',\n",
    "               'legally authorized to work in  Yes  No(If No',\n",
    "               'reply to part b) the U.S.?  Yes  No Employer',\n",
    "               'Name Group Number Date of Hire(mm/ dd / yyyy)',\n",
    "               'Facebook 11 - 555566 11 / 30 / 2016 Occupation',\n",
    "               'Eligibility Class Software Engineer 7 Scheduled',\n",
    "               'Number of Work Hours per Week Work Phone # 35',\n",
    "               '854-555-6622 SECTION 2: Spouse Information ',\n",
    "               'Complete Only if applying for Spouse coverage Name',\n",
    "               '(First, Middle, Last) Social Security Number',\n",
    "               'Gender Date of Birth(mm / dd / yyyy) Does the',\n",
    "               '1019 - 07 - AZ 1',\n",
    "              'if claint is for a child, please state your relationship 10 the child',\n",
    "              'date of accident 3d _ time of accident ram. 0 p.m.',\n",
    "              'have you slopped working? (of yes [1 no if yes, what was the last day that you worked? (mm/ddryy)_| —3 | —{% cnslamegs bil =']\n",
    "               \n",
    "for input_text in input_texts:\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    pre_corrected_sentence = word_spell_correct(input_text)\n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "\n",
    "\n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "\n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    #print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    #print('Spell Decoded sentence:', pre_corrected_sentence) \n",
    "    #print('Char Decoded sentence:', decoded_sentence)   \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fai \n",
      "10 \n",
      "7521509 \n",
      "(FISTDEOO) \n",
      "at \n",
      "11/3/2017 \n",
      "5:23:19 \n",
      "from \n",
      "-9373834004 \n",
      "Req \n",
      "IC \n",
      "2017:1030525109:292E. \n",
      "Page \n",
      "4 \n",
      "of \n",
      "5 \n",
      "Act \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11/3/2017 \n",
      "FRI \n",
      "8:26 \n",
      "FAX \n",
      "2373834004 \n",
      "Kjooas00s \n",
      "\n",
      "\n",
      "\n",
      "as3-ursasy3 \n",
      "11:30:11 \n",
      "11/2/2017 \n",
      "vis \n",
      "\n",
      "\n",
      "\n",
      "a \n",
      "a \n",
      "a \n",
      "ACCIDENT \n",
      "CLAIM \n",
      "FORM \n",
      "\n",
      "UU \n",
      "numb \n",
      "Tha \n",
      "Benefits \n",
      "Canter \n",
      "\n",
      "Poor \n",
      "Bax \n",
      "100158, \n",
      "Calumbin, \n",
      "EC \n",
      "20202-3150 \n",
      "\n",
      "Tol-frea: \n",
      "1-800-635-5587 \n",
      "Fax \n",
      "1-800-447-2488 \n",
      "\n",
      "Gall \n",
      "toll-free \n",
      "Monday \n",
      "through \n",
      "Friday \n",
      "8 \n",
      "am \n",
      "lo \n",
      "8 \n",
      "pm \n",
      "Eastern \n",
      "Time \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[ \n",
      "ATTENDING \n",
      "PHYSICIAN \n",
      "STATEMENT \n",
      "] \n",
      "\n",
      "\n",
      "IneurexiPolicyt \n",
      "altar \n",
      "Hama \n",
      "Lael \n",
      "Name \n",
      "Flisk \n",
      "Naman \n",
      "MID \n",
      "Suffix \n",
      "Data \n",
      "of \n",
      "Isth \n",
      "{msmidrfyy) \n",
      "- \n",
      "\n",
      "\n",
      "Fault \n",
      "Nana \n",
      "Claut \n",
      "Humet \n",
      "Filial \n",
      "Numac \n",
      "1 \n",
      "Suth \n",
      "Days \n",
      "al \n",
      "Ban \n",
      "raved \n",
      "Ul \n",
      "a \n",
      "\n",
      "-[ECIpENT \n",
      "DETAILS \n",
      "] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a \n",
      "this \n",
      "Gundilan \n",
      "the \n",
      "result \n",
      "of \n",
      "a \n",
      "accidental \n",
      "inury \n",
      "yes \n",
      "O \n",
      "No \n",
      "if \n",
      "yas \n",
      "dale \n",
      "of \n",
      "accident \n",
      "qre/ddlyy) \n",
      "[1 \n",
      "0] \n",
      "[z]e \n",
      "[=] \n",
      "\n",
      "\n",
      "\n",
      "Is \n",
      "Mig \n",
      "condition \n",
      "Lhe \n",
      "result \n",
      "of \n",
      "refer \n",
      "employment \n",
      "£1 \n",
      "Yes \n",
      "no \n",
      "[1 \n",
      "Unknown \n",
      "\n",
      "\n",
      "\n",
      "Place \n",
      "verily \n",
      "treatment \n",
      "for \n",
      "the \n",
      "accident \n",
      "Lala \n",
      "above \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dalar \n",
      "of \n",
      "Diagnosis \n",
      "Diagnosis \n",
      "Description \n",
      "Procedure \n",
      "Procedure \n",
      "Description \n",
      "\n",
      "Branden \n",
      "Including \n",
      "a \n",
      "Cuds \n",
      "PGDP \n",
      "ous \n",
      "\n",
      "Confinement \n",
      "er \n",
      "ap \n",
      "HAS \n",
      "TTT \n",
      "\n",
      "BEEF \n",
      "er \n",
      "\n",
      "wiz \n",
      ". \n",
      "S33,5XxA \n",
      "Hh \n",
      "rises \n",
      "ey \n",
      "race \n",
      "Word \n",
      "\n",
      "awqd] \n",
      "\n",
      "weak \n",
      "3 \n",
      "n \n",
      "aveny \n",
      "d \n",
      "\n",
      "WiFi \n",
      "Wl \n",
      "\n",
      "oa \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Has \n",
      "lhe \n",
      "pallet \n",
      "bean \n",
      "trusted \n",
      "for \n",
      "tha \n",
      "same \n",
      "ar \n",
      "a \n",
      "Similar \n",
      "candillan \n",
      "by \n",
      "another \n",
      "phyalelan \n",
      "In \n",
      "tha \n",
      "past \n",
      "[1 \n",
      "Yen \n",
      "Who \n",
      "\n",
      "M \n",
      "yor \n",
      "Poona \n",
      "proved \n",
      "tha \n",
      "fares \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Diagnosis \n",
      "Tamiment \n",
      "Datenu \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "id \n",
      "ya.1 \n",
      "having \n",
      "Lhe \n",
      "patient \n",
      "to \n",
      "clap \n",
      "working \n",
      "RECEIVED \n",
      "\n",
      "It \n",
      "yes \n",
      "B8 \n",
      "of \n",
      "what \n",
      "cater \n",
      "(mmidkyy) \n",
      "\n",
      "\n",
      "\n",
      "[23] \n",
      "[117] \n",
      "\n",
      "\n",
      "\n",
      "Cih \n",
      "cielih \n",
      "fa \n",
      "rotated \n",
      "to \n",
      "normal \n",
      "prepnency, \n",
      "please \n",
      "gravida \n",
      "tha \n",
      "idliawing: \n",
      "NOV \n",
      "\n",
      "Expected \n",
      "Delivery \n",
      "Dale \n",
      "(mimicd/yy) \n",
      "Actual \n",
      "Delivery \n",
      "Dale \n",
      "mmiddlyy \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physician \n",
      "information \n",
      "HUMAN \n",
      "REGOURCITE \n",
      "\n",
      "\n",
      "\n",
      "FRAUD \n",
      "NOTICE \n",
      "Any \n",
      "person \n",
      "wha \n",
      "knowingly \n",
      "files \n",
      "a \n",
      "statement \n",
      "of \n",
      "claim \n",
      "containing \n",
      "FALSE \n",
      "or \n",
      "misleading \n",
      "information \n",
      "8 \n",
      "\n",
      "subject \n",
      "to \n",
      "criminal \n",
      "and \n",
      "evil \n",
      "penalties \n",
      "This \n",
      "includes \n",
      "Attending \n",
      "Physician \n",
      "portions \n",
      "of \n",
      "the \n",
      "claim \n",
      "farm \n",
      "\n",
      "\n",
      "\n",
      "CS \n",
      "Yma \n",
      "SEAS \n",
      "Ta \n",
      "he \n",
      "plan \n",
      "a \n",
      "\n",
      "The \n",
      "above \n",
      "statements \n",
      "ara \n",
      "trun \n",
      "And \n",
      "rompints \n",
      "to \n",
      "tho \n",
      "bot \n",
      "of \n",
      "my \n",
      "knowledge \n",
      "and \n",
      "bolluf. \n",
      "\n",
      "\n",
      "\n",
      "Physician \n",
      "Name \n",
      "Clear \n",
      "Name \n",
      "First \n",
      "Name \n",
      "MID \n",
      "Suit \n",
      "Places \n",
      "Print \n",
      "Co \n",
      "Fhma \n",
      "log \n",
      "Mm \n",
      "\n",
      "/ \n",
      "a \n",
      "\n",
      "\n",
      "\n",
      "Medical \n",
      "Speclaty \n",
      "Tr \n",
      "eactal- \n",
      "] \n",
      "a \n",
      "D \n",
      "of \n",
      "r \n",
      "of \n",
      "Ch \n",
      "2 \n",
      "\n",
      "2 \n",
      "Le \n",
      "\n",
      "\n",
      "of \n",
      "Zoic \n",
      "M \n",
      "o \n",
      "“Fanart \n",
      "\n",
      "\n",
      "a \n",
      "Balfrone \n",
      "ie \n",
      "2 \n",
      "Sle \n",
      "IU \n",
      "\n",
      "il \n",
      "HY \n",
      "BY \n",
      "1942 \n",
      "Fax \n",
      "Number \n",
      "you \n",
      "43 \n",
      "-8 \n",
      "7775 \n",
      "Fhyalafans \n",
      "Tax \n",
      "ID \n",
      "Number \n",
      "\n",
      "\n",
      "\n",
      "Aro \n",
      "you \n",
      "related \n",
      "to \n",
      "hiv \n",
      "pollens \n",
      "0 \n",
      "Yoe \n",
      "llm \n",
      "a \n",
      "yes \n",
      "Wal \n",
      "iv \n",
      "the \n",
      "relelianshipT \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physician \n",
      "Signature \n",
      "Date \n",
      "\n",
      "CL-1023 \n",
      "-2717 \n",
      "a \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "— \n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_texts = ['text',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Fai',\n",
    "'10',\n",
    "'7521509',\n",
    "'(FISTDEOO)',\n",
    "'at',\n",
    "'11/3/2017',\n",
    "'5:23:19',\n",
    "'from',\n",
    "'-9373834004',\n",
    "'Req',\n",
    "'IC',\n",
    "'2017:1030525109:292E.',\n",
    "'Page',\n",
    "'4',\n",
    "'of',\n",
    "'5',\n",
    "'(C)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'11/3/2017',\n",
    "'FRI',\n",
    "'8:26',\n",
    "'FAX',\n",
    "'2373834004',\n",
    "'Kjooas00s',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'as3-ursasy3',\n",
    "'11:30:11',\n",
    "'11/2/2017',\n",
    "'vis',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'®',\n",
    "'®',\n",
    "'&',\n",
    "'ACCIDENT',\n",
    "'CLAIM',\n",
    "'FORM',\n",
    "'',\n",
    "'uu',\n",
    "'num’',\n",
    "'Tha',\n",
    "'Benelits',\n",
    "'Canter',\n",
    "'',\n",
    "'P.O.',\n",
    "'Bax',\n",
    "'100158,',\n",
    "'Calumbin,',\n",
    "'EC',\n",
    "'20202-3150',\n",
    "'',\n",
    "'Tol-frea:',\n",
    "'1-800-635-5587',\n",
    "'Fax:',\n",
    "'1-800-447-2488',\n",
    "'',\n",
    "'Gall',\n",
    "'toll-free',\n",
    "'Monday',\n",
    "'through',\n",
    "'Friday,',\n",
    "'8',\n",
    "'a.m.',\n",
    "'lo',\n",
    "'8',\n",
    "'p.m,',\n",
    "'Eagtarn',\n",
    "'Time.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[',\n",
    "'ATTENDING',\n",
    "'PHYSICIAN',\n",
    "'STATEMENT',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'IneurexiPolicyt',\n",
    "'alcar',\n",
    "'Hama',\n",
    "'(Lael',\n",
    "'Name,',\n",
    "'Flis!',\n",
    "'Nama,',\n",
    "'MI,',\n",
    "'Suffix)',\n",
    "'Data',\n",
    "'of',\n",
    "'Risth',\n",
    "'{msmidrfyy)',\n",
    "'-',\n",
    "'',\n",
    "'',\n",
    "'Faupi',\n",
    "'Nana',\n",
    "'{Laut',\n",
    "'Hume,',\n",
    "'Flial',\n",
    "'Numa,',\n",
    "'1',\n",
    "'Sut)',\n",
    "'Dats',\n",
    "'al',\n",
    "'Bln',\n",
    "'rAvad)',\n",
    "'Ul',\n",
    "'_',\n",
    "'',\n",
    "'-[ECIpENT',\n",
    "'DETAILS',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'a',\n",
    "'thls',\n",
    "'Gundilan',\n",
    "'the',\n",
    "'result',\n",
    "'of',\n",
    "'a',\n",
    "'acddental',\n",
    "'inury?',\n",
    "'ves',\n",
    "'O',\n",
    "'No',\n",
    "'if',\n",
    "'yas,',\n",
    "'dale',\n",
    "'of',\n",
    "'accident',\n",
    "'qre/ddlyy)',\n",
    "'[1',\n",
    "'0]',\n",
    "'[z]e',\n",
    "'[=]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Is',\n",
    "'Mig',\n",
    "'condition',\n",
    "'Lhe',\n",
    "'result',\n",
    "'of',\n",
    "'hefer',\n",
    "'employment',\n",
    "'£1',\n",
    "'Yes',\n",
    "'pNo',\n",
    "'[1',\n",
    "'Unknown',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Plaaze',\n",
    "'verily',\n",
    "'treatment',\n",
    "'for',\n",
    "'the',\n",
    "'accident',\n",
    "'lalad',\n",
    "'above.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Dalaw',\n",
    "'of',\n",
    "'Diagnosis',\n",
    "'Diagncsis',\n",
    "'Description',\n",
    "'Prosadure',\n",
    "'Procedure',\n",
    "'Dascription',\n",
    "'',\n",
    "'Branden',\n",
    "'(Including',\n",
    "'|',\n",
    "'Cudo',\n",
    "'(GD)',\n",
    "'ous',\n",
    "'',\n",
    "'Confinement)',\n",
    "'eR',\n",
    "'ap',\n",
    "'HAS',\n",
    "'TTT',\n",
    "'',\n",
    "'BEEF',\n",
    "'eR',\n",
    "'',\n",
    "'wiz]',\n",
    "'.',\n",
    "'S33,5XxA',\n",
    "'Hh',\n",
    "'rioes',\n",
    "'ey',\n",
    "'race',\n",
    "'Word',\n",
    "'',\n",
    "'awqd]',\n",
    "'',\n",
    "'weak',\n",
    "'3',\n",
    "'n',\n",
    "'[aveny',\n",
    "'[d',\n",
    "'',\n",
    "'wifi',\n",
    "'Wl',\n",
    "'',\n",
    "'oa',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Has',\n",
    "'lhe',\n",
    "'pallet',\n",
    "'bean',\n",
    "'trastad',\n",
    "'for',\n",
    "'tha',\n",
    "'same',\n",
    "'ar',\n",
    "'&',\n",
    "'S(tilar',\n",
    "'candillan',\n",
    "'by',\n",
    "'anolher',\n",
    "'phyalelan',\n",
    "'In',\n",
    "'tha',\n",
    "'past?',\n",
    "'[1',\n",
    "'Yen',\n",
    "'Bho',\n",
    "'',\n",
    "'M',\n",
    "'yor,',\n",
    "'pioona',\n",
    "'provid',\n",
    "'tha',\n",
    "'fares:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Diageosis:',\n",
    "'Tramiment',\n",
    "'Daten:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'id',\n",
    "'ya.1',\n",
    "'#dving',\n",
    "'Lhe',\n",
    "'patient',\n",
    "'to',\n",
    "'clap',\n",
    "'working?',\n",
    "'RECEIVED',\n",
    "'',\n",
    "'It',\n",
    "'yes,',\n",
    "'B8',\n",
    "'of',\n",
    "'what',\n",
    "'cate?',\n",
    "'(mmidkyy)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[23]',\n",
    "'[117]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[Ih',\n",
    "'cielih',\n",
    "'fa',\n",
    "'rotated',\n",
    "'to',\n",
    "'normal',\n",
    "'prepnency,',\n",
    "'please',\n",
    "'grovida',\n",
    "'tha',\n",
    "'idliawing:',\n",
    "'NOV',\n",
    "'',\n",
    "'Expecigd',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'(mimicd/yy)',\n",
    "'Aclual',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'{mmiddlyy',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Phyeiclan',\n",
    "'informaiton',\n",
    "'HUMAN',\n",
    "'REGOURCITE',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'FRAUD',\n",
    "'NOTICE:',\n",
    "'Any',\n",
    "'person',\n",
    "'wha',\n",
    "'knowingly',\n",
    "'files',\n",
    "'&',\n",
    "'statement',\n",
    "'of',\n",
    "'clalm',\n",
    "'containing',\n",
    "'FALSE',\n",
    "'or',\n",
    "'misleading',\n",
    "'information',\n",
    "'8',\n",
    "'',\n",
    "'subject',\n",
    "'to',\n",
    "'criminal',\n",
    "'and',\n",
    "'elvil',\n",
    "'penallies.',\n",
    "'This',\n",
    "'includes',\n",
    "'Attending',\n",
    "'Physician',\n",
    "'portions',\n",
    "'of',\n",
    "'the',\n",
    "'claim',\n",
    "'farm.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'CS',\n",
    "'yma',\n",
    "'SEAS',\n",
    "'Ta',\n",
    "'hve',\n",
    "'glan',\n",
    "'=',\n",
    "'',\n",
    "'The',\n",
    "'above',\n",
    "'statements',\n",
    "'ara',\n",
    "'trun',\n",
    "'And',\n",
    "'rompints',\n",
    "'to',\n",
    "'tho',\n",
    "'bot',\n",
    "'of',\n",
    "'my',\n",
    "'knowledge',\n",
    "'and',\n",
    "'bolluf.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physician',\n",
    "'Name',\n",
    "'(Lea!',\n",
    "'Name,',\n",
    "'Firat',\n",
    "'Name,',\n",
    "'MI,',\n",
    "'Suita)',\n",
    "'Plases',\n",
    "'Print',\n",
    "'Co',\n",
    "'FHman',\n",
    "'log',\n",
    "'Mm',\n",
    "'',\n",
    "'/',\n",
    "'‘',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Medical',\n",
    "'Speclaty',\n",
    "'[Tr',\n",
    "'eactal-',\n",
    "']',\n",
    "'|',\n",
    "'D',\n",
    "'of',\n",
    "'r',\n",
    "'of',\n",
    "'Ch',\n",
    "'2',\n",
    "'',\n",
    "'2',\n",
    "'Le',\n",
    "'',\n",
    "'',\n",
    "'==',\n",
    "'Zoi!',\n",
    "'M',\n",
    "'o',\n",
    "'“Fanart',\n",
    "'',\n",
    "'',\n",
    "'=',\n",
    "'Balfrone',\n",
    "'ie',\n",
    "'2',\n",
    "'Sle',\n",
    "'iu',\n",
    "'',\n",
    "'il',\n",
    "'HY',\n",
    "'BY',\n",
    "'1942',\n",
    "'Fax',\n",
    "'Number',\n",
    "'yz—',\n",
    "'43',\n",
    "'-8',\n",
    "'7775',\n",
    "'Fhyalafans',\n",
    "'Tax',\n",
    "'ID',\n",
    "'Number.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Aro',\n",
    "'you',\n",
    "'refateq',\n",
    "'to',\n",
    "'hiv',\n",
    "'pollen?',\n",
    "'0',\n",
    "'Yoe',\n",
    "'LlMo',\n",
    "'|',\n",
    "'yes,',\n",
    "'wal',\n",
    "'iv',\n",
    "'the',\n",
    "'relelianshipT',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physlclan',\n",
    "'Slgnature',\n",
    "'Date',\n",
    "'',\n",
    "'CL-1023',\n",
    "'-2717',\n",
    "'=',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'—',]\n",
    "               \n",
    "for input_text in input_texts:\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    pre_corrected_sentence = word_spell_correct(input_text)\n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "\n",
    "\n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "\n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    #print('-Lenght = ', len_range)\n",
    "    #print('Input sentence:', input_text)\n",
    "    #print('Spell Decoded sentence:', pre_corrected_sentence) \n",
    "    #print('Char Decoded sentence:', decoded_sentence)   \n",
    "    \n",
    "    #print('Word Decoded sentence:', corrected_sentence) \n",
    "    print(corrected_sentence) \n",
    "    #print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☑ @ New Enrollee ☐ Change to Existing Coverage ☐ Reinstatement\n",
      "☑ @ New Enrollee ☐ Change to Existing Coverage ☐ Reinstatement \n",
      "☑ @ New Enrollee ☐ Change to Existing Coverage ☐ Reinstatement \n"
     ]
    }
   ],
   "source": [
    "input_texts = ['☑ @ New Enrollee ☐ Change to Existing Coverage ☐ Reinstatement']\n",
    "for input_text in input_texts:\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    print(input_text)\n",
    "    pre_corrected_sentence = word_spell_correct(input_text)\n",
    "    print(pre_corrected_sentence)\n",
    "    input_text_ = input_text\n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "\n",
    "\n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "\n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text_)\n",
    "    #print('-Lenght = ', len_range)\n",
    "    #print('Input sentence:', input_text)\n",
    "    #print('Spell Decoded sentence:', pre_corrected_sentence) \n",
    "    #print('Char Decoded sentence:', decoded_sentence)   \n",
    "    \n",
    "    #print('Word Decoded sentence:', corrected_sentence) \n",
    "    print(corrected_sentence) \n",
    "    #print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3749\n",
      " is insisting on a policy of change . \n",
      " \n",
      " \tis insisting on a policy of change . \n",
      "\n",
      " 12/29/17 \n",
      " \n",
      " \t12/29/17 \n",
      "\n",
      " SIAL)TH \n",
      " \n",
      " \tSLP(L) THA \n",
      "\n",
      " Arcadia CA 91007 \n",
      " \n",
      " \tArcadia CA 91007 \n",
      "\n",
      " (012) 667 9375 \n",
      " \n",
      " \t(012) 6674375 \n",
      "\n",
      " In this 200-fathom trench the herring do not tood the botton . \n",
      " \n",
      " \tIn this 200-fathom trench the herring do not touch the bottom . \n",
      "\n",
      " 43638556X1 \n",
      " \n",
      " \t43638556X1 \n",
      "\n",
      " Pretoria \n",
      " \n",
      " \tPretoria \n",
      "\n",
      " fiddaling about with bils of cost . \n",
      " \n",
      " \tfiddling about with bills of cost . \n",
      "\n",
      " ( Fig. 3) . Loop threed tound liite finger t \n",
      " \n",
      " \t( Fig. 3 ) . Loop thread round little finger , \n",
      "\n",
      " 200681383 \n",
      " \n",
      " \t200681383 \n",
      "\n",
      " for a working week of 34 to 36 houns . \n",
      " \n",
      " \tfor a working week of 34 to 36 hours . \n",
      "\n",
      " Daugher \n",
      " \n",
      " \tDaugther \n",
      "\n",
      " Electronically Signed \n",
      " \n",
      " \tElectronically Signed \n",
      "\n",
      " 15122 \n",
      " \n",
      " \t15122 \n",
      "\n",
      " 50 \n",
      " \n",
      " \t50 \n",
      "\n",
      " ShE WAS MOVVD A Picvic TablE TO SWEAP leAveS And DROREd iT oN hER BSC \n",
      " \n",
      " \tShE wAs Moving A Picnic TAble To swEEp lEAvEs And DRoPEd iT on hER ToE \n",
      "\n",
      " 0724603309 \n",
      " \n",
      " \t0724603509 \n",
      "\n",
      " lwas car ping a box into the basement Nhen ) sipped on the stairs and landed on my loft ba toe . \n",
      " \n",
      " \tI was carrying a box into the basement When I slipped on the stairs and landed on my left big toe. \n",
      "\n",
      " Leconte ER \n",
      " \n",
      " \tLeconte ER \n",
      "\n",
      " 715-832-0520 \n",
      " \n",
      " \t715-832-0520 \n",
      "\n",
      " 7 \n",
      " \n",
      " \t7 \n",
      "\n",
      " 1/26/18 \n",
      " \n",
      " \t1/26/18 \n",
      "\n",
      " 004-47788-187052 \n",
      " \n",
      " \t004-47788-187052 \n",
      "\n",
      " 03-05-18 \n",
      " \n",
      " \t03-05-18 \n",
      "\n",
      " 100713 5242 083 \n",
      " \n",
      " \t100713 5242 083 \n",
      "\n",
      " is to be made at a meeting of Labour \n",
      " \n",
      " \tis to be made at a meeting of Labour \n",
      "\n",
      " 02/27/18 \n",
      " \n",
      " \t02/27/18 \n",
      "\n",
      " Governement . Immediately Mr. Rennely rushed a letter \n",
      " \n",
      " \tGovernment . Immediately Mr. Kennedy rushed a letter \n",
      "\n",
      " 01-06-2018 \n",
      " \n",
      " \t01-06-2018 \n",
      "\n",
      " United weight beaning/walking \n",
      " \n",
      " \tUnited weight bearing/walking \n",
      "\n",
      " 6406085079088 \n",
      " \n",
      " \t6406085079088 \n",
      "\n",
      " and unconttilutlonal \" interferonce by \n",
      " \n",
      " \tand unconstitutional \" interference by \n",
      "\n",
      " (412)267-8040 \n",
      " \n",
      " \t(412)-267-5040 \n",
      "\n",
      " Senator Allen Elender , of louisiana , \n",
      " \n",
      " \tSenator Allen Ellender , of Louisiana , \n",
      "\n",
      " monique Norman \n",
      " \n",
      " \tMonique Norman \n",
      "\n",
      " Artham EWALD \n",
      " \n",
      " \tAnthan EWALD \n",
      "\n",
      " Butn & Woud CalR \n",
      " \n",
      " \tBuRn & Wound cAre \n",
      "\n",
      " 2 \n",
      " \n",
      " \t2 \n",
      "\n",
      " Mphe 17 Monshioagae \n",
      " \n",
      " \tMpho M Monstshioagae \n",
      "\n",
      " Seven Commonwealth countiies hve told Mr. Jandys , \n",
      " \n",
      " \tSeven Commonwealth countries have told Mr. Sandys , \n",
      "\n",
      " and again and again it is the visual qualities \n",
      " \n",
      " \tand again and again it is the visual qualities \n",
      "\n",
      " 061 360 0643 \n",
      " \n",
      " \t061 360 0643 \n",
      "\n",
      " and peveals then the required amonns \n",
      " \n",
      " \tend reveals when the required amount \n",
      "\n",
      " qaontity surveyor . We have descended \n",
      " \n",
      " \tquantity surveyor . We have descended \n",
      "\n",
      " 3 \n",
      " \n",
      " \t3 \n",
      "\n",
      " WARBLEA STR. 27 SOmERSET wAS \n",
      " \n",
      " \tWARBLER STR. 21 SOMERSET WES \n",
      "\n",
      " Frractue Ostal \n",
      " \n",
      " \tFracture Distal \n",
      "\n",
      " oton has been boycotbed by the the man \n",
      " \n",
      " \tLondon has been boycotted by the two main \n",
      "\n",
      " South-Africa \n",
      " \n",
      " \tSouth-Africa \n",
      "\n",
      " N/A \n",
      " \n",
      " \tN/A \n",
      "\n",
      " 11A \n",
      " \n",
      " \t11A \n",
      "\n",
      " 432-262-2440 \n",
      " \n",
      " \t432-262-2440 \n",
      "\n",
      " 070624 6132 080 \n",
      " \n",
      " \t070624 6132 080 \n",
      "\n",
      " 0 \n",
      " \n",
      " \t0 \n",
      "\n",
      " 8 \n",
      " \n",
      " \t8 \n",
      "\n",
      " vandyksandra23(a)gmail.com \n",
      " \n",
      " \tvandysandra23(a)gmail.com \n",
      "\n",
      " OV \n",
      " \n",
      " \tOV \n",
      "\n",
      " Deict Mantz \n",
      " \n",
      " \tDerick Maritz \n",
      "\n",
      " 087) 3451380 \n",
      " \n",
      " \t087 3451380 \n",
      "\n",
      " food scheme . It was maintained dwring \n",
      " \n",
      " \tfood scheme . It was maintained during \n",
      "\n",
      " that he proved to be to the Jees frreing fo \n",
      " \n",
      " \tthat he proved to be to the Jews fleeing from \n",
      "\n",
      " Gabarel Adam Viles \n",
      " \n",
      " \tGabriel Adam VIles \n",
      "\n",
      " whiplash- S13.9X85 \n",
      " \n",
      " \twhiplash - S13.4XXD \n",
      "\n",
      " 9:15 \n",
      " \n",
      " \t9:15 \n",
      "\n",
      " 3/2/2018 \n",
      " \n",
      " \t3/2/2018 \n",
      "\n",
      " 0825648359 \n",
      " \n",
      " \t0825648359 \n",
      "\n",
      " mother \n",
      " \n",
      " \tmother \n",
      "\n",
      " Robertson \n",
      " \n",
      " \tRobertson \n",
      "\n",
      " 045149682X2 \n",
      " \n",
      " \t045149682X2 \n",
      "\n",
      " Electronically Signed \n",
      " \n",
      " \tElectronically Signed \n",
      "\n",
      " 0834540900 \n",
      " \n",
      " \t0834540900 \n",
      "\n",
      " mohlokohlong S mokomo \n",
      " \n",
      " \tmohlokahlong S mokomo \n",
      "\n",
      " 6908130222088 \n",
      " \n",
      " \t6908130222084 \n",
      "\n",
      " Sarel David Them \n",
      " \n",
      " \tSarel David Theron \n",
      "\n",
      " NA \n",
      " \n",
      " \tNA \n",
      "\n",
      " 02-20-18 \n",
      " \n",
      " \t02-20-18 \n",
      "\n",
      " 2 \n",
      " \n",
      " \t2 \n",
      "\n",
      " m75.102 \n",
      " \n",
      " \tm75.102 \n",
      "\n",
      " Michael Milne \n",
      " \n",
      " \tMichael Milne \n",
      "\n",
      " 2-12-18 \n",
      " \n",
      " \t2-12-18 \n",
      "\n",
      " 4/20/18 \n",
      " \n",
      " \t4/20/18 \n",
      "\n",
      " Nalioalists and 20 Uniced party candi- \n",
      " \n",
      " \tNationalists and 20 United party candi- \n",
      "\n",
      " thhent Bill whet bought life prers inte eristence , they \n",
      " \n",
      " \tment Bill which brought life peers into existence , they \n",
      "\n",
      " He shid bluntly in Washington yestedny \n",
      " \n",
      " \tHe said bluntly in Washington yesterday \n",
      "\n",
      " (S92.412D) \n",
      " \n",
      " \t(S92.412D) \n",
      "\n",
      " 2 \n",
      " \n",
      " \t2 \n",
      "\n",
      " 10/01/2008 \n",
      " \n",
      " \t10/01/2008 \n",
      "\n",
      " \" \" \n",
      " \n",
      " \t\" \" \n",
      "\n",
      " 470521 0002 085 \n",
      " \n",
      " \t470521 0002 085 \n",
      "\n",
      " 33,4 \n",
      " \n",
      " \t33,4 \n",
      "\n",
      " Mr. Brown , parsionate and worm-hearted , lnd \n",
      " \n",
      " \tMr. Brown , passionate and warm-hearted , led \n",
      "\n",
      " 1 \n",
      " \n",
      " \t1 \n",
      "\n",
      " 32159 \n",
      " \n",
      " \t32159 \n",
      "\n",
      " create the atmosphere of a city . \n",
      " \n",
      " \tcreate the atmosphere of a city . \n",
      "\n",
      " Amrit Khalsa MD. \n",
      " \n",
      " \tAmrit Khalsa M.D. \n",
      "\n",
      " 30 \n",
      " \n",
      " \t30 \n",
      "\n",
      " RSA \n",
      " \n",
      " \tRSA \n",
      "\n",
      " Standard Tmst (Maulene Von Blerk), \n",
      " \n",
      " \tStandard Twist (Marlene Von Blerk) \n",
      "\n",
      " appearance of nonlar ) ; and ( 4 ) an attitude to bean \n",
      " \n",
      " \tappearance of mortar ) ; and ( 4 ) an attitude to lean \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000000\n",
    "\n",
    "OCR_data = os.path.join(data_path, 'handwritten_output.txt')\n",
    "input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len=10000, min_sent_len=0, delimiter='|', gt_index=0, prediction_index=1)\n",
    "\n",
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(100):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: is insisting on a policy of change .\n",
      "Word Decoded sentence: is insisting on a policy of change . \n",
      "Input sentence: 12/29/17\n",
      "Word Decoded sentence: 12/29/17 \n",
      "Input sentence: SIAL)TH\n",
      "Word Decoded sentence: SIAL)TH \n",
      "Input sentence: Arcadia CA 91007\n",
      "Word Decoded sentence: Arcadia CA 91007 \n",
      "Input sentence: (012) 667 9375\n",
      "Word Decoded sentence: (012) 667 9375 \n",
      "Input sentence: In this 200-fathom trench the herring do not tood the botton .\n",
      "Word Decoded sentence: In this 200-fathom trench the herring do not good the cotton . \n",
      "Input sentence: 43638556X1\n",
      "Word Decoded sentence: 43638556X1 \n",
      "Input sentence: Pretoria\n",
      "Word Decoded sentence: Pretoria \n",
      "Input sentence: fiddaling about with bils of cost .\n",
      "Word Decoded sentence: fiddling about with bill of cost . \n",
      "Input sentence: ( Fig. 3) . Loop threed tound liite finger t\n",
      "Word Decoded sentence: ( Fig 3) . Loop three Tound lite finger t \n",
      "Input sentence: 200681383\n",
      "Word Decoded sentence: 200681383 \n",
      "Input sentence: for a working week of 34 to 36 houns .\n",
      "Word Decoded sentence: for a working week of 34 to 36 hours . \n",
      "Input sentence: Daugher\n",
      "Word Decoded sentence: Daughter \n",
      "Input sentence: Electronically Signed\n",
      "Word Decoded sentence: Electronically Signed \n",
      "Input sentence: 15122\n",
      "Word Decoded sentence: 15122 \n",
      "Input sentence: 50\n",
      "Word Decoded sentence: 50 \n",
      "Input sentence: ShE WAS MOVVD A Picvic TablE TO SWEAP leAveS And DROREd iT oN hER BSC\n",
      "Word Decoded sentence: she WAS MOVED A Picric table TO SWEAT leaves And Droped it on her BSC \n",
      "Input sentence: 0724603309\n",
      "Word Decoded sentence: 0724603309 \n",
      "Input sentence: lwas car ping a box into the basement Nhen ) sipped on the stairs and landed on my loft ba toe .\n",
      "Word Decoded sentence: was car ping a box into the basement When ) sipped on the stairs and landed on my loft ba toe . \n",
      "Input sentence: Leconte ER\n",
      "Word Decoded sentence: Leconte ER \n",
      "Input sentence: 715-832-0520\n",
      "Word Decoded sentence: 715-832-0520 \n",
      "Input sentence: 7\n",
      "Word Decoded sentence: 7 \n",
      "Input sentence: 1/26/18\n",
      "Word Decoded sentence: 1/26/18 \n",
      "Input sentence: 004-47788-187052\n",
      "Word Decoded sentence: 004-47788-187052 \n",
      "Input sentence: 03-05-18\n",
      "Word Decoded sentence: 03-05-18 \n",
      "Input sentence: 100713 5242 083\n",
      "Word Decoded sentence: 100713 5242 083 \n",
      "Input sentence: is to be made at a meeting of Labour\n",
      "Word Decoded sentence: is to be made at a meeting of Labour \n",
      "Input sentence: 02/27/18\n",
      "Word Decoded sentence: 02/27/18 \n",
      "Input sentence: Governement . Immediately Mr. Rennely rushed a letter\n",
      "Word Decoded sentence: Government . Immediately Mrp Densely rushed a letter \n",
      "Input sentence: 01-06-2018\n",
      "Word Decoded sentence: 01-06-2018 \n",
      "Input sentence: United weight beaning/walking\n",
      "Word Decoded sentence: United weight beaning/walking \n",
      "Input sentence: 6406085079088\n",
      "Word Decoded sentence: 6406085079088 \n",
      "Input sentence: and unconttilutlonal \" interferonce by\n",
      "Word Decoded sentence: and unconttilutlonal \" interference by \n",
      "Input sentence: (412)267-8040\n",
      "Word Decoded sentence: (412)267-8040 \n",
      "Input sentence: Senator Allen Elender , of louisiana ,\n",
      "Word Decoded sentence: Senator Allen Slender , of louisiana , \n",
      "Input sentence: monique Norman\n",
      "Word Decoded sentence: Monique Norman \n",
      "Input sentence: Artham EWALD\n",
      "Word Decoded sentence: Artha EWALD \n",
      "Input sentence: Butn & Woud CalR\n",
      "Word Decoded sentence: But a Would call \n",
      "Input sentence: 2\n",
      "Word Decoded sentence: 2 \n",
      "Input sentence: Mphe 17 Monshioagae\n",
      "Word Decoded sentence: Mihe 17 Monshioagae \n",
      "Input sentence: Seven Commonwealth countiies hve told Mr. Jandys ,\n",
      "Word Decoded sentence: Seven Commonwealth countries he told Mrp Jandy , \n",
      "Input sentence: and again and again it is the visual qualities\n",
      "Word Decoded sentence: and again and again it is the visual qualities \n",
      "Input sentence: 061 360 0643\n",
      "Word Decoded sentence: 061 360 0643 \n",
      "Input sentence: and peveals then the required amonns\n",
      "Word Decoded sentence: and reveals then the required among \n",
      "Input sentence: qaontity surveyor . We have descended\n",
      "Word Decoded sentence: quantity surveyor . We have descended \n",
      "Input sentence: 3\n",
      "Word Decoded sentence: 3 \n",
      "Input sentence: WARBLEA STR. 27 SOmERSET wAS\n",
      "Word Decoded sentence: WARBLED STRA 27 Somerset was \n",
      "Input sentence: Frractue Ostal\n",
      "Word Decoded sentence: Fracture Postal \n",
      "Input sentence: oton has been boycotbed by the the man\n",
      "Word Decoded sentence: ton has been boycotted by the the man \n",
      "Input sentence: South-Africa\n",
      "Word Decoded sentence: South-Africa \n",
      "Input sentence: N/A\n",
      "Word Decoded sentence: Na \n",
      "Input sentence: 11A\n",
      "Word Decoded sentence: 11A \n",
      "Input sentence: 432-262-2440\n",
      "Word Decoded sentence: 432-262-2440 \n",
      "Input sentence: 070624 6132 080\n",
      "Word Decoded sentence: 070624 6132 080 \n",
      "Input sentence: 0\n",
      "Word Decoded sentence: 0 \n",
      "Input sentence: 8\n",
      "Word Decoded sentence: 8 \n",
      "Input sentence: vandyksandra23(a)gmail.com\n",
      "Word Decoded sentence: vandyksandra23(a)gmail.com \n",
      "Input sentence: OV\n",
      "Word Decoded sentence: OV \n",
      "Input sentence: Deict Mantz\n",
      "Word Decoded sentence: Edict Mintz \n",
      "Input sentence: 087) 3451380\n",
      "Word Decoded sentence: 087) 3451380 \n",
      "Input sentence: food scheme . It was maintained dwring\n",
      "Word Decoded sentence: food scheme . It was maintained during \n",
      "Input sentence: that he proved to be to the Jees frreing fo\n",
      "Word Decoded sentence: that he proved to be to the Jees freeing fo \n",
      "Input sentence: Gabarel Adam Viles\n",
      "Word Decoded sentence: Gabarel Adam Miles \n",
      "Input sentence: whiplash- S13.9X85\n",
      "Word Decoded sentence: whiplash S13.9X85 \n",
      "Input sentence: 9:15\n",
      "Word Decoded sentence: 9:15 \n",
      "Input sentence: 3/2/2018\n",
      "Word Decoded sentence: 3/2/2018 \n",
      "Input sentence: 0825648359\n",
      "Word Decoded sentence: 0825648359 \n",
      "Input sentence: mother\n",
      "Word Decoded sentence: mother \n",
      "Input sentence: Robertson\n",
      "Word Decoded sentence: Robertson \n",
      "Input sentence: 045149682X2\n",
      "Word Decoded sentence: 045149682X2 \n",
      "Input sentence: Electronically Signed\n",
      "Word Decoded sentence: Electronically Signed \n",
      "Input sentence: 0834540900\n",
      "Word Decoded sentence: 0834540900 \n",
      "Input sentence: mohlokohlong S mokomo\n",
      "Word Decoded sentence: mohlokohlong S pokomo \n",
      "Input sentence: 6908130222088\n",
      "Word Decoded sentence: 6908130222088 \n",
      "Input sentence: Sarel David Them\n",
      "Word Decoded sentence: Parel David Them \n",
      "Input sentence: NA\n",
      "Word Decoded sentence: NA \n",
      "Input sentence: 02-20-18\n",
      "Word Decoded sentence: 02-20-18 \n",
      "Input sentence: 2\n",
      "Word Decoded sentence: 2 \n",
      "Input sentence: m75.102\n",
      "Word Decoded sentence: m75.102 \n",
      "Input sentence: Michael Milne\n",
      "Word Decoded sentence: Michael Milne \n",
      "Input sentence: 2-12-18\n",
      "Word Decoded sentence: 2-12-18 \n",
      "Input sentence: 4/20/18\n",
      "Word Decoded sentence: 4/20/18 \n",
      "Input sentence: Nalioalists and 20 Uniced party candi-\n",
      "Word Decoded sentence: Nationalists and 20 Uniced party candid \n",
      "Input sentence: thhent Bill whet bought life prers inte eristence , they\n",
      "Word Decoded sentence: then Bill whet bought life press inte existence , they \n",
      "Input sentence: He shid bluntly in Washington yestedny\n",
      "Word Decoded sentence: He said bluntly in Washington yesterday \n",
      "Input sentence: (S92.412D)\n",
      "Word Decoded sentence: (S92.412D) \n",
      "Input sentence: 2\n",
      "Word Decoded sentence: 2 \n",
      "Input sentence: 10/01/2008\n",
      "Word Decoded sentence: 10/01/2008 \n",
      "Input sentence: \" \"\n",
      "Word Decoded sentence: \" \" \n",
      "Input sentence: 470521 0002 085\n",
      "Word Decoded sentence: 470521 0002 085 \n",
      "Input sentence: 33,4\n",
      "Word Decoded sentence: 33,4 \n",
      "Input sentence: Mr. Brown , parsionate and worm-hearted , lnd\n",
      "Word Decoded sentence: Mrp Brown , passionate and worm-hearted , and \n",
      "Input sentence: 1\n",
      "Word Decoded sentence: 1 \n",
      "Input sentence: 32159\n",
      "Word Decoded sentence: 32159 \n",
      "Input sentence: create the atmosphere of a city .\n",
      "Word Decoded sentence: create the atmosphere of a city . \n",
      "Input sentence: Amrit Khalsa MD.\n",
      "Word Decoded sentence: Amrit Khalsa MDI \n",
      "Input sentence: 30\n",
      "Word Decoded sentence: 30 \n",
      "Input sentence: RSA\n",
      "Word Decoded sentence: RSA \n",
      "Input sentence: Standard Tmst (Maulene Von Blerk),\n",
      "Word Decoded sentence: Standard Test (Maulene Von Blerk), \n",
      "Input sentence: appearance of nonlar ) ; and ( 4 ) an attitude to bean\n",
      "Word Decoded sentence: appearance of nonpar ) ; and ( 4 ) an attitude to bean \n",
      "Input sentence: was waiting for his senios militarg\n",
      "Word Decoded sentence: was waiting for his senior military \n",
      "Input sentence: to read on the train . Cotherine i 13\n",
      "Word Decoded sentence: to read on the train . Catherine i 13 \n",
      "Input sentence: 5106205034085\n",
      "Word Decoded sentence: 5106205034085 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: 336-545-5000\n",
      "Word Decoded sentence: 336-545-5000 \n",
      "Input sentence: ()\n",
      "Word Decoded sentence: of \n",
      "Input sentence: as 1830 , when Anglesey believed himself\n",
      "Word Decoded sentence: as 1830 , when Anglesey believed himself \n",
      "Input sentence: gunshot, wound to abdomen\n",
      "Word Decoded sentence: gunshot wound to abdomen \n",
      "Input sentence: Commie of 100 , the anti-nuclear army\n",
      "Word Decoded sentence: Commie of 100 , the antinuclear army \n",
      "Input sentence: NA\n",
      "Word Decoded sentence: NA \n",
      "Input sentence: no\n",
      "Word Decoded sentence: no \n",
      "Input sentence: 9321 North Oak Traffieway\n",
      "Word Decoded sentence: 9321 North Oak Trafficway \n",
      "Input sentence: 3:00 P\n",
      "Word Decoded sentence: 3:00 P \n",
      "Input sentence: M. Macleod thought the two Rhodesian\n",
      "Word Decoded sentence: My Macleod thought the two Rhodesian \n",
      "Input sentence: 3-9-18\n",
      "Word Decoded sentence: 3-9-18 \n",
      "Input sentence: 20/07/2018\n",
      "Word Decoded sentence: 20/07/2018 \n",
      "Input sentence: 045154387X2\n",
      "Word Decoded sentence: 045154387X2 \n",
      "Input sentence: Tylendl 325ig\n",
      "Word Decoded sentence: Tylenol 325ig \n",
      "Input sentence: 9563724X5 41350303X8\n",
      "Word Decoded sentence: 9563724X5 41350303X8 \n",
      "Input sentence: Gg Molre Ryne\n",
      "Word Decoded sentence: Gg More Rye \n",
      "Input sentence: 01-05-2016\n",
      "Word Decoded sentence: 01-05-2016 \n",
      "Input sentence: lengths of 8 ft. by 4 ft. ply for this\n",
      "Word Decoded sentence: lengths of 8 ftp by 4 ftp ply for this \n",
      "Input sentence: in touch in Lomon . Three of them- Canaoa , Australiay\n",
      "Word Decoded sentence: in touch in Lemon . Three of them Canada , Australia \n",
      "Input sentence: 2/13/18\n",
      "Word Decoded sentence: 2/13/18 \n",
      "Input sentence: 1:08-2018\n",
      "Word Decoded sentence: 1:08-2018 \n",
      "Input sentence: tos create the atmosphere of a city .\n",
      "Word Decoded sentence: tos create the atmosphere of a city . \n",
      "Input sentence: unemobyment , but there will be 21\n",
      "Word Decoded sentence: unemobyment , but there will be 21 \n",
      "Input sentence: 2/8/2018\n",
      "Word Decoded sentence: 2/8/2018 \n",
      "Input sentence: ABIGAIL NVABAD2A\n",
      "Word Decoded sentence: ABIGAIL NVABAD2A \n",
      "Input sentence: 95\n",
      "Word Decoded sentence: 95 \n",
      "Input sentence: 11714955X9\n",
      "Word Decoded sentence: 11714955X9 \n",
      "Input sentence: 999024\n",
      "Word Decoded sentence: 999024 \n",
      "Input sentence: Sent 10:26 27/01/1\n",
      "Word Decoded sentence: Sent 10:26 27/01/1 \n",
      "Input sentence: Americans say Giermany is having it too\n",
      "Word Decoded sentence: Americans say Germany is having it too \n",
      "Input sentence: being limited or an adjutment being made\n",
      "Word Decoded sentence: being limited or an adjustment being made \n",
      "Input sentence: 2/25/18\n",
      "Word Decoded sentence: 2/25/18 \n",
      "Input sentence: Mis Nelaney of the script , and the great advantages\n",
      "Word Decoded sentence: Mis Delaney of the script , and the great advantages \n",
      "Input sentence: 1906-09-03\n",
      "Word Decoded sentence: 1906-09-03 \n",
      "Input sentence: 30 7 18\n",
      "Word Decoded sentence: 30 7 18 \n",
      "Input sentence: a man to mate sinple , stright- fervard thngs , and in\n",
      "Word Decoded sentence: a man to mate single , stright forward things , and in \n",
      "Input sentence: Wili be 11 German divisions in Nate\n",
      "Word Decoded sentence: Wili be 11 German divisions in Nate \n",
      "Input sentence: 216 N, Broad St\n",
      "Word Decoded sentence: 216 No Broad St \n",
      "Input sentence: Dr.Garg\n",
      "Word Decoded sentence: Dr.Garg \n",
      "Input sentence: 04/03/18\n",
      "Word Decoded sentence: 04/03/18 \n",
      "Input sentence: 27 SIXTH AVENUE, ESSENWEOD, 4001\n",
      "Word Decoded sentence: 27 SIXTH AVENUE ESSENWEOD, 4001 \n",
      "Input sentence: 02-3-2018\n",
      "Word Decoded sentence: 02-3-2018 \n",
      "Input sentence: Gerecee Thdmitted he did nat hnow\n",
      "Word Decoded sentence: Gerecee Admitted he did nat now \n",
      "Input sentence: 2\n",
      "Word Decoded sentence: 2 \n",
      "Input sentence: Dec 14, 2017\n",
      "Word Decoded sentence: Dec 14, 2017 \n",
      "Input sentence: 044766389X9\n",
      "Word Decoded sentence: 044766389X9 \n",
      "Input sentence: Type 11 Biabetes Mellitur\n",
      "Word Decoded sentence: Type 11 Diabetes Mellitus \n",
      "Input sentence: Orthopedic Surgery\n",
      "Word Decoded sentence: Orthopedic Surgery \n",
      "Input sentence: held last antomn , 38 per cent for the\n",
      "Word Decoded sentence: held last Anton , 38 per cent for the \n",
      "Input sentence: as needed\n",
      "Word Decoded sentence: as needed \n",
      "Input sentence: 102211000747\n",
      "Word Decoded sentence: 102211000747 \n",
      "Input sentence: of tin Ray Welenstyy , Pime Minister of the\n",
      "Word Decoded sentence: of tin Ray Welenstyy , Time Minister of the \n",
      "Input sentence: pinned into place . A similar piece of 1 in.\n",
      "Word Decoded sentence: pinned into place . A similar piece of 1 in \n",
      "Input sentence: the puarmacentical industry . \" The health\n",
      "Word Decoded sentence: the puarmacentical industry . \" The health \n",
      "Input sentence: 247-880-4201\n",
      "Word Decoded sentence: 247-880-4201 \n",
      "Input sentence: 20/1/18\n",
      "Word Decoded sentence: 20/1/18 \n",
      "Input sentence: mD\n",
      "Word Decoded sentence: md \n",
      "Input sentence: 3/6/18\n",
      "Word Decoded sentence: 3/6/18 \n",
      "Input sentence: n 7a OffeV1\n",
      "Word Decoded sentence: n 7a OffeV1 \n",
      "Input sentence: derived from this unity of conception and\n",
      "Word Decoded sentence: derived from this unity of conception and \n",
      "Input sentence: waat sus crtrin is that thoir who\n",
      "Word Decoded sentence: what sus citrin is that their who \n",
      "Input sentence: 9/12/18\n",
      "Word Decoded sentence: 9/12/18 \n",
      "Input sentence: Dr Kink Hutton\n",
      "Word Decoded sentence: Dr Kink Hutton \n",
      "Input sentence: 298.1\n",
      "Word Decoded sentence: 298.1 \n",
      "Input sentence: MD\n",
      "Word Decoded sentence: MD \n",
      "Input sentence: (517) 205-1713\n",
      "Word Decoded sentence: (517) 205-1713 \n",
      "Input sentence: the derived from this inity of conception and\n",
      "Word Decoded sentence: the derived from this unity of conception and \n",
      "Input sentence: SLSANNA M CONRAOR\n",
      "Word Decoded sentence: SUSANNA M CONRAOR \n",
      "Input sentence: 4-19-18\n",
      "Word Decoded sentence: 4-19-18 \n",
      "Input sentence: Macmillan at Chequers .\n",
      "Word Decoded sentence: Macmillan at Chequers . \n",
      "Input sentence: 10/17/17\n",
      "Word Decoded sentence: 10/17/17 \n",
      "Input sentence: 6tic douloureus . As early as 1830 , when\n",
      "Word Decoded sentence: 6tic douloureus . As early as 1830 , when \n",
      "Input sentence: 045141073X2\n",
      "Word Decoded sentence: 045141073X2 \n",
      "Input sentence: 2/28/18\n",
      "Word Decoded sentence: 2/28/18 \n",
      "Input sentence: 2/27/18\n",
      "Word Decoded sentence: 2/27/18 \n",
      "Input sentence: apartend as evil and indefemsible .\n",
      "Word Decoded sentence: parted as evil and indefeasible . \n",
      "Input sentence: other man was a priest : hre the priest is 1superceded by\n",
      "Word Decoded sentence: other man was a priest : HRE the priest is 1superceded by \n",
      "Input sentence: 050903 532 8087\n",
      "Word Decoded sentence: 050903 532 8087 \n",
      "Input sentence: aoerd in his nythmae world . It is this that\n",
      "Word Decoded sentence: word in his nythmae world . It is this that \n",
      "Input sentence: 5'3\"\n",
      "Word Decoded sentence: 5'3\" \n",
      "Input sentence: Kubashnie) blveskyogislics.co.za.\n",
      "Word Decoded sentence: Kubashnie) blveskyogislics.co.za. \n",
      "Input sentence: 2/13/18\n",
      "Word Decoded sentence: 2/13/18 \n",
      "Input sentence: Kobus(2)hams-schelbema.co.za\n",
      "Word Decoded sentence: Kobus(2)hams-schelbema.co.za \n",
      "Input sentence: US Hospital Dr Sute 107\n",
      "Word Decoded sentence: US Hospital Dr Sure 107 \n",
      "Input sentence: Powards the end of 1950 , Mr. Dandel Grant ,\n",
      "Word Decoded sentence: Towards the end of 1950 , Mrp Jandel Grant , \n",
      "Input sentence: wound Care\n",
      "Word Decoded sentence: wound Care \n",
      "Input sentence: SOw2 oland Dreves Trevethin, Pontypool NP 68 MD, United Kingdom\n",
      "Word Decoded sentence: SOw2 Oland Drives Trevethin, Pontypool NP 68 MDI United Kingdom \n",
      "Input sentence: Hughes in his Make Mine Nusic \" ( BBC , 930 p.m. ) .\n",
      "Word Decoded sentence: Hughes in his Make Mine Music \" ( BBC , 930 pm ) . \n",
      "Input sentence: CC\n",
      "Word Decoded sentence: CC \n",
      "Input sentence: 2-16-18\n",
      "Word Decoded sentence: 2-16-18 \n",
      "Input sentence: reported on Mr. Weaver . He beiered\n",
      "Word Decoded sentence: reported on Mrp Weaver . He briered \n",
      "Input sentence: 3/8/18\n",
      "Word Decoded sentence: 3/8/18 \n",
      "Input sentence: Fefasing to sit round the conference table .\n",
      "Word Decoded sentence: Feasing to sit round the conference table . \n",
      "Input sentence: P.O. Box 8012 Gneenstone 1616\n",
      "Word Decoded sentence: Poor Box 8012 Greenstone 1616 \n",
      "Input sentence: DR. KeiTh Helton\n",
      "Word Decoded sentence: DRY Keith Heston \n",
      "Input sentence: 02/27/18\n",
      "Word Decoded sentence: 02/27/18 \n",
      "Input sentence: NtP\n",
      "Word Decoded sentence: NTP \n",
      "Input sentence: 4\n",
      "Word Decoded sentence: 4 \n",
      "Input sentence: cintence Datahase 03-189\n",
      "Word Decoded sentence: intense Database 03-189 \n",
      "Input sentence: In the Pollowing year , 1380 , the last and\n",
      "Word Decoded sentence: In the Following year , 1380 , the last and \n",
      "Input sentence: 402-609-1500\n",
      "Word Decoded sentence: 402-609-1500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Karnes, Jonathon L.\n",
      "Word Decoded sentence: Earnest Jonathon Ll \n",
      "Input sentence: 2/14/18\n",
      "Word Decoded sentence: 2/14/18 \n",
      "Input sentence: Julie Vandee Werff PAC\n",
      "Word Decoded sentence: Julie Candee Werf PAC \n",
      "Input sentence: lost 2days\n",
      "Word Decoded sentence: lost 2days \n",
      "Input sentence: 6305270199083\n",
      "Word Decoded sentence: 6305270199083 \n",
      "Input sentence: 4541A\n",
      "Word Decoded sentence: 4541A \n",
      "Input sentence: 12-15-17\n",
      "Word Decoded sentence: 12-15-17 \n",
      "Input sentence: services was wholly difterent - findamentally\n",
      "Word Decoded sentence: services was wholly different - fundamentally \n",
      "Input sentence: ben uhu()mweb.co.za.\n",
      "Word Decoded sentence: ben uhu()mweb.co.za. \n",
      "Input sentence: the 6tic douloureux . As early as 1830 , whan\n",
      "Word Decoded sentence: the 6tic douloureux . As early as 1830 , whan \n",
      "Input sentence: in fighting . Pesterday the shirs turned away\n",
      "Word Decoded sentence: in fighting . Yesterday the shirs turned away \n",
      "Input sentence: 10\n",
      "Word Decoded sentence: 10 \n",
      "Input sentence: Mr. Macmillan at Chequers .\n",
      "Word Decoded sentence: Mrp Macmillan at Chequers . \n",
      "Input sentence: memters THE twe meul Afrec Mahimlit\n",
      "Word Decoded sentence: members THE the meal Afric Mahimlit \n",
      "Input sentence: 7211095046088\n",
      "Word Decoded sentence: 7211095046088 \n",
      "Input sentence: (038) 231 104\n",
      "Word Decoded sentence: (038) 231 104 \n",
      "Input sentence: Electronically Signed\n",
      "Word Decoded sentence: Electronically Signed \n",
      "Input sentence: RSA\n",
      "Word Decoded sentence: RSA \n",
      "Input sentence: 1Son\n",
      "Word Decoded sentence: 1Son \n",
      "Input sentence: Flynn, Arthur E.\n",
      "Word Decoded sentence: Flynn Arthur E \n",
      "Input sentence: MD\n",
      "Word Decoded sentence: MD \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_text in input_texts:\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    #print(input_text)\n",
    "    pre_corrected_sentence = word_spell_correct(input_text)\n",
    "    #print(pre_corrected_sentence)\n",
    "    \n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "\n",
    "\n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "\n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    #print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    #print('Spell Decoded sentence:', pre_corrected_sentence) \n",
    "    #print('Char Decoded sentence:', decoded_sentence)   \n",
    "    \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    #print(corrected_sentence) \n",
    "    #print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(gt_texts, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_word_correction = calculate_WER(gt_texts, corrected_sentences)\n",
    "print('WER_spell_word_correction |TEST= ', WER_spell_word_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(gt_texts, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
