{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_terms_with_noise(json_file, num_samples, noise_threshold):\n",
    "    with open(json_file) as f:\n",
    "        med_terms_dict = json.load(f)\n",
    "    med_terms = list(med_terms_dict.keys())\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    cnt = 0\n",
    "    while cnt < num_samples:\n",
    "        for term in med_terms:\n",
    "            if cnt < num_samples :\n",
    "                input_text = noise_maker(term, noise_threshold)\n",
    "                input_text = input_text[:-1]   \n",
    "\n",
    "                target_text = '\\t' + term + '\\n'\n",
    "\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])        \n",
    "                cnt += 1\n",
    "    return input_texts, target_texts, gt_texts, med_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accidents_terms_with_noise(file_name, limit, num_samples, noise_threshold):\n",
    "\n",
    "    f = open(file_name, encoding='utf8')\n",
    "    line = 0    \n",
    "    med_terms = []\n",
    "    try:\n",
    "        for r in f:\n",
    "            if(line < limit):\n",
    "\n",
    "                med_terms.extend(r.split('|'))\n",
    "                line += 1\n",
    "    except:\n",
    "        print('finished')\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    cnt = 0\n",
    "    while cnt < num_samples:\n",
    "        for term in med_terms:\n",
    "            if cnt < num_samples :\n",
    "                input_text = noise_maker(term, noise_threshold)\n",
    "                input_text = input_text[:-1]   \n",
    "\n",
    "                target_text = '\\t' + term + '\\n'\n",
    "\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])        \n",
    "                cnt += 1\n",
    "                \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_procedures_tests_with_noise(file_name, num_samples, noise_threshold):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                \n",
    "                input_text = noise_maker(row, noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + row + '\\n'            \n",
    "\n",
    "                cnt += 1\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', 'â€”' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    print(decoder_outputs)\n",
    "    print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    print(encoder_inputs)\n",
    "    print(encoder_outputs)\n",
    "    print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 1000000\n",
    "min_sent_len = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of lenghts\n",
    "lengths = []\n",
    "for text in input_texts:\n",
    "    lengths.append(len(text))\n",
    "    lengths.append(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc102a8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = plt.hist(lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1180.,   940.,  1384.,  1322.,  1174.,   722.,   592.,   536.,\n",
       "         332.,   286.,   242.,   186.,   180.,   214.,   112.,   154.,\n",
       "          68.,    82.,    58.,    88.,    70.,    64.,    32.,    30.,\n",
       "          20.,    26.,    32.,    24.,    58.,    14.,    64.,     6.,\n",
       "          28.,    16.,    24.,    28.,     8.,    18.,    14.,    18.,\n",
       "          12.,    24.,    14.,    28.,    14.,     4.,    12.,    44.,\n",
       "           4.,     6.,     2.,     4.,     6.,     0.,    36.,     0.,\n",
       "           4.,     4.,     8.,     6.,    14.,     8.,     8.,     8.,\n",
       "           2.,     0.,     6.,     2.,     2.,     4.,    12.,    14.,\n",
       "           8.,    12.,     6.,     0.,     4.,     4.,     2.,     0.,\n",
       "           2.,     0.,     4.,     6.,     0.,     4.,    14.,    26.,\n",
       "           4.,     0.,     2.,     4.,     4.,     2.,     4.,     2.,\n",
       "           6.,     0.,     2.,     2.,     2.,     6.,     4.,     2.,\n",
       "          40.,     4.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,    20.,     8.,    18.,     4.,     0.,\n",
       "           0.,     0.,     0.,     6.,    24.,     2.,     0.,     0.,\n",
       "           0.,     2.,     2.,     0.,     0.,     2.,     2.,     4.,\n",
       "           2.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     4.,     4.,\n",
       "          16.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     4.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     2.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   ,     4.878,     9.756,    14.634,    19.512,    24.39 ,\n",
       "          29.268,    34.146,    39.024,    43.902,    48.78 ,    53.658,\n",
       "          58.536,    63.414,    68.292,    73.17 ,    78.048,    82.926,\n",
       "          87.804,    92.682,    97.56 ,   102.438,   107.316,   112.194,\n",
       "         117.072,   121.95 ,   126.828,   131.706,   136.584,   141.462,\n",
       "         146.34 ,   151.218,   156.096,   160.974,   165.852,   170.73 ,\n",
       "         175.608,   180.486,   185.364,   190.242,   195.12 ,   199.998,\n",
       "         204.876,   209.754,   214.632,   219.51 ,   224.388,   229.266,\n",
       "         234.144,   239.022,   243.9  ,   248.778,   253.656,   258.534,\n",
       "         263.412,   268.29 ,   273.168,   278.046,   282.924,   287.802,\n",
       "         292.68 ,   297.558,   302.436,   307.314,   312.192,   317.07 ,\n",
       "         321.948,   326.826,   331.704,   336.582,   341.46 ,   346.338,\n",
       "         351.216,   356.094,   360.972,   365.85 ,   370.728,   375.606,\n",
       "         380.484,   385.362,   390.24 ,   395.118,   399.996,   404.874,\n",
       "         409.752,   414.63 ,   419.508,   424.386,   429.264,   434.142,\n",
       "         439.02 ,   443.898,   448.776,   453.654,   458.532,   463.41 ,\n",
       "         468.288,   473.166,   478.044,   482.922,   487.8  ,   492.678,\n",
       "         497.556,   502.434,   507.312,   512.19 ,   517.068,   521.946,\n",
       "         526.824,   531.702,   536.58 ,   541.458,   546.336,   551.214,\n",
       "         556.092,   560.97 ,   565.848,   570.726,   575.604,   580.482,\n",
       "         585.36 ,   590.238,   595.116,   599.994,   604.872,   609.75 ,\n",
       "         614.628,   619.506,   624.384,   629.262,   634.14 ,   639.018,\n",
       "         643.896,   648.774,   653.652,   658.53 ,   663.408,   668.286,\n",
       "         673.164,   678.042,   682.92 ,   687.798,   692.676,   697.554,\n",
       "         702.432,   707.31 ,   712.188,   717.066,   721.944,   726.822,\n",
       "         731.7  ,   736.578,   741.456,   746.334,   751.212,   756.09 ,\n",
       "         760.968,   765.846,   770.724,   775.602,   780.48 ,   785.358,\n",
       "         790.236,   795.114,   799.992,   804.87 ,   809.748,   814.626,\n",
       "         819.504,   824.382,   829.26 ,   834.138,   839.016,   843.894,\n",
       "         848.772,   853.65 ,   858.528,   863.406,   868.284,   873.162,\n",
       "         878.04 ,   882.918,   887.796,   892.674,   897.552,   902.43 ,\n",
       "         907.308,   912.186,   917.064,   921.942,   926.82 ,   931.698,\n",
       "         936.576,   941.454,   946.332,   951.21 ,   956.088,   960.966,\n",
       "         965.844,   970.722,   975.6  ,   980.478,   985.356,   990.234,\n",
       "         995.112,   999.99 ,  1004.868,  1009.746,  1014.624,  1019.502,\n",
       "        1024.38 ,  1029.258,  1034.136,  1039.014,  1043.892,  1048.77 ,\n",
       "        1053.648,  1058.526,  1063.404,  1068.282,  1073.16 ,  1078.038,\n",
       "        1082.916,  1087.794,  1092.672,  1097.55 ,  1102.428,  1107.306,\n",
       "        1112.184,  1117.062,  1121.94 ,  1126.818,  1131.696,  1136.574,\n",
       "        1141.452,  1146.33 ,  1151.208,  1156.086,  1160.964,  1165.842,\n",
       "        1170.72 ,  1175.598,  1180.476,  1185.354,  1190.232,  1195.11 ,\n",
       "        1199.988,  1204.866,  1209.744,  1214.622,  1219.5  ,  1224.378,\n",
       "        1229.256,  1234.134,  1239.012,  1243.89 ,  1248.768,  1253.646,\n",
       "        1258.524,  1263.402,  1268.28 ,  1273.158,  1278.036,  1282.914,\n",
       "        1287.792,  1292.67 ,  1297.548,  1302.426,  1307.304,  1312.182,\n",
       "        1317.06 ,  1321.938,  1326.816,  1331.694,  1336.572,  1341.45 ,\n",
       "        1346.328,  1351.206,  1356.084,  1360.962,  1365.84 ,  1370.718,\n",
       "        1375.596,  1380.474,  1385.352,  1390.23 ,  1395.108,  1399.986,\n",
       "        1404.864,  1409.742,  1414.62 ,  1419.498,  1424.376,  1429.254,\n",
       "        1434.132,  1439.01 ,  1443.888,  1448.766,  1453.644,  1458.522,\n",
       "        1463.4  ,  1468.278,  1473.156,  1478.034,  1482.912,  1487.79 ,\n",
       "        1492.668,  1497.546,  1502.424,  1507.302,  1512.18 ,  1517.058,\n",
       "        1521.936,  1526.814,  1531.692,  1536.57 ,  1541.448,  1546.326,\n",
       "        1551.204,  1556.082,  1560.96 ,  1565.838,  1570.716,  1575.594,\n",
       "        1580.472,  1585.35 ,  1590.228,  1595.106,  1599.984,  1604.862,\n",
       "        1609.74 ,  1614.618,  1619.496,  1624.374,  1629.252,  1634.13 ,\n",
       "        1639.008,  1643.886,  1648.764,  1653.642,  1658.52 ,  1663.398,\n",
       "        1668.276,  1673.154,  1678.032,  1682.91 ,  1687.788,  1692.666,\n",
       "        1697.544,  1702.422,  1707.3  ,  1712.178,  1717.056,  1721.934,\n",
       "        1726.812,  1731.69 ,  1736.568,  1741.446,  1746.324,  1751.202,\n",
       "        1756.08 ,  1760.958,  1765.836,  1770.714,  1775.592,  1780.47 ,\n",
       "        1785.348,  1790.226,  1795.104,  1799.982,  1804.86 ,  1809.738,\n",
       "        1814.616,  1819.494,  1824.372,  1829.25 ,  1834.128,  1839.006,\n",
       "        1843.884,  1848.762,  1853.64 ,  1858.518,  1863.396,  1868.274,\n",
       "        1873.152,  1878.03 ,  1882.908,  1887.786,  1892.664,  1897.542,\n",
       "        1902.42 ,  1907.298,  1912.176,  1917.054,  1921.932,  1926.81 ,\n",
       "        1931.688,  1936.566,  1941.444,  1946.322,  1951.2  ,  1956.078,\n",
       "        1960.956,  1965.834,  1970.712,  1975.59 ,  1980.468,  1985.346,\n",
       "        1990.224,  1995.102,  1999.98 ,  2004.858,  2009.736,  2014.614,\n",
       "        2019.492,  2024.37 ,  2029.248,  2034.126,  2039.004,  2043.882,\n",
       "        2048.76 ,  2053.638,  2058.516,  2063.394,  2068.272,  2073.15 ,\n",
       "        2078.028,  2082.906,  2087.784,  2092.662,  2097.54 ,  2102.418,\n",
       "        2107.296,  2112.174,  2117.052,  2121.93 ,  2126.808,  2131.686,\n",
       "        2136.564,  2141.442,  2146.32 ,  2151.198,  2156.076,  2160.954,\n",
       "        2165.832,  2170.71 ,  2175.588,  2180.466,  2185.344,  2190.222,\n",
       "        2195.1  ,  2199.978,  2204.856,  2209.734,  2214.612,  2219.49 ,\n",
       "        2224.368,  2229.246,  2234.124,  2239.002,  2243.88 ,  2248.758,\n",
       "        2253.636,  2258.514,  2263.392,  2268.27 ,  2273.148,  2278.026,\n",
       "        2282.904,  2287.782,  2292.66 ,  2297.538,  2302.416,  2307.294,\n",
       "        2312.172,  2317.05 ,  2321.928,  2326.806,  2331.684,  2336.562,\n",
       "        2341.44 ,  2346.318,  2351.196,  2356.074,  2360.952,  2365.83 ,\n",
       "        2370.708,  2375.586,  2380.464,  2385.342,  2390.22 ,  2395.098,\n",
       "        2399.976,  2404.854,  2409.732,  2414.61 ,  2419.488,  2424.366,\n",
       "        2429.244,  2434.122,  2439.   ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  9.756\n",
      "Count of most probable lenght =  1384.0\n",
      "Min length =  4.878\n"
     ]
    }
   ],
   "source": [
    "max_sent_len =  h[1][np.argmax(h[0])]\n",
    "min_sent_len = h[1][1]\n",
    "print('Most probable length = ', max_sent_len)\n",
    "print('Count of most probable lenght = ', np.max(h[0]))\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len =  50#int(np.ceil(max_sent_len))\n",
    "min_sent_len = 4#int(np.floor(min_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  50\n",
      "Min length =  4\n"
     ]
    }
   ],
   "source": [
    "print('Most probable length = ', max_sent_len)\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR_tess, target_texts_tess, gt_tess = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "num_samples = 0\n",
    "OCR_data = os.path.join(data_path, 'output_handwritten.txt')\n",
    "input_texts_OCR_hand, target_texts_OCR_hand, gt_texts_OCR_hand = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texts_OCR\n",
    "#target_texts = target_texts_OCR\n",
    "input_texts_OCR = input_texts_OCR_tess + input_texts_OCR_hand\n",
    "target_texts_OCR = target_texts_tess + target_texts_OCR_hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 40000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "threshold = 0.5\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = target_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Medical Terms dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = os.path.join(data_path, 'abbrevs.json')\n",
    "threshold = 0.9\n",
    "num_samples = 0\n",
    "input_texts_MedTerms, target_texts_MedTerms, _, med_terms_dict = load_medical_terms_with_noise(json_file, num_samples, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'det.': 'let it be given', 'p.v.': 'through the vagina', 'PT': 'prothrombin time', 'Ta': 'tantalum', 'lat': 'lateral', 'BCG': 'bacille Calmette-GuÃ©rin', 'MM': 'mucous membrane', 'AK': 'above the knee', 'a.c., ac': 'before a meal', 'KI': 'potassium iodine', 'cg': 'centigram', 'mEq': 'milliequivalent', 'sol': 'solution, dissolved', 'PEFR': 'peak expiratory flow rate', 'MI': 'myocardial infarction', 'BMS': 'bone marrow suppression', 'TPI': '', 'EPS': 'extrapyramidal symptoms', 'O.S.': 'left eye', 'LUE': 'left upper extremity', 'ECG': 'electrocardiogram, electrocardiograph', 'S.E.': 'standard error', 'q.v.': 'as much as you please', 'ECMO': 'extracorporeal membrane oxygenation', 'ACLS': 'advanced cardiac life support', 'PND': 'paroxysmal nocturnal dyspnea', 'ASC-US': 'atypical squamous cells of undetermined significance', 'UV': 'ultraviolet', 'MDI': 'metered-dose inhaler', 'b.i.n.': 'twice a night', 'LLL': 'left lower lobe', 'ad sat.': 'to saturation', 'EIA': 'enzyme immunosorbent assay', 'dieb. tert.': 'every third day', 'BBB': 'blood-brain barrier', 'GnRH': 'gonadotropin-releasing hormone', 'TB': 'tuberculin', 'RAI': 'radioactive iodine', 'PNS': 'peripheral nervous system', 'D and C': 'dilatation and curettage', 'FHT': 'fetal heart tone', 'EP': 'extrapyramidal', 'VOE': 'VistA-Office Electronic Health Record', 'ASA': 'acetylsalicylic acid', 'dim.': 'halved', 'elix.': 'elixir', 'PPD': 'purified protein derivative (TB test)', 'q.2h.': 'every 2 hours', 'TG': 'thyroglobulin', 'GABA': 'gamma-aminobutyric acid', 'CNS': 'central nervous system', 'THR': 'total hip replacement', 'WF/BF': 'white female/black female', 'TUMA': 'transurethral microwave antenna', 'CEA': 'carcinoembryonic antigen', 'TNT': 'trinitrotoluene', 'FBS': 'fasting blood sugar', 'ARC': 'AIDS-related complex', 'at. wt.': 'atomic weight', 'ant.': 'anterior', 'RLL': 'right lower lobe', 'VORB': 'verbal order read back', 'LE': 'lower extremity', 'GDM': 'gestational diabetes mellitus', 'CHF': 'congestive heart failure', 't.d.s.': 'to be taken three times daily', 'cath': 'catheter', 'hor. decub.': 'bedtime', 'Pu': 'plutonium', 'TAH': 'total abdominal hysterectomy', 'ap': 'before dinner', 'Pt': 'platinum', 'ASCA': 'anti-', 'IOP': 'intraocular pressure', 'mist.': 'a mixture', 'OT': 'occupational therapy', 'REM': 'rapid eye movement', 'Tx': 'treatment', 'CLL': 'chronic lymphocytic leukemia', 'COX-2': 'cyclooxygenase 2 inhibitors', 'PAD': 'peripheral arterial disease', 'AIDS': 'acquired immunodeficiency syndrome', 'TURP': 'transurethral resection of the prostate', 'dr.': 'dram', 'ICS': 'intercostal space', 'DBP': 'diastolic blood pressure', 'DPat': 'diphtheria-acellular pertussis tetanus (vaccine)', 'ERCP': 'endoscopic retrograde cholangiopancreatography', 'Em': 'emmetropia', 'US': 'ultrasonic, ultrasound', 'ARDS': 'acute respiratory distress syndrome', 'T6': 'thoracic nerve pair 6', 'MAT': 'Miller Analogies Test', 'post.': 'posterior', 'Sed rate': 'sedimentation rate', 'dB': 'decibel', 'NSAID': 'nonsteroidal anti-inflammatory drug', 'TLC, tlc': 'tender loving care', 'Bi': 'bismuth', 'ASC': 'atypical squamous cells', 'SGA': 'small for gestational age', 'h/o': 'history of', 'tr, tinct.': 'tincture', 'AC': 'adrenal cortex', 'Hz': 'hertz (cycles per second)', 'EST': 'electroshock therapy', 'yo': 'years old', 'RLQ': 'right lower quadrant', 'GDS': 'Geriatric Depression Scale', 'INR': 'international normalized ratio', 'RDA': 'recommended daily/dietary allowance', 'IgG': 'immunoglobulin G', 'N&V, N/V': 'nausea and vomiting', 'FTT': 'failure to thrive', 'DNR': 'do not resuscitate', 'HAV': 'hepatitis A virus', 'Fe': 'iron', 'VZIG': 'varicella zoster immune globulin', 'AML': 'acute myelogenous (myeloblastic) leukemia', 'CPR': 'cardiopulmonary resuscitation', 'Al': 'aluminum', 'inf.': 'inferior', 'ATCC': 'American Type Culture Collection', 'ss': 'a half', 'EBV': 'Epstein-Barr virus', 't.i.n.': 'three times a night', 'ICP': 'intracranial pressure', 'GYN': 'gynecology', 'ROS': 'review of systems', 'USP': 'United States Pharmacopeia', 'RT': 'radiation therapy', 'lmp': 'last menstrual period', 'FUO': 'fever of unknown origin', 'part. vic': 'in divided doses', 'INF': 'interferon', 'GI': 'gastrointestinal', 'CR': 'conditioned reflex', 'IgE': 'immunoglobulin E', 'bpm': 'beats per minute', 'BE': 'barium enema', 'MCV': 'mean corpuscular volume', 'lig': 'ligament', 'NGT': 'nasogastric tube', 'S.D.': 'standard deviation', 'ult. praes.': 'the last ordered', 'IV': 'intravenous', 'SDAT': 'senile dementia of the Alzheimer type', 'NRC': 'normal retinal correspondence', 'TNF-I': 'tumor necrosis factor inhibitor', 'CIS': 'carcinoma in situ', 'ICD': 'implantable cardioverter defibrillator', 'A/CA': 'accommodative/convergence accommodation ratio', 'Co': 'cobalt', 'PFP, P4P': 'pay for performance', 'OCD': 'obsessive-compulsive disorder', 'PACU': 'postanesthesia care unit', 'PUBS': 'percutaneous umbilical blood sampling', 'PE': 'physical examination', 'CHD': 'congenital heart disease', 'NKA': 'no known allergies', 'WM/BM': 'white male/black male', 'NMS': 'neuroleptic malignant syndrome', 'Hy': 'hyperopia', 'BRM': 'biologic response modifier', 'O.D.': 'right eye', 'p.r.n.': 'as needed', 'ICU': 'intensive care unit', 'GVHD': 'graft-versus-host disease', 'RD': 'Raynaud disease', 'DFV': 'Doppler flow velocimetry', 'TPR': 'temperature, pulse, and respiration', 'ung.': 'ointment', 'AICD': 'automatic implantable cardiac defibrillator', 'ESWL': 'extracorporeal shock wave lithotripsy', 'dL': 'deciliter', 'ROM': 'range of motion', 'ABO': 'three basic blood groups', 'Mg': 'magnesium', 'LVAD': 'left ventricular assist device', 'RF': 'rheumatoid factor', 'IED': 'improvised explosive device', 'SNS': 'sympathetic nervous system', 'P, p': 'melting point', 'Cu': 'copper', 't.i.d.': 'three times a day', 'LEEP': 'loop electrosurgical excision procedure', 'Mn': 'manganese', 'PDA': 'patent ductus arteriosus', 'CBC': 'complete blood count', 'HGSIL': 'high-grade squamous intraepithelial lesion', 'LTD': 'lowest tolerated dose', 'TNF-Î±': 'tumor necrosis factor alpha', 'EMG': 'electromyogram, electromyography', 'ICSH': 'interstitial cell-stimulating hormone', 's.q.': 'subcutaneous(ly)', 'F and E': 'fluid and electrolyte', 'EOM': 'extraocular muscles', 'SSRI': 'selective serotonin reuptake inhibitor', 'Vf': 'field of vision', 'MLF': 'medial longitudinal fasciculus', 'SNRI': 'serotonin and norepinephrine reuptake inhibitor', 'AVM': 'arteriovenous malformation', 'DVT': 'deep vein thrombosis', 'HOB': 'head of bed', 'G, g, gm': 'gram', 'OSHA': 'Occupational Safety and Health Administration', 'RUL': 'right upper lobe', 'OOB': 'out of bed', 'FA': 'fatty acid', 'WAP': 'written action plan', 'RPM': 'revolutions per minute', 'CVRB': 'critical value read back', 'A/G': 'albumin/globulin ratio', 'HAART': 'highly active antiretroviral therapy', 'Cal': 'large calorie', 'PCP': '', 'DTs': 'delirium tremens', 'c.n.': 'tomorrow night', 'MELD': 'Model for End-Stage Liver Disease', 'aa': 'of each', 'RE': 'right eye', 'DOE': 'dyspnea on exertion', 'SLP': 'speech-language pathology', 'TAT': 'thematic apperception test', 'UHF': 'ultrahigh frequency', 'S.': 'sacral', 'ME ratio': 'myeloid/erythroid ratio', 'PMH': 'past medical history', 'VA': 'visual acuity', 'SOB': 'shortness of breath', 'TA': 'toxin-antitoxin', 'IUFD': 'intrauterine fetal death', 'PCWP': 'pulmonary capillary wedge pressure', 'PKU': 'phenylketonuria', 'mm': 'millimeter', 'SCI': 'spinal cord injury', 'bol.': 'pill', 'RBC': 'red blood cell', 'CAH': 'chronic active hepatitis', 'EENT': 'eye, ear, nose, and throat', 'IDDM': 'insulin-dependent diabetes mellitus', 'TENS': 'transcutaneous electrical nerve stimulation', 'TN': 'trigeminal nerve', 'PDR': '', 'HSIL': 'high-grade squamous intraepithelial lesion', 'IC': 'inspiratory capacity', 'MAO-B': 'monoamine oxidase-B', 'H&H': 'hematocrit and hemoglobin', 'LGA': 'large for gestational age', 'BMI': 'body mass index', 'LLQ': 'left lower quadrant', 'AD': 'advance directive', 'q.s.': 'as much as needed', 'EGD': 'esophagogastroduodenoscopy', 'CF': 'cystic fibrosis', 'BROW': 'barley, rye, oats, and wheat', 'ABI': 'ankle-brachial index', 'FFP': 'fresh frozen plasma', 'A&P': 'auscultation and percussion', 'wt.': 'weight', 'URI': 'upper respiratory infection', 'GH': 'growth hormone', 'CO': 'carbon monoxide', 'TPN': 'total parenteral nutrition', 'BCP': 'birth control pills', 'cm': 'centimeter', 'ml': 'milliliter', 'PABA': 'para-aminobenzoic acid (vitamin B10)', 'DKA': 'diabetic ketoacidosis', 'HPI': 'history of present illness', 'PID': 'pelvic inflammatory disease', 'a.m.a.': 'against medical advice', 'COLD': 'chronic obstructive lung disease', 'BMR': 'basal metabolic rate', 'LH ': 'luteinizing hormone', 'RUQ': 'right upper quadrant', 'C&S': 'culture and sensitivity', 'asc.': 'ascending', 'B.P.': 'British Pharmacopeia', 'AMI': 'acute myocardial infarction', 'AM': 'morning', 'ACE': 'angiotensin-converting enzyme', 'ALP': 'alkaline phosphatase', 'LUQ': 'left upper quadrant', 'VT': 'ventricular tachycardia', 'TNM': 'tumor-node-metastasis', 'LUL': 'left upper lobe', 'Hib': '', 'BSE': 'breast self-examination', 'AChR': 'acetylcholine receptor', 'PVC': 'premature ventricular contraction', 'Cl': 'chlorine', 'liq.': 'liquid', 'CDC': 'Centers for Disease Control and Prevention', 'TORB': 'telephone order read back', 'DOA': 'dead on arrival', 'EDD': 'estimated date of delivery (formerly EDC: estimated date of confinement)', 'PVR': 'peripheral vascular resistance', 'DNA': 'deoxyribonucleic acid', 'CBI': 'continuous bladder irrigation', 'MRI': 'magnetic resonance imaging', 'Tl': 'thallium', 'A-P': 'anterior-posterior', 'pH': 'hydrogen ion concentration', 'MG': 'myasthenia gravis', 'MS': 'mitral stenosis', 'PCA': 'patient-controlled analgesia', 'DOB': 'date of birth', 'FAP': 'familial adenomatous polyposis', 'NS': 'normal saline', 'IL-8': 'interleukin 8', 'TRAP criteria': 'tremor, rigidity, akinesia or postural instablity bradykinesia, and postural instability', 'Alb': 'albumin', 'PCOS': 'polycystic ovarian syndrome', 'VLDL': 'very low density lipoprotein', 'BBT': 'basal body temperature', 'PSV': 'prostate-specific antigen', 'aq. dest.': 'distilled water', 'Rh': 'rhesus factor', 'CD4': 'T-helper cells', 'tTG': 'antitransglutaminase', 'TKR': 'total knee replacement', 'Zn': 'zinc', 'pil.': 'pill', 'noct.': 'in the night', 'dc': 'discontinue', 'IPPB': 'intermittent positive pressure breathing', 'I&O': 'intake and output', 'CC': 'chief complaint', 'top.': 'topically', 'MMSE': 'Mini-Mental Status Examination', 'SC, sc, s.c.': 'subcutaneous(ly)', 'cap.': 'capsule', 'GERD': 'gastroesophageal reflux disease', 'BW': 'birth weight', 'IUCD': 'intrauterine contraceptive device', 'Dx': 'diagnosis', 'AVP': 'arginine vasopressin', 'AF': 'atrial fibrillation', 'gr': 'grain', 'PMI': 'point of maximal impulse', 'UC': 'ulcerative colitis', 'ESRD': 'end-stage renal disease', 'PIPDA (scan)': '99mTc-para-isopropylacetanilido-iminodiaacetic acid (cholescintigraphy)', 'Se': 'selenium', 'stat.': 'immediately', 'mc': 'millicurie', 'amp': 'ampule', 'CVS': 'chorionic villi sampling', 'CVA': 'cardiovascular accident', 'omn. hor.': 'every hour', 'anti-CCP': 'anticyclic citrullinated peptide', 'NIDDM': 'noninsulin-dependent diabetes mellitus', 'NIH': 'National Institutes of Health', 'mol wt': 'molecular weight', 'SSS': 'sick sinus syndrome', 'mV': 'millivolt', 'ad lib.': 'freely', 'PO': 'orally', 'dil.': 'dilute', 'semih.': 'half an hour', 'Pb': 'lead', 'VD': 'venereal disease', 'CD8': 'cytotoxic cells', 'AST': 'aspartate aminotransferase', 'MAP': 'mean arterial pressure', 'RHD': 'rheumatic heart disease', 'NSR': 'normal sinus rhythm', 'PI': 'present illness', 'LFT': 'liver function test', 'My': 'myopia', 'LLE': 'left lower extremity', 's.o.s.': 'if necessary', 'trit.': 'triturate, grind', 'N/A': 'not applicable', 'CVC': 'central venous catheter', 'PSA': 'prostate-specific antigen', 'TMJ': 'temporomandibular joint', 'CT': 'computed/computerized tomography', 'BM': 'bowel movement', 'TNTM': 'too numerous to mention', 'RNA': 'ribonucleic acid', 'CIN': 'cervical intraepithelial neoplasia', 'VC': 'vital capacity', 'ol.': 'oil', 'STD': 'sexually transmitted disease', 'CV': 'cardiovascular', 'R/O': 'rule out', 'NAA': 'nucleic acid amplification', 'TM': 'tympanic membrane', 'OTC': 'over-the-counter', 'DTR': 'deep tendon reflex(es)', 'Re': 'rhenium', 'WAIS': 'Wechsler Adult Intelligence Scale', 'EF': 'ejection fraction', 'D5/Â½ /NS': '5% dextrose and half-normal saline solution (0.45% NaCl)', 'kv': 'kilovolt', 'DMARD': 'disease-modulating antirheumatic drug', 'p.r.': 'through the rectum', 'HDV': 'hepatitis D', 'NK': 'natural killer', 'bib.': 'drink', 'CP': 'cerebral palsy', 'ad': 'up to', 'TEN': 'toxic epidermal necrolysis', 'c.m.s.': 'to be taken tomorrow morning', 'QFT-G': 'QuantiFERON-TB Gold', 'RDS': 'respiratory distress syndrome', 'MED': 'minimum effective dose', 'Tb': 'terbium', 'CI': 'cardiac index', 'WN': 'well-nourished', 'per': 'through or by', 'D5W': 'dextrose 5% in water', 'Sn': 'tin', 'MLD': 'minimum lethal dose', 'MD': 'muscular dystrophy', 'KVO': 'keep vein open', 'FISH': 'fluorescence in situ hybridization', 'MA': 'mental age', 'SPECT': 'single-photon emission computed tomography', 'aPTT': 'activated partial thromboplastin', 'ADHD': 'attention deficit-hyperactivity disorder', 'HEPA': 'high-efficiency particulate air', 'Sr': 'strontium', 'GTT': 'glucose tolerance test', 'IL-1': 'interleukin 1', 'MPN': 'most probable number', 'mcg': 'microgram', 'GRAS': 'generally recognized as safe', 'FDA': '(U.S.) Food and Drug Administration', 'alt. hor.': 'every other hour', 'NDC': 'National Drug Code', 'Ci': 'curie', 'MBD': 'minimal brain dysfunction', 'ER': 'Emergency Room, extended-release', 'ALL': 'acute lymphocytic leukemia', 'mm Hg': 'millimeters of mercury', 'hx, Hx': 'history', 'admov.': 'apply', 'CPM': 'continuous passive motion', 'HDL': 'high-density lipoprotein', 'PMN': 'polymorphonuclear neutrophil leukocytes', 'DHT': 'dihydrotestosterone', 'A-V': 'arteriovenous', 'Te': 'tellurium', 'DNH': 'do not hospitalize', 'RLE': 'right lower extremity', 'Âµg': 'microgram', 'Gtt, gtt': 'drops', 'CS': 'cardiogenic shock', 'HELLP': 'hemolysis, elevated liver enzymes, low platelets', 'DWI': 'driving while intoxicated', 'PET': 'positron emission tomography', 'ASD': 'atrial septal defect', 'mr ': 'milliroentgen', 'HCT, Hct': 'hematocrit', 'AED': 'antiepileptic drug', 'R/T': 'related to', 'av.': 'avoirdupois', 'FSH': 'follicle-stimulating hormone', 'PFT': 'pulmonary function test', 'HTLV-III': 'human T lymphotropic virus type III', 'PMS': 'premenstrual syndrome', 'Ast': 'astigmatism', 'WNL': 'within normal limits', 'DRG': 'diagnosis-related group', 'CABG': 'coronary artery bypass graft', 'ECF': 'extended care facility', 'Rn': 'radon', 'Treg': 'regulatory T cell', 'OPD': 'outpatient department', 'EMA-IgA': 'immunoglobulin A antiendomysial', 'SNF': 'skilled nursing facility', 'q.h.': 'every hour', 'q.4h.': 'every 4 hours', 'HCP': 'health care professional', 'MMR': 'measles-mumps-rubella (vaccine)', 'rad': 'radiation absorbed dose', 'VF': 'ventricular fibrillation', 'med': 'medial', 'Au': 'gold', 'CPD': 'cephalopelvic disproportion', 'HEENT': 'head, eye, ear, nose, and throat', 'Endo': 'endocrine', 'S-A': 'sinoatrial', 'CBRNE': 'chemical, biological, radiological, nuclear, and explosive agents', 'FD': 'fatal dose', 'mor. sol.': 'as accustomed', 'ut. dict.': 'as directed', 'mg': 'milligram', 'om. mane vel noc.': 'every morning or night', 'ASCVD': 'atherosclerotic cardiovascular disease', 'KUB': 'kidney, ureter, and bladder', 'WBC': 'white blood cell', 'fl.': 'flexor', 'MCH': 'mean corpuscular hemoglobin', 'HIV': 'human immunodeficiency virus', 'Li': 'lithium', 'BMT': 'bone marrow transplantation', 'SERM': 'selective estrogen receptor modulator', 'CAP': 'let (the patient) take', 'mEq/L': 'milliequivalent per liter', 'BSA': 'body surface area', 'HR': 'heart rate', 'pro time/PT': 'prothrombin time', 'alt. noc.': 'every other night', 'syr.': 'syrup', 'sph': 'spherical', 'inj.': 'injection', 'CSF': 'cerebrospinal fluid', 'qns': 'quantity not sufficient', 'HBV': 'hepatitis B virus', 'SLE': 'systemic lupus erythematosus', 'int.': 'internal', 'S/P': 'no change after', 'ACTH': 'adrenocorticotropic hormone', 'GGT': 'gamma-glutamyl transferase', 'SJS': 'Stevens-Johnson syndrome', 'IRV': 'inspiratory reserve volume', 'CRS-R': 'Conners Rating Scales-Revised', 'LV': 'left ventricle', 'NPO': 'nothing by mouth', 'L&D': 'labor and delivery', 'Fld': 'fluid', 'jt.': 'joint', 'garg': 'gargle', 'SV': 'stroke volume', 'TUR': 'transurethral resection', 'NMJ': 'neuromuscular junction', 'supf.': 'superficial', 'CMT': 'certified medication technician', 'MRgFUS': 'MR-guided focused ultrasound surgery', 'APAP': 'acetaminophen', 'PRBCs': 'packed red blood cells', 'bipap': 'bilevel positive airway pressure', 'NICU': 'neonatal intensive care unit', 'ChE': 'cholinesterase', 'ppm': 'parts per million', 'acc.': 'accommodation', 'HCG': 'human chorionic gonadotropin', 'HPV': 'human papillomavirus', 'UE': 'upper extremity', 'BAC': 'blood alcohol concentration', 'h, hr': 'hour', 'in d.': 'daily', 'JRA': 'juvenile rheumatoid arthritis', 'Na': 'sodium', 'omn. noct.': 'every night', 'ABG': 'arterial blood gas', 'TPO': 'thyroid peroxidase', 'Ao.': 'aorta', 'D5/0.9 NaCl': '5% dextrose and normal saline solution (0.9% NaCl)', 'UTI': 'urinary tract infection', 'UA': 'urinalysis', 'TIA': 'transient ischemic attack', 'Derm': 'dermatology', 'Th': 'thorium', 'vol.': 'volume', 'NPN': 'nonprotein nitrogen', 'LDL': 'low-density lipoprotein', 'LBW': 'low birth weight', 'lab': 'laboratory', 'FP': 'family practice', 'NMDA': '', 'BP': 'blood pressure', 'DC': 'direct current', 'RAIU': 'radioactive iodine uptake', 'BLS': 'basic life support', 'AHF': 'antihemophilic factor', 'HF': 'heart failure', 'q.l.': 'as much as wanted', 'ET-1': 'endothelin-1', 'dur. dolor': 'while pain lasts', 'TSD': 'time since death', 'VSD': 'ventricular septal defect', 'CPC': 'clinicopathologic conference', 'OR': 'operating room', 'guttat.': 'drop by drop', 'MRA': 'magnetic resonance angiography', 'HLA': 'human leukocyteantigen', 'KOH': 'potassium hydroxide', 'c.n.s.': 'to be taken tomorrow night', 'WDWN': 'well-developed, well-nourished', 'ECT': 'electroconvulsive therapy', 'c/o': 'complains of', 'H1N1': 'hemagglutinin type 1 and neuraminidase type 1', 'b.': 'bone', 'HEV': 'hepatitis E', 'ELISA': 'enzyme-linked immunosorbent assay', 'COPD': 'chronic obstructive pulmonary disease', 'spt.': 'spirit', 'RR': 'recovery room', 'sup.': 'superior', 'ADL, ADLs': 'activities of daily living', 'Strep': '', 'KS': 'Kaposi sarcoma', 'T.A.T.': 'toxin-antitoxin', 'BNP': 'brain natriuretic peptide', 'CXR': 'chest x-ray', 'OB': 'obstetrics', 'CPK': 'creatine phosphokinase', 'PIH': 'pregnancy-induced hypertension', 'SAD': 'seasonal affective disorder', 'qt': 'quart', 'AFB': 'acid-fast bacillus', 'SARS': 'severe acute respiratory syndrome', 'hgb': 'hemoglobin', 'ah': 'hypermetropic astigmatism', 'T&A': 'tonsillectomy and adenoidectomy', 'GC': 'gonococcus or gonorrheal', 'LDH': 'lactate dehydrogenase', 'nCi': 'nanocurie', 'RA': 'rheumatoid arthritis', 'OmPC': 'outer membrane porin C', 'AAA': 'abdominal aortic aneurysm', 'VDRL': 'Venereal Disease Research Laboratories', 'BPH': 'benign prostatic hyperplasia', 'AChE': 'acetylcholinesterase', 'IQ': 'intelligence quotient', 'sp gr': 'specific gravity', 'POLST': 'physician orders for life-sustaining therapy', 'man. prim.': 'first thing in the morning', 'STU': 'skin test unit', 'IM': 'intramuscular', 'Staph': '', 'SPF': 'skin protection factor', 'P-A': 'placenta abruption', 'Si': 'silicon', 'COMT': 'catechol-O-methyltransferase', 'Sig.': 'write on label', 'HRT': 'hormone replacement therapy', 'PEEP': 'positive end expiratory pressure', 'PBI': 'protein-bound iodine', 'TNF': 'tumor necrosis factor', 'BCLS': 'basic cardiac life support', 'grad': 'by degrees', 'TEE': 'transesophageal echocardiogram', 'EMS': 'emergency medical service', 'EKG': 'electrocardiogram', 'PEG': 'percutaneous endoscopic gastrostomy', 'GP': 'general practitioner', 'AsH': 'hypermetropic astigmatism', 'Be': 'beryllium', 'LOC': 'level/loss of consciousness', 'pd': 'prism diopter', 'FEV': 'forced expiratory volume', 'AMLS': 'Advanced Medical Life Support', 'ENT': 'ear, nose, and throat', 'MVA': 'motor vehicle accident', 'ALS': 'amyotrophic lateral sclerosis', 'IVP': 'intravenous pyelogram', 'IDM': 'infants of diabetic mothers', 'oz': 'ounce', 'PNH': 'paroxysmal nocturnal hemoglobinuria', 'IBW': 'ideal body weight', 'IUD': 'intrauterine device', 'CSH': 'combat support hospital', 'aq. frig.': 'cold water', 'st.': 'let it/them stand', 'PD': 'interpupillary distance', 'RFT': 'renal function test', 'Am': 'mixed astigmatism', 'BHS': 'beta-hemolytic streptococci', 'CCU': 'coronary care unit', 'DJD': 'degenerative joint disease', 'p.c.': 'after meals', 'DEXA': 'dual-energy x-ray absorptiometry', 'CK-MB': 'serum creatine kinase, myocardial-bound', 'STS': 'serologic test for syphilis', 'Bx': 'biopsy', 'CA': 'coronary artery', 'Pap, Pap test': 'Papanicolaou smear', 'MW': 'molecular weight', 'BK': 'below the knee', 'PUVA': 'psoralen ultraviolet A', 'HTN': 'hypertension', 'OC': 'oral contraceptive', 'WH': 'well-hydrated', 'SIADH': 'syndrome of inappropriate diuretic hormone', 'w/v.': 'weight in volume', 'NG, ng': 'nasogastric', '/d': 'per day', 'CVP': 'central venous pressure', 'NAD': 'no acute distress', 'vol %': 'volume percent', 'DIC': 'disseminated intravascular coagulation', 'dieb. alt.': 'every other day', 'CBT': 'cognitive behavioral therapy', 'Ba': 'barium', 'tinct., tr': 'tincture', 'SI': 'international system of units', 'PCR': 'polymerase chain reaction', 'OU': 'each eye', 'CREST': 'calcinosis, Raynaud phenomenon, esophageal dysfunction, sclerodactyly, telangiectasia (cluster of features of systemic sclerosis scleroderma)', 'PIP': 'proximal interphalangeal', 'kg': 'kilogram', 'ÂµEq': 'microequivalent', 'VMA': 'vanillylmandelic acid', 'ESR': 'erythrocyte sedimentation rate', 'Sb': 'antimony', 'Sx': 'symptoms', 'Hg': 'mercury', 'CMV': 'cytomegalovirus', 'Pharm': 'pharmacy', 'AsM': 'myopic astigmatism', 'ARMD': 'age-related macular degeneration', 'HER2': 'human EGF (epidermal growth factor) receptor 2', 'Id.': 'the same', 'mMol': 'millimole', 'DI': 'diabetes insipidus', 'G6PD': 'glucose-6-phosphate dehydrogenase', 'Ra': 'radium', 'hor. som, h.s.': 'bedtime', 'P-ANCA': 'perinuclear antineutrophil cytoplasmic antibody', 'LP': 'lumbar puncture', 'EEG': 'electroencephalogram', 'VS': 'volumetric solution', 'vv': 'veins', 'ANP': 'atrial natriuretic peptide', 'Ig': 'immunoglobulin', 'PTT': 'partial thromboplastin time', 'AFP': 'alpha-fetoprotein', 'Umb': 'umbilicus', 'abd': 'abdominal/abdomen', 'PALS': 'pediatric advanced life support', 'ORIF': 'open reduction with/and internal fixation', 'AI': 'aortic incompetence', 'MCHC': 'mean corpuscular hemoglobin concentration', 'HD': 'hearing distance', 'q.p.': 'as much as desired', 'PM': 'afternoon/evening', 'n.b.': 'note well', 'MPC': 'maximum permitted concentration', 'ADH': 'antidiuretic hormone', 'MV': 'mitral valve', 'noct. maneq.': 'night and morning', 'RQ': 'respiratory quotient', 'CFTR': 'cystic fibrosis transmembrane regulator', 'lb': 'pound', 'LSIL': 'low-grade squamous epithelial lesion', 'ED': 'emergency department', 'pt': 'pint', 'anat': 'anatomy or anatomic', 'V/Q': 'ventilation/perfusion', 'ETOH, EtOH': 'ethyl alcohol', 'DM': 'diabetes mellitus', 'Ni': 'nickel', 'ext.': 'extensor', 'q.3h.': 'every 3 hours', 'CPHSS': 'Cincinnati Prehospital Stroke Scale', 'TIBC': 'total iron-binding capacity', 'HCV': 'hepatitis C virus', 'GFR': 'glomerular filtration rate', 'HIDA': 'hepatobiliary iminodiacetic acid (cholescintigraphy)', 'CPAP': 'continuous positive airway pressure', 'ECHO': 'echocardiography', 'alt. dieb.': 'every other day', 'SIDS': 'sudden infant death syndrome', 'RUE': 'right upper extremity', 'BD': 'Buerger disease', 'GB': 'gallbladder', 'CAD': 'coronary artery disease', 'CK': 'creatine kinase', 'yr': 'year', 'GSW': 'gunshot wound', 'HSV': 'herpes simplex virus', 'Ag': 'silver', 'DRE': 'digital rectal examination', 'ACh': 'acetylcholine', 'ANNA': 'anti-neuronal nuclear antibody', 'RML': 'right middle lobe of lung', 'AGC': 'atypical glandular cells', 'SB': 'small bowel', 'GU': 'genitourinary', 'LVH': 'left ventricular hypertrophy', 'BUN': 'blood urea nitrogen', 'b.i.d., bid': 'twice a day', 'PICC': 'peripherally inserted central catheter', 'VLBW': 'very low birth weight', 'non rep': 'do not repeat', 'PERRLA': 'pupils equal, regular, react to light and accommodation', 'LR': 'lactated Ringer (solution)', 'MID': 'minimum infective dose', 'AQ, aq': 'water', 'nn': 'nerves', 'mor. dict.': 'as directed', 'DISIDA (scan)': 'diisopropyl iminodiacetic acid (cholescintigraphy)', 'instill.': 'instillation', 'RSV': 'respiratory syncytial virus', 'AS': 'ankylosing spondylitis', 'ANA': 'antinuclear antibody', 'SVC': 'superior vena cava', 'ALT': 'alanine aminotransferase', 'comp.': 'compound', 'DPT': 'diphtheria-pertussis-tetanus (vaccine)', 'CRP': 'c. reactive protein', 'TSE': 'testicular self-examination', 'SBP': 'systolic blood pressure', 'As.': 'astigmatism', 'cc': 'cubic centimeter', 'TSH': 'thyroid-stimulating hormone', 'PP': 'placenta previa', 'USAN': 'United States Adopted Name'}\n"
     ]
    }
   ],
   "source": [
    "print(med_terms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts += input_texts_MedTerms\n",
    "target_texts += target_texts_MedTerms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load accident terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "file_name = os.path.join(data_path, 'AccidentsL.txt')\n",
    "threshold = 0.9\n",
    "num_samples = 0\n",
    "limit = 100\n",
    "input_texts_AccTerms, target_texts_AccTerms, _ = load_accidents_terms_with_noise(file_name, limit, num_samples, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts += input_texts_AccTerms\n",
    "target_texts += target_texts_AccTerms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load procedures and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_path, 'procedures_tests.txt')\n",
    "threshold = 0.9\n",
    "num_samples = 0\n",
    "input_texts_ProcTests, target_texts_ProcTests, _ = load_procedures_tests_with_noise(file_name, num_samples, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts += input_texts_ProcTests\n",
    "target_texts += target_texts_ProcTests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43579\n",
      "lCia mTypef: cAdiment -Abccdizentadl Injluryj \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "\n",
      "Potlciylhlofderr/Onwre Ifnormahinor \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "\n",
      "ris Name: \n",
      " \tFirst Name:\n",
      "\n",
      "\n",
      "iMddoeNamInitla \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "\n",
      "aLstNa \n",
      " \tLast Name:\n",
      "\n",
      "\n",
      "sSocai eSckurtiyNumqber: \n",
      " \tSocial Security Number:\n",
      "\n",
      "\n",
      "Bith rDt: \n",
      " \tBirth Date:\n",
      "\n",
      "\n",
      "ngder:q \n",
      " \tGender:\n",
      "\n",
      "\n",
      "rnukaeh Praefreenec \n",
      " \tLanguage Preference:\n",
      "\n",
      "\n",
      "dAers sen :1 \n",
      " \tAddress Line 1:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)\n",
    "np.savez('vocab-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 43579\n",
      "Number of unique input tokens: 115\n",
      "Number of unique output tokens: 115\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '!': 104,\n",
       " '\"': 95,\n",
       " '#': 67,\n",
       " '$': 80,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 48,\n",
       " '/': 29,\n",
       " '0': 54,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 57,\n",
       " '4': 56,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 70,\n",
       " '8': 61,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '<': 107,\n",
       " '=': 94,\n",
       " '?': 60,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 45,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 52,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 50,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 46,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 49,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 51,\n",
       " 'X': 79,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '[': 91,\n",
       " '\\\\': 97,\n",
       " ']': 92,\n",
       " '^': 86,\n",
       " '_': 105,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 59,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 58,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 62,\n",
       " 'y': 10,\n",
       " 'z': 63,\n",
       " '{': 108,\n",
       " '|': 82,\n",
       " '}': 100,\n",
       " '~': 103,\n",
       " 'Â£': 113,\n",
       " 'Â§': 109,\n",
       " 'Â«': 111,\n",
       " 'Â®': 114,\n",
       " 'Â°': 90,\n",
       " 'Â»': 110,\n",
       " 'Ã©': 106,\n",
       " 'â€“': 93,\n",
       " 'â€”': 101,\n",
       " 'â€˜': 99,\n",
       " 'â€™': 66,\n",
       " 'â€œ': 102,\n",
       " 'â€': 89,\n",
       " 'â€¢': 84,\n",
       " 'â‚¬': 112,\n",
       " 'â—': 87,\n",
       " 'âœ“': 96,\n",
       " 'ï¬': 88,\n",
       " 'ï¬‚': 98}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int # Some special chars need to be removed TODO: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'E',\n",
       " 46: 'R',\n",
       " 47: 'Y',\n",
       " 48: '.',\n",
       " 49: 'U',\n",
       " 50: 'K',\n",
       " 51: 'W',\n",
       " 52: 'H',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: '4',\n",
       " 57: '3',\n",
       " 58: 'q',\n",
       " 59: 'k',\n",
       " 60: '?',\n",
       " 61: '8',\n",
       " 62: 'x',\n",
       " 63: 'z',\n",
       " 64: '(',\n",
       " 65: ')',\n",
       " 66: 'â€™',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: 'X',\n",
       " 80: '$',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: 'â€¢',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: 'â—',\n",
       " 88: 'ï¬',\n",
       " 89: 'â€',\n",
       " 90: 'Â°',\n",
       " 91: '[',\n",
       " 92: ']',\n",
       " 93: 'â€“',\n",
       " 94: '=',\n",
       " 95: '\"',\n",
       " 96: 'âœ“',\n",
       " 97: '\\\\',\n",
       " 98: 'ï¬‚',\n",
       " 99: 'â€˜',\n",
       " 100: '}',\n",
       " 101: 'â€”',\n",
       " 102: 'â€œ',\n",
       " 103: '~',\n",
       " 104: '!',\n",
       " 105: '_',\n",
       " 106: 'Ã©',\n",
       " 107: '<',\n",
       " 108: '{',\n",
       " 109: 'Â§',\n",
       " 110: 'Â»',\n",
       " 111: 'Â«',\n",
       " 112: 'â‚¬',\n",
       " 113: 'Â£',\n",
       " 114: 'Â®'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37042, 49)\n",
      "(37042, 49, 115)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n",
      "Tensor(\"lstm_2/transpose_2:0\", shape=(?, ?, 512), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 115)    13225       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  761856      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 115)    13225       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1286144     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 115)    117875      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,192,325\n",
      "Trainable params: 2,165,875\n",
      "Non-trainable params: 26,450\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Tensor(\"input_1:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  \n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37042 samples, validate on 6537 samples\n",
      "Epoch 1/10\n",
      "37042/37042 [==============================] - 199s 5ms/step - loss: 1.1325 - categorical_accuracy: 0.6568 - val_loss: 0.3648 - val_categorical_accuracy: 0.8568\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.85684, saving model to best_model-50.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "37042/37042 [==============================] - 206s 6ms/step - loss: 0.2519 - categorical_accuracy: 0.8871 - val_loss: 0.2146 - val_categorical_accuracy: 0.8965\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.85684 to 0.89655, saving model to best_model-50.hdf5\n",
      "Epoch 3/10\n",
      "37042/37042 [==============================] - 205s 6ms/step - loss: 0.1513 - categorical_accuracy: 0.9140 - val_loss: 0.1658 - val_categorical_accuracy: 0.9106\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.89655 to 0.91063, saving model to best_model-50.hdf5\n",
      "Epoch 4/10\n",
      "37042/37042 [==============================] - 206s 6ms/step - loss: 0.1311 - categorical_accuracy: 0.9189 - val_loss: 0.1728 - val_categorical_accuracy: 0.9076\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.91063\n",
      "Epoch 5/10\n",
      "37042/37042 [==============================] - 205s 6ms/step - loss: 0.1279 - categorical_accuracy: 0.9188 - val_loss: 0.1687 - val_categorical_accuracy: 0.9084\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.91063\n",
      "Epoch 6/10\n",
      "37042/37042 [==============================] - 198s 5ms/step - loss: 0.1305 - categorical_accuracy: 0.9179 - val_loss: 0.1819 - val_categorical_accuracy: 0.9051\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.91063\n",
      "Epoch 7/10\n",
      "37042/37042 [==============================] - 206s 6ms/step - loss: 0.1314 - categorical_accuracy: 0.9174 - val_loss: 0.1728 - val_categorical_accuracy: 0.9075\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.91063\n",
      "Epoch 8/10\n",
      "37042/37042 [==============================] - 206s 6ms/step - loss: 0.1344 - categorical_accuracy: 0.9162 - val_loss: 0.1822 - val_categorical_accuracy: 0.9041\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.91063\n",
      "Epoch 9/10\n",
      "37042/37042 [==============================] - 206s 6ms/step - loss: 0.1287 - categorical_accuracy: 0.9177 - val_loss: 0.1825 - val_categorical_accuracy: 0.9039\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.91063\n",
      "Epoch 10/10\n",
      "37042/37042 [==============================] - 204s 6ms/step - loss: 0.1438 - categorical_accuracy: 0.9133 - val_loss: 0.1560 - val_categorical_accuracy: 0.9123\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.91063 to 0.91227, saving model to best_model-50.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc102acf60>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('encoder_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_gt_sequence(encoder_input_data[5:6], int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: uSnic kha orSlapauryr Conwtinuatoifn\n",
      "GT sentence: Sick Pay or Salary Continuation\n",
      "\n",
      "Decoded sentence: Sincitur Surgical Continuary Continuation\n",
      "-\n",
      "Input sentence: ED Disposition Discharge .' Yes\n",
      "GT sentence: ED Disposition Discharge: Yes\n",
      "\n",
      "Decoded sentence: ED Disposition Discharge.Yes\n",
      "-\n",
      "Input sentence: Dagvoih: \n",
      "GT sentence: Diagnosis: \n",
      "\n",
      "Decoded sentence: Diagnos:s \n",
      "-\n",
      "Input sentence: GropuP oliyk#:\n",
      "GT sentence: Group Policy #:\n",
      "\n",
      "Decoded sentence: Group Policy :\n",
      "-\n",
      "Input sentence: ISNERm NAEo:\n",
      "GT sentence: INSURER NAME:\n",
      "\n",
      "Decoded sentence: INSURER NA:\n",
      "-\n",
      "Input sentence: â€¢ CVs - 87k.2 aRng:e 77-95- f \n",
      "GT sentence: â€¢ MCV - 87.2 Range: 77-95 - fL\n",
      "\n",
      "Decoded sentence: â€¢ COVE- 87.2 Ran:e77-95-  fL\n",
      "-\n",
      "Input sentence: Aers iLne :\n",
      "\n",
      "GT sentence: Address Line 1: \n",
      "\n",
      "Decoded sentence: Address Li:e  \n",
      "-\n",
      "Input sentence: Flu lyextesnin\n",
      "GT sentence: Full extension\n",
      "\n",
      "Decoded sentence: Full yetson\n",
      "-\n",
      "Input sentence: Ineqligi\n",
      "GT sentence: Ineligible\n",
      "\n",
      "Decoded sentence: Ineligible\n",
      "-\n",
      "Input sentence: Pelsae hceeckr tehi py oef cm uo areflii:g\n",
      "GT sentence: Please check the type of claim you are filing:\n",
      "\n",
      "Decoded sentence: Please check the type of chenk bout No\n",
      "-\n",
      "Input sentence: yEpmolyqrramn:\n",
      "GT sentence: Employer Name:\n",
      "\n",
      "Decoded sentence: Employer\n",
      "-\n",
      "Input sentence: oFkistN am:\n",
      "GT sentence: First Name:\n",
      "\n",
      "Decoded sentence: First Name:\n",
      "-\n",
      "Input sentence: hDtae\n",
      "GT sentence: Date\n",
      "\n",
      "Decoded sentence: Date\n",
      "-\n",
      "Input sentence: Hezyedmna .MD, P..\n",
      "GT sentence: Heydemann M.D., P.A.\n",
      "\n",
      "Decoded sentence: Heediedd M.D, P.. mgD\n",
      "-\n",
      "Input sentence: qPIEMDOkN TEHATHCRg\n",
      "GT sentence: PIEDMONT HEALTHCARE\n",
      "\n",
      "Decoded sentence: PIEDMONT HEALTHCARE\n",
      "-\n",
      "Input sentence: Adds\n",
      "\n",
      "GT sentence: Address\n",
      "\n",
      "Decoded sentence: Address\n",
      "-\n",
      "Input sentence: DAKOTA RADIOLOGY\n",
      "GT sentence: DAKOTA RADIOLOGY\n",
      "\n",
      "Decoded sentence: DAKOTA RADIOLOGY\n",
      "-\n",
      "Input sentence: iFrost hoipe Hasl Nedwro,k Iknc.\n",
      "GT sentence: First Choice Health Network, Inc.\n",
      "\n",
      "Decoded sentence: First Choice Health New,ld\n",
      "-\n",
      "Input sentence: Comlpaintvfmrs area vailanble dat\n",
      "GT sentence: Complaint forms are available at\n",
      "\n",
      "Decoded sentence: Complaint forms are available at\n",
      "-\n",
      "Input sentence: ACfCmIDENTg LCnI MRO\n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "Decoded sentence: ACCIDENT CLAIM FORM\n",
      "-\n",
      "Input sentence: geb 2m o fy2\n",
      "GT sentence: Page 2 of 2\n",
      "\n",
      "Decoded sentence: Stat2  of 2\n",
      "-\n",
      "Input sentence: moaprsfon :None\n",
      "GT sentence: Comparison: None\n",
      "\n",
      "Decoded sentence: Comparison: None\n",
      "-\n",
      "Input sentence: wotal Paqtiretn ezsoonsibiity4$0.67j\n",
      "GT sentence: Total Patient Responsibility $40.76\n",
      "\n",
      "Decoded sentence: Total Patient Responsibility 4$0.67\n",
      "-\n",
      "Input sentence: fDwon East Orhtovpeidvcsptsv Meedciisne\n",
      "GT sentence: Down East Orthopedics Sports Medicine\n",
      "\n",
      "Decoded sentence: Down East Orthopedics Sports Medicine\n",
      "-\n",
      "Input sentence: Cmlami EenInfrmftei\n",
      "\n",
      "GT sentence: Claim Event Information\n",
      "\n",
      "Decoded sentence: Claim Event Information\n",
      "-\n",
      "Input sentence: Cqlcai mteal\n",
      "GT sentence: Claim Detail\n",
      "\n",
      "Decoded sentence: Claim Event\n",
      "-\n",
      "Input sentence: Coplainit frosmk ar eanvalalbe taq\n",
      "GT sentence: Complaint forms are available at\n",
      "\n",
      "Decoded sentence: Complaint forms are and treatment\n",
      "-\n",
      "Input sentence: Wt caviyt\n",
      "GT sentence: With activity\n",
      "\n",
      "Decoded sentence: With active\n",
      "-\n",
      "Input sentence: wSikn l-. kSi vsho sonrasher seionys.\n",
      "GT sentence: Skin -. Skin shows no rashes or lesions.\n",
      "\n",
      "Decoded sentence: Skin -.kin shows no rashes no\n",
      "-\n",
      "Input sentence: it SateZii\n",
      "GT sentence: City State Zip\n",
      "\n",
      "Decoded sentence: City State Zip\n",
      "-\n",
      "Input sentence: uD.Inofrmaton bAout oru odtiion\n",
      "GT sentence: D. Information About Your Condition\n",
      "\n",
      "Decoded sentence: D.Information About Your Condition\n",
      "-\n",
      "Input sentence: Ralsow Codep\n",
      "GT sentence: Reason Code\n",
      "\n",
      "Decoded sentence: Reason Code\n",
      "-\n",
      "Input sentence: BuisasvTeglephone\n",
      "\n",
      "GT sentence: Business Telephone:\n",
      "\n",
      "Decoded sentence: Business Telephone\n",
      "-\n",
      "Input sentence: Spous On&  Of-JbA cwcA priil 1,o2b015\n",
      "GT sentence: Spouse On & Off-Job Acc April 1, 2015\n",
      "\n",
      "Decoded sentence: Spouse On & O-fJob Acc April1,2015\n",
      "-\n",
      "Input sentence: Mvecation nNajet\n",
      "GT sentence: Medication Name\n",
      "\n",
      "Decoded sentence: Medication Name\n",
      "-\n",
      "Input sentence: qTyhpe f mEaxveiswe: feilv hockegy, ccadoi\n",
      "GT sentence: Type of Exercise: field hockey, cardio\n",
      "\n",
      "Decoded sentence: Type Op Eaver view :ack of check i, code\n",
      "-\n",
      "Input sentence: Alylowepd Amount\n",
      "GT sentence: Allowed Amount\n",
      "\n",
      "Decoded sentence: Allowed Amount\n",
      "-\n",
      "Input sentence: Cxontiumusl7-10%0\n",
      "GT sentence: Continuously 67-100%\n",
      "\n",
      "Decoded sentence: Cautinumb7-10%0\n",
      "-\n",
      "Input sentence: M:Nb DOB eSx\n",
      "GT sentence: MRN: DOB Sex:\n",
      "\n",
      "Decoded sentence: M:N DOB Sex\n",
      "-\n",
      "Input sentence: CeratedyB: Hgueh, rBittayni\n",
      "GT sentence: Created By: Hughes, Brittany\n",
      "\n",
      "Decoded sentence: Created B: Hugh,s Brittany\n",
      "-\n",
      "Input sentence: d(m/ddd/y)y g(mmd/dyy) (md//dyy)\n",
      "GT sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: (m/dd/y) m(dd/yy)(m//yy)\n",
      "-\n",
      "Input sentence: Other Carrier Paid\"\n",
      "GT sentence: Other Carrier Paid\n",
      "\n",
      "Decoded sentence: Other Carrier Paid\"\n",
      "-\n",
      "Input sentence: lPaln\n",
      "GT sentence: Plan\n",
      "\n",
      "Decoded sentence: Plan\n",
      "-\n",
      "Input sentence: OtMMvERCvILAeAYMNE\n",
      "GT sentence: COMMERCIAL PAYMENT\n",
      "\n",
      "Decoded sentence: COMMERCIAL NON\n",
      "-\n",
      "Input sentence: puleeaes nednp emtnps ot\n",
      "GT sentence: please send payments to:\n",
      "\n",
      "Decoded sentence: pers searment of the relationship? Yes\n",
      "-\n",
      "Input sentence: wOffeiecp Visi:t oPsft OpR tnee\n",
      "GT sentence: Office Visit: Post Op Rt knee\n",
      "\n",
      "Decoded sentence: Office Visit :ost Op Rt\n",
      "-\n",
      "Input sentence: cY haesand yowr tghroprcacc psni.ez\n",
      "GT sentence: You have strained your thoracic spine.\n",
      "\n",
      "Decoded sentence: Your hearned work rhurt react  i.s\n",
      "-\n",
      "Input sentence: dncine:noy nepdcoirnes mypsmr.\n",
      "GT sentence: Endocrine: no endocrine symptoms.\n",
      "\n",
      "Decoded sentence: Encoun:en never smoker sympt.ms\n",
      "-\n",
      "Input sentence: c10f0\n",
      "GT sentence: 100\n",
      "\n",
      "Decoded sentence: 10\n",
      "-\n",
      "Input sentence: eRsdnismbePrwovvdie:D.t Thmposn McqGuirleM\n",
      "\n",
      "GT sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "\n",
      "Decoded sentence: Responsible Provide:.D Thompson McGuire MD\n",
      "-\n",
      "Input sentence: Ify ees ,o nkwa tdyaltew (rmkd/yfy)?\n",
      "GT sentence: If yes, on what date (mm/dd/yy)?\n",
      "\n",
      "Decoded sentence: If yes ,n what date mm(dyy/ \n",
      "-\n",
      "Input sentence: Hospiat aNme\n",
      "GT sentence: Hospital Name\n",
      "\n",
      "Decoded sentence: Hospital Name\n",
      "-\n",
      "Input sentence: â€¢u Mkarde\n",
      "GT sentence: â€¢ Married\n",
      "\n",
      "Decoded sentence: â€¢ Married\n",
      "-\n",
      "Input sentence: irztihD te\n",
      "\n",
      "GT sentence: Birth Date:\n",
      "\n",
      "Decoded sentence: Birth Date\n",
      "-\n",
      "Input sentence: iDdy oudazvc eethep hatientt o sto working?p\n",
      "GT sentence: Did you advice the patient to stop working?\n",
      "\n",
      "Decoded sentence: Did you advice the patient to stop working?\n",
      "-\n",
      "Input sentence: sArUHTROIiZAITNN O\n",
      "GT sentence: AUTHORIZATION NO.\n",
      "\n",
      "Decoded sentence: AUTHORIZATION OF NO PACH\n",
      "-\n",
      "Input sentence: Deilvvery Typee:k sVaginall CS-cetxino\n",
      "GT sentence: Delivery Type: Vaginal C-Section\n",
      "\n",
      "Decoded sentence: Delivery Type :aginal CSecti-n\n",
      "-\n",
      "Input sentence: Ptsu Dfat\n",
      "GT sentence: Post Date\n",
      "\n",
      "Decoded sentence: Post Date\n",
      "-\n",
      "Input sentence: SRUGEO(NS:\n",
      "GT sentence: SURGEON(S):\n",
      "\n",
      "Decoded sentence: SURGEO(S:\n",
      "-\n",
      "Input sentence: eric pDatilas ro hlis lCvabi\n",
      "GT sentence: Service Details for This Claim\n",
      "\n",
      "Decoded sentence: Service Details for Claim you are liably\n",
      "-\n",
      "Input sentence: zFul lsmift latsd ya-  nob\n",
      "GT sentence: Full shift last day - no\n",
      "\n",
      "Decoded sentence: Full shif latls ated-dor hap no chable\n",
      "-\n",
      "Input sentence: UQETSONS?r? Plaee calyl (925s)2 2311h1z0\n",
      "GT sentence: QUESTIONS?? Please call (952) 232-1110\n",
      "\n",
      "Decoded sentence: DUE STANES? Pale DO c(925)2231\n",
      "-\n",
      "Input sentence: wotaAomwunllezd byv\n",
      "GT sentence: Total Amount Billed by\n",
      "\n",
      "Decoded sentence: Total Amount \n",
      "-\n",
      "Input sentence: 1. high tkene diangcostc rathsocoqpy\n",
      "GT sentence: 1. Right knee diagnostic arthroscopy.\n",
      "\n",
      "Decoded sentence: 1. Right knee diagnostic arthistoryoo\n",
      "-\n",
      "Input sentence: SapxeqciatltyC tyS tajt eZjirpa aFx Nbo.\n",
      "GT sentence: Specialty City State Zip Fax No.\n",
      "\n",
      "Decoded sentence: Specialty City State Zip Fax No\n",
      "-\n",
      "Input sentence: uunumb\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: unum\n",
      "-\n",
      "Input sentence: eSttaei/Provjince:\n",
      "GT sentence: State/Province:\n",
      "\n",
      "Decoded sentence: StateP/ovince\n",
      "-\n",
      "Input sentence: WTI NICpTsIE\n",
      "\n",
      "GT sentence: TWIN CITIES\n",
      "\n",
      "Decoded sentence: TWIN CITIES\n",
      "-\n",
      "Input sentence: cDaitoe o foVsilit/Atdisziojnt:\n",
      "GT sentence: Date of Visit/Admission:\n",
      "\n",
      "Decoded sentence: Date of VisitAdmi/sion\n",
      "-\n",
      "Input sentence: Polibholdre/OwrI nbforomatoionp\n",
      "GT sentence: Policyholder/Owner Information\n",
      "\n",
      "Decoded sentence: Policyholde/Owner Information\n",
      "-\n",
      "Input sentence: Rumddickc, Citlni , DoSrcdimbe\n",
      "GT sentence: Ruddick, Caitlin, ED Scribe\n",
      "\n",
      "Decoded sentence: Ruddick ,itical ,escribe\n",
      "-\n",
      "Input sentence: mEalq Adrdse\n",
      "GT sentence: Email Address:\n",
      "\n",
      "Decoded sentence: Email Address\n",
      "-\n",
      "Input sentence: OD: ACsCT# :\n",
      "GT sentence: DOB: ACCT #:\n",
      "\n",
      "Decoded sentence: DO: ACCT #:\n",
      "-\n",
      "Input sentence: I kyaes,c sar pof what ydta?e( mm/dd/y)\n",
      "GT sentence: If yes, as of what date? (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: If yes,as of what date? mm(dy/\n",
      "-\n",
      "Input sentence: PaitentN mae:n\n",
      "GT sentence: Patient Name:\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "-\n",
      "Input sentence: dDereep\n",
      "GT sentence: Degree\n",
      "\n",
      "Decoded sentence: Degree\n",
      "-\n",
      "Input sentence: Pwedmoont eHlayhcar\n",
      "GT sentence: Piedmont Healthcare\n",
      "\n",
      "Decoded sentence: Piedmont Healthcare\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: .1C eqvcxaln strrianq, auyte S16.1XgX\n",
      "\n",
      "GT sentence: 1. Cervical strain, acute S16.1XXA\n",
      "\n",
      "Decoded sentence: .1 Cervical strain a, Wrist S16.1XA\n",
      "-\n",
      "Input sentence: Social History\n",
      "GT sentence: Social History\n",
      "\n",
      "Decoded sentence: Social History\n",
      "-\n",
      "Input sentence: Discharge\n",
      "GT sentence: Discharge\n",
      "\n",
      "Decoded sentence: Discharge\n",
      "-\n",
      "Input sentence: APYMvETNS\n",
      "GT sentence: PAYMENTS\n",
      "\n",
      "Decoded sentence: PAYMENTS\n",
      "-\n",
      "Input sentence: Bsnies Tceelpnoen\n",
      "GT sentence: Business Telephone:\n",
      "\n",
      "Decoded sentence: Business Telephone\n",
      "-\n",
      "Input sentence: onTnhTlPaleon tehe laterkalj on talni\n",
      "\n",
      "GT sentence: nonTTP along the lateral joint line\n",
      "\n",
      "Decoded sentence: nonTTP along the lateral joint line\n",
      "-\n",
      "Input sentence: xpectdD sievry Dtaes: (mtm/ddy/y)\n",
      "GT sentence: Expected Delivery Date: (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Expected Delivery Da:e( m/dd/y)\n",
      "-\n",
      "Input sentence: Suimssjiont Dta:e\n",
      "GT sentence: Submission Date:\n",
      "\n",
      "Decoded sentence: Submission Date:\n",
      "-\n",
      "Input sentence: sClnicila SuxmmaryR ceprtf\n",
      "GT sentence: Clinical Summary Report\n",
      "\n",
      "Decoded sentence: Clinical Summary Report\n",
      "-\n",
      "Input sentence: gPxAY/DA\n",
      "\n",
      "GT sentence: PAY/ADJ\n",
      "\n",
      "Decoded sentence: PAYA/J\n",
      "-\n",
      "Input sentence: o:D tade:\n",
      "GT sentence: To: Date:\n",
      "\n",
      "Decoded sentence: C: Date:\n",
      "-\n",
      "Input sentence: Ppostal Coed\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code\n",
      "-\n",
      "Input sentence: gCmpapin of raksta\n",
      "\n",
      "GT sentence: Complains of asthma.\n",
      "\n",
      "Decoded sentence: Comparison made to\n",
      "-\n",
      "Input sentence: Cutemsr oPlciym #k:w\n",
      "GT sentence: Customer Policy #:\n",
      "\n",
      "Decoded sentence: Customer Policy #:\n",
      "-\n",
      "Input sentence: DEUeCTrIBLO TU tOF PrOCKy\n",
      "GT sentence: DEDUCTIBLE OUT OF POCKET\n",
      "\n",
      "Decoded sentence: DEDUCTIBLE OUT OF POCKET\n",
      "-\n",
      "Input sentence: Paotsya ljCyoe:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code\n",
      "-\n",
      "Input sentence: uQaliaiedsignanuvageinerieters\n",
      "GT sentence: Qualified sign language interpreters\n",
      "\n",
      "Decoded sentence: Qualified sign language interpreters\n",
      "-\n",
      "Input sentence: FisrtCohifce aelth\n",
      "GT sentence: First Choice Health\n",
      "\n",
      "Decoded sentence: First Choice Health\n",
      "-\n",
      "Input sentence: Datelelï¬‚'s'nolsl of Trnannant\n",
      "GT sentence: Date(s)/Type(s) of Treatment\n",
      "\n",
      "Decoded sentence: DatesLusal for Tax Paning\n",
      "-\n",
      "Input sentence: zâ€¢ GHa -n h4.a xRag:e1.5s-y155. t- g/cd\n",
      "GT sentence: â€¢ HGB - 14.0 Range: 11.5-15.5 - g/dL\n",
      "\n",
      "Decoded sentence: â€¢ HGN -4.Range:1.5-155\n",
      "-\n",
      "Input sentence: ALWExDAMOnUT\n",
      "GT sentence: ALLOWED AMOUNT\n",
      "\n",
      "Decoded sentence: ALLOWED AMOUNT\n",
      "-\n",
      "Input sentence: Acbuot YuoS tameuet\n",
      "GT sentence: About Your Statement\n",
      "\n",
      "Decoded sentence: Account You Selfated\n",
      "-\n",
      "Input sentence: xEDGWOOD\n",
      "GT sentence: EDGEWOOD\n",
      "\n",
      "Decoded sentence: EDGEWOOD\n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Medical Terms dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-bed408e686fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtarget_texts_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mWER_spell_correction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_WER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_texts_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WER_spell_correction |TEST= '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWER_spell_correction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5d333ca2832e>\u001b[0m in \u001b[0;36mcalculate_WER\u001b[0;34m(gt, pred)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnb_w\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWER\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnb_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "input_texts = input_texts_MedTerms\n",
    "target_texts = target_texts_MedTerms\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Procedures and Tests dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_ProcTests\n",
    "target_texts = target_texts_ProcTests\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    text = input_texts[seq_index]\n",
    "    decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "    print('-')\n",
    "    print('Input sentence:', text)\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for seq_index in range(100):\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    text = test_input_texts[seq_index]\n",
    "\n",
    "    decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "    print('-')\n",
    "    print('Input sentence:', text)\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on separate tesseract corrected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    text = input_texts[seq_index]\n",
    "    decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "    print('-')\n",
    "    print('Input sentence:', text)\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain transfer from noisy spelling mistakes to OCR corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on noisy spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_transfer.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on OCR correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "# Keep test data from the corrected OCR, as this what we care about\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# Vectorize train data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "# Vectorize test data\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.001# Reduce the learning rate for fine tuning\n",
    "model.load_weights('best_model_transfer.hdf5')\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_transfer.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
