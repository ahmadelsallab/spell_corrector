{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    print(decoder_outputs)\n",
    "    print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    print(encoder_inputs)\n",
    "    print(encoder_outputs)\n",
    "    print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 1000000\n",
    "min_sent_len = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of lenghts\n",
    "lengths = []\n",
    "for text in input_texts:\n",
    "    lengths.append(len(text))\n",
    "    lengths.append(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f728144ab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = plt.hist(lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1180.,   940.,  1384.,  1322.,  1174.,   722.,   592.,   536.,\n",
       "         332.,   286.,   242.,   186.,   180.,   214.,   112.,   154.,\n",
       "          68.,    82.,    58.,    88.,    70.,    64.,    32.,    30.,\n",
       "          20.,    26.,    32.,    24.,    58.,    14.,    64.,     6.,\n",
       "          28.,    16.,    24.,    28.,     8.,    18.,    14.,    18.,\n",
       "          12.,    24.,    14.,    28.,    14.,     4.,    12.,    44.,\n",
       "           4.,     6.,     2.,     4.,     6.,     0.,    36.,     0.,\n",
       "           4.,     4.,     8.,     6.,    14.,     8.,     8.,     8.,\n",
       "           2.,     0.,     6.,     2.,     2.,     4.,    12.,    14.,\n",
       "           8.,    12.,     6.,     0.,     4.,     4.,     2.,     0.,\n",
       "           2.,     0.,     4.,     6.,     0.,     4.,    14.,    26.,\n",
       "           4.,     0.,     2.,     4.,     4.,     2.,     4.,     2.,\n",
       "           6.,     0.,     2.,     2.,     2.,     6.,     4.,     2.,\n",
       "          40.,     4.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,    20.,     8.,    18.,     4.,     0.,\n",
       "           0.,     0.,     0.,     6.,    24.,     2.,     0.,     0.,\n",
       "           0.,     2.,     2.,     0.,     0.,     2.,     2.,     4.,\n",
       "           2.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     4.,     4.,\n",
       "          16.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     4.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     2.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   ,     4.878,     9.756,    14.634,    19.512,    24.39 ,\n",
       "          29.268,    34.146,    39.024,    43.902,    48.78 ,    53.658,\n",
       "          58.536,    63.414,    68.292,    73.17 ,    78.048,    82.926,\n",
       "          87.804,    92.682,    97.56 ,   102.438,   107.316,   112.194,\n",
       "         117.072,   121.95 ,   126.828,   131.706,   136.584,   141.462,\n",
       "         146.34 ,   151.218,   156.096,   160.974,   165.852,   170.73 ,\n",
       "         175.608,   180.486,   185.364,   190.242,   195.12 ,   199.998,\n",
       "         204.876,   209.754,   214.632,   219.51 ,   224.388,   229.266,\n",
       "         234.144,   239.022,   243.9  ,   248.778,   253.656,   258.534,\n",
       "         263.412,   268.29 ,   273.168,   278.046,   282.924,   287.802,\n",
       "         292.68 ,   297.558,   302.436,   307.314,   312.192,   317.07 ,\n",
       "         321.948,   326.826,   331.704,   336.582,   341.46 ,   346.338,\n",
       "         351.216,   356.094,   360.972,   365.85 ,   370.728,   375.606,\n",
       "         380.484,   385.362,   390.24 ,   395.118,   399.996,   404.874,\n",
       "         409.752,   414.63 ,   419.508,   424.386,   429.264,   434.142,\n",
       "         439.02 ,   443.898,   448.776,   453.654,   458.532,   463.41 ,\n",
       "         468.288,   473.166,   478.044,   482.922,   487.8  ,   492.678,\n",
       "         497.556,   502.434,   507.312,   512.19 ,   517.068,   521.946,\n",
       "         526.824,   531.702,   536.58 ,   541.458,   546.336,   551.214,\n",
       "         556.092,   560.97 ,   565.848,   570.726,   575.604,   580.482,\n",
       "         585.36 ,   590.238,   595.116,   599.994,   604.872,   609.75 ,\n",
       "         614.628,   619.506,   624.384,   629.262,   634.14 ,   639.018,\n",
       "         643.896,   648.774,   653.652,   658.53 ,   663.408,   668.286,\n",
       "         673.164,   678.042,   682.92 ,   687.798,   692.676,   697.554,\n",
       "         702.432,   707.31 ,   712.188,   717.066,   721.944,   726.822,\n",
       "         731.7  ,   736.578,   741.456,   746.334,   751.212,   756.09 ,\n",
       "         760.968,   765.846,   770.724,   775.602,   780.48 ,   785.358,\n",
       "         790.236,   795.114,   799.992,   804.87 ,   809.748,   814.626,\n",
       "         819.504,   824.382,   829.26 ,   834.138,   839.016,   843.894,\n",
       "         848.772,   853.65 ,   858.528,   863.406,   868.284,   873.162,\n",
       "         878.04 ,   882.918,   887.796,   892.674,   897.552,   902.43 ,\n",
       "         907.308,   912.186,   917.064,   921.942,   926.82 ,   931.698,\n",
       "         936.576,   941.454,   946.332,   951.21 ,   956.088,   960.966,\n",
       "         965.844,   970.722,   975.6  ,   980.478,   985.356,   990.234,\n",
       "         995.112,   999.99 ,  1004.868,  1009.746,  1014.624,  1019.502,\n",
       "        1024.38 ,  1029.258,  1034.136,  1039.014,  1043.892,  1048.77 ,\n",
       "        1053.648,  1058.526,  1063.404,  1068.282,  1073.16 ,  1078.038,\n",
       "        1082.916,  1087.794,  1092.672,  1097.55 ,  1102.428,  1107.306,\n",
       "        1112.184,  1117.062,  1121.94 ,  1126.818,  1131.696,  1136.574,\n",
       "        1141.452,  1146.33 ,  1151.208,  1156.086,  1160.964,  1165.842,\n",
       "        1170.72 ,  1175.598,  1180.476,  1185.354,  1190.232,  1195.11 ,\n",
       "        1199.988,  1204.866,  1209.744,  1214.622,  1219.5  ,  1224.378,\n",
       "        1229.256,  1234.134,  1239.012,  1243.89 ,  1248.768,  1253.646,\n",
       "        1258.524,  1263.402,  1268.28 ,  1273.158,  1278.036,  1282.914,\n",
       "        1287.792,  1292.67 ,  1297.548,  1302.426,  1307.304,  1312.182,\n",
       "        1317.06 ,  1321.938,  1326.816,  1331.694,  1336.572,  1341.45 ,\n",
       "        1346.328,  1351.206,  1356.084,  1360.962,  1365.84 ,  1370.718,\n",
       "        1375.596,  1380.474,  1385.352,  1390.23 ,  1395.108,  1399.986,\n",
       "        1404.864,  1409.742,  1414.62 ,  1419.498,  1424.376,  1429.254,\n",
       "        1434.132,  1439.01 ,  1443.888,  1448.766,  1453.644,  1458.522,\n",
       "        1463.4  ,  1468.278,  1473.156,  1478.034,  1482.912,  1487.79 ,\n",
       "        1492.668,  1497.546,  1502.424,  1507.302,  1512.18 ,  1517.058,\n",
       "        1521.936,  1526.814,  1531.692,  1536.57 ,  1541.448,  1546.326,\n",
       "        1551.204,  1556.082,  1560.96 ,  1565.838,  1570.716,  1575.594,\n",
       "        1580.472,  1585.35 ,  1590.228,  1595.106,  1599.984,  1604.862,\n",
       "        1609.74 ,  1614.618,  1619.496,  1624.374,  1629.252,  1634.13 ,\n",
       "        1639.008,  1643.886,  1648.764,  1653.642,  1658.52 ,  1663.398,\n",
       "        1668.276,  1673.154,  1678.032,  1682.91 ,  1687.788,  1692.666,\n",
       "        1697.544,  1702.422,  1707.3  ,  1712.178,  1717.056,  1721.934,\n",
       "        1726.812,  1731.69 ,  1736.568,  1741.446,  1746.324,  1751.202,\n",
       "        1756.08 ,  1760.958,  1765.836,  1770.714,  1775.592,  1780.47 ,\n",
       "        1785.348,  1790.226,  1795.104,  1799.982,  1804.86 ,  1809.738,\n",
       "        1814.616,  1819.494,  1824.372,  1829.25 ,  1834.128,  1839.006,\n",
       "        1843.884,  1848.762,  1853.64 ,  1858.518,  1863.396,  1868.274,\n",
       "        1873.152,  1878.03 ,  1882.908,  1887.786,  1892.664,  1897.542,\n",
       "        1902.42 ,  1907.298,  1912.176,  1917.054,  1921.932,  1926.81 ,\n",
       "        1931.688,  1936.566,  1941.444,  1946.322,  1951.2  ,  1956.078,\n",
       "        1960.956,  1965.834,  1970.712,  1975.59 ,  1980.468,  1985.346,\n",
       "        1990.224,  1995.102,  1999.98 ,  2004.858,  2009.736,  2014.614,\n",
       "        2019.492,  2024.37 ,  2029.248,  2034.126,  2039.004,  2043.882,\n",
       "        2048.76 ,  2053.638,  2058.516,  2063.394,  2068.272,  2073.15 ,\n",
       "        2078.028,  2082.906,  2087.784,  2092.662,  2097.54 ,  2102.418,\n",
       "        2107.296,  2112.174,  2117.052,  2121.93 ,  2126.808,  2131.686,\n",
       "        2136.564,  2141.442,  2146.32 ,  2151.198,  2156.076,  2160.954,\n",
       "        2165.832,  2170.71 ,  2175.588,  2180.466,  2185.344,  2190.222,\n",
       "        2195.1  ,  2199.978,  2204.856,  2209.734,  2214.612,  2219.49 ,\n",
       "        2224.368,  2229.246,  2234.124,  2239.002,  2243.88 ,  2248.758,\n",
       "        2253.636,  2258.514,  2263.392,  2268.27 ,  2273.148,  2278.026,\n",
       "        2282.904,  2287.782,  2292.66 ,  2297.538,  2302.416,  2307.294,\n",
       "        2312.172,  2317.05 ,  2321.928,  2326.806,  2331.684,  2336.562,\n",
       "        2341.44 ,  2346.318,  2351.196,  2356.074,  2360.952,  2365.83 ,\n",
       "        2370.708,  2375.586,  2380.464,  2385.342,  2390.22 ,  2395.098,\n",
       "        2399.976,  2404.854,  2409.732,  2414.61 ,  2419.488,  2424.366,\n",
       "        2429.244,  2434.122,  2439.   ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  9.756\n",
      "Count of most probable lenght =  1384.0\n",
      "Min length =  4.878\n"
     ]
    }
   ],
   "source": [
    "max_sent_len =  h[1][np.argmax(h[0])]\n",
    "min_sent_len = h[1][1]\n",
    "print('Most probable length = ', max_sent_len)\n",
    "print('Count of most probable lenght = ', np.max(h[0]))\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len =  50#int(np.ceil(max_sent_len))\n",
    "min_sent_len = 4#int(np.floor(min_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  50\n",
      "Min length =  4\n"
     ]
    }
   ],
   "source": [
    "print('Most probable length = ', max_sent_len)\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR_tess, target_texts_tess, gt_tess = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "num_samples = 0\n",
    "OCR_data = os.path.join(data_path, 'output_handwritten.txt')\n",
    "input_texts_OCR_hand, target_texts_OCR_hand, gt_texts_OCR_hand = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texts_OCR\n",
    "#target_texts = target_texts_OCR\n",
    "input_texts_OCR = input_texts_OCR_tess + input_texts_OCR_hand\n",
    "target_texts_OCR = target_texts_tess + target_texts_OCR_hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "threshold = 0.9\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = target_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3579\n",
      "Claim Type: VB Accident - Accidental Injury \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "\n",
      "Pol inyhold elm-Chm er [11 form arlon \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "\n",
      "First Name: \n",
      " \tFirst Name:\n",
      "\n",
      "\n",
      "Middle Namenitial: \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "\n",
      "Last Name: \n",
      " \tLast Name:\n",
      "\n",
      "\n",
      "Social S ecurity Number: \n",
      " \tSocial Security Number:\n",
      "\n",
      "\n",
      "Birth Date: \n",
      " \tBirth Date:\n",
      "\n",
      "\n",
      "Gender: \n",
      " \tGender:\n",
      "\n",
      "\n",
      "Language Preference: \n",
      " \tLanguage Preference:\n",
      "\n",
      "\n",
      "Address Line 1: \n",
      " \tAddress Line 1:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)\n",
    "np.savez('vocab', vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3579\n",
      "Number of unique input tokens: 115\n",
      "Number of unique output tokens: 115\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '!': 104,\n",
       " '\"': 95,\n",
       " '#': 67,\n",
       " '$': 79,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 61,\n",
       " ')': 62,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 54,\n",
       " '/': 29,\n",
       " '0': 63,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 52,\n",
       " '4': 66,\n",
       " '5': 74,\n",
       " '6': 65,\n",
       " '7': 70,\n",
       " '8': 58,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '<': 107,\n",
       " '=': 94,\n",
       " '?': 56,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 45,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 57,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 49,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 46,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 48,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 50,\n",
       " 'X': 80,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '[': 91,\n",
       " '\\\\': 97,\n",
       " ']': 92,\n",
       " '^': 86,\n",
       " '_': 105,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 55,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 51,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 59,\n",
       " 'y': 10,\n",
       " 'z': 60,\n",
       " '{': 108,\n",
       " '|': 82,\n",
       " '}': 100,\n",
       " '~': 103,\n",
       " '': 113,\n",
       " '': 109,\n",
       " '': 111,\n",
       " '': 114,\n",
       " '': 90,\n",
       " '': 110,\n",
       " '': 106,\n",
       " '': 93,\n",
       " '': 101,\n",
       " '': 99,\n",
       " '': 64,\n",
       " '': 102,\n",
       " '': 89,\n",
       " '': 84,\n",
       " '': 112,\n",
       " '': 87,\n",
       " '': 96,\n",
       " '': 88,\n",
       " '': 98}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int # Some special chars need to be removed TODO: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'E',\n",
       " 46: 'R',\n",
       " 47: 'Y',\n",
       " 48: 'U',\n",
       " 49: 'K',\n",
       " 50: 'W',\n",
       " 51: 'q',\n",
       " 52: '3',\n",
       " 53: '2',\n",
       " 54: '.',\n",
       " 55: 'k',\n",
       " 56: '?',\n",
       " 57: 'H',\n",
       " 58: '8',\n",
       " 59: 'x',\n",
       " 60: 'z',\n",
       " 61: '(',\n",
       " 62: ')',\n",
       " 63: '0',\n",
       " 64: '',\n",
       " 65: '6',\n",
       " 66: '4',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: '$',\n",
       " 80: 'X',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '',\n",
       " 88: '',\n",
       " 89: '',\n",
       " 90: '',\n",
       " 91: '[',\n",
       " 92: ']',\n",
       " 93: '',\n",
       " 94: '=',\n",
       " 95: '\"',\n",
       " 96: '',\n",
       " 97: '\\\\',\n",
       " 98: '',\n",
       " 99: '',\n",
       " 100: '}',\n",
       " 101: '',\n",
       " 102: '',\n",
       " 103: '~',\n",
       " 104: '!',\n",
       " 105: '_',\n",
       " 106: '',\n",
       " 107: '<',\n",
       " 108: '{',\n",
       " 109: '',\n",
       " 110: '',\n",
       " 111: '',\n",
       " 112: '',\n",
       " 113: '',\n",
       " 114: ''}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3042, 49)\n",
      "(3042, 49, 115)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n",
      "Tensor(\"lstm_2/transpose_2:0\", shape=(?, ?, 512), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 115)    13225       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  761856      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 115)    13225       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1286144     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 115)    117875      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,192,325\n",
      "Trainable params: 2,165,875\n",
      "Non-trainable params: 26,450\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Tensor(\"input_1:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1  \n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3042 samples, validate on 537 samples\n",
      "Epoch 1/1\n",
      "3042/3042 [==============================] - 19s 6ms/step - loss: 3.5472 - categorical_accuracy: 0.1254 - val_loss: 2.9125 - val_categorical_accuracy: 0.1954\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.19544, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7281544a20>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('encoder_model.hdf5')\n",
    "decoder_model.save('decoder_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_gt_sequence(encoder_input_data[5:6], int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: unumQ\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: Co  \n",
      "\n",
      "-\n",
      "Input sentence: Customer Policy #:\n",
      "GT sentence: Customer Policy #:\n",
      "\n",
      "Decoded sentence: Eor               \n",
      "\n",
      "-\n",
      "Input sentence: (10th\n",
      "GT sentence: Country:\n",
      "\n",
      "Decoded sentence: Co   \n",
      "\n",
      "-\n",
      "Input sentence: INITIAL EVALUATION\n",
      "GT sentence: INITIAL EVALUATION\n",
      "\n",
      "Decoded sentence: EAEEEEEEAEAEAEAA\n",
      "\n",
      "-\n",
      "Input sentence: Date First Unable to Wort-t\n",
      "GT sentence: Date First Unable to Work (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Soo                        \n",
      "\n",
      "-\n",
      "Input sentence: Medical Problems:\n",
      "GT sentence: Medical Problems:\n",
      "\n",
      "Decoded sentence: Soo              \n",
      "\n",
      "-\n",
      "Input sentence: Claim Type: VB Accident - Accidental Injury\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "\n",
      "Decoded sentence: Coo                                         \n",
      "\n",
      "-\n",
      "Input sentence: cc: Samara Shimmani,_ NP\n",
      "GT sentence: cc: Samara Shiromani, NP\n",
      "\n",
      "Decoded sentence: Eor                     \n",
      "\n",
      "-\n",
      "Input sentence: Therapy: 21 Jan2018 to Recorded\n",
      "GT sentence: Therapy: 21Jan2018 to Recorded\n",
      "\n",
      "Decoded sentence: Sore                          \n",
      "\n",
      "-\n",
      "Input sentence: 0 : 10 years\n",
      "GT sentence:  : 10 years\n",
      "\n",
      "Decoded sentence: Eo          \n",
      "\n",
      "-\n",
      "Input sentence: Confirmation of Coverage\n",
      "GT sentence: Confirmation of Coverage\n",
      "\n",
      "Decoded sentence: Cooo o  o                \n",
      "\n",
      "-\n",
      "Input sentence: Hospital Name. Minnesota Valley Surgery Center\n",
      "GT sentence: Hospital Name: Minnesota Valley Surgery Center\n",
      "\n",
      "Decoded sentence: Sooo o o  o  o  o   o   o   o   o   o   o  e  \n",
      "\n",
      "-\n",
      "Input sentence: DEDUCTEBLE\n",
      "GT sentence: DEDUCTIBLE OUT OF POCKET\n",
      "\n",
      "Decoded sentence: EAEE E E \n",
      "\n",
      "-\n",
      "Input sentence: SugarVJaor affmqa Email:\n",
      "GT sentence: Supervisor Office Email:\n",
      "\n",
      "Decoded sentence: Sore                    \n",
      "\n",
      "-\n",
      "Input sentence: Medication Name\n",
      "GT sentence: Medication Name\n",
      "\n",
      "Decoded sentence: Soo            \n",
      "\n",
      "-\n",
      "Input sentence: TIER 1 Family Deductible\n",
      "GT sentence: TIER 1 Family Deductible\n",
      "\n",
      "Decoded sentence: Eor                    \n",
      "\n",
      "-\n",
      "Input sentence: Employee 0n & Off-Job Acc January 1, 2018\n",
      "GT sentence: Employee On & Off-Job Acc January 1, 2018\n",
      "\n",
      "Decoded sentence: Eoo                                      \n",
      "\n",
      "-\n",
      "Input sentence: (mmlddlyy) (mnnudiyy) (mmfddiyy)\n",
      "GT sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Sore                         e  \n",
      "\n",
      "-\n",
      "Input sentence: E]!l]!]CI_TJ]l ent Information\n",
      "GT sentence: Employment Information\n",
      "\n",
      "Decoded sentence: Eoo o                          \n",
      "\n",
      "-\n",
      "Input sentence: Doductible\n",
      "GT sentence: Deductible\n",
      "\n",
      "Decoded sentence: Eor       \n",
      "\n",
      "-\n",
      "Input sentence: ACCIDENT CLAIM FORM\n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "Decoded sentence: EAEEEEAEAEAAAAAAA\n",
      "\n",
      "-\n",
      "Input sentence: C mmtry',\n",
      "GT sentence: Country:\n",
      "\n",
      "Decoded sentence: Eo       \n",
      "\n",
      "-\n",
      "Input sentence: -- posterior drawer\n",
      "GT sentence: -- posterior drawer\n",
      "\n",
      "Decoded sentence: Sooo o  o          \n",
      "\n",
      "-\n",
      "Input sentence: Marshfield Clinic-Phillips Center\n",
      "GT sentence: Marshfield Clinic-Phillips Center\n",
      "\n",
      "Decoded sentence: Sooo o o  o   o   o      o       \n",
      "\n",
      "-\n",
      "Input sentence: Prooodure Code\n",
      "GT sentence: Procedure Code\n",
      "\n",
      "Decoded sentence: Eore          \n",
      "\n",
      "-\n",
      "Input sentence: Guarantor Account {for Hospital Account }\n",
      "GT sentence: Guarantor Account (for Hospital Account )\n",
      "\n",
      "Decoded sentence: Soooo oo o o o o o o o o o o o o o o     \n",
      "\n",
      "-\n",
      "Input sentence: COM-WER-CIAL PAYMENT\n",
      "GT sentence: COMMERCIAL PAYMENT\n",
      "\n",
      "Decoded sentence: EAEEEEEEEEEEEEAEAA \n",
      "\n",
      "-\n",
      "Input sentence: mien! Taiapi'iona Number\n",
      "GT sentence: Patient Telephone Number\n",
      "\n",
      "Decoded sentence: Soo                      \n",
      "\n",
      "-\n",
      "Input sentence: Fiberglass;gaunt1et Cast;11 Yrs +\n",
      "GT sentence: Fiberglass; gauntlet Cast; 11 Yrs +\n",
      "\n",
      "Decoded sentence: Cor                             \n",
      "\n",
      "-\n",
      "Input sentence: Neck: Neck supple. No rigidity.\n",
      "GT sentence: Neck: Neck supple. No rigidity.\n",
      "\n",
      "Decoded sentence: Soo                             \n",
      "\n",
      "-\n",
      "Input sentence: Enznlngs Typu: Kuuxly\n",
      "GT sentence: Earnings Type: Hourly\n",
      "\n",
      "Decoded sentence: Eoo  o               \n",
      "\n",
      "-\n",
      "Input sentence: Elsewhere\n",
      "GT sentence: Elsewhere N\n",
      "\n",
      "Decoded sentence: Eore     \n",
      "\n",
      "-\n",
      "Input sentence: Accident Date:\n",
      "GT sentence: Accident Date:\n",
      "\n",
      "Decoded sentence: Cor           \n",
      "\n",
      "-\n",
      "Input sentence: Omega 369 1200 mg capsule one a clay N\n",
      "GT sentence: Omega 3-6-9 1,200 mg capsule one a day N\n",
      "\n",
      "Decoded sentence: Eor                                     \n",
      "\n",
      "-\n",
      "Input sentence: IMPRESSION:\n",
      "GT sentence: IMPRESSION:\n",
      "\n",
      "Decoded sentence: EAEE E E N\n",
      "\n",
      "-\n",
      "Input sentence: Dictated:\n",
      "GT sentence: Dictated:\n",
      "\n",
      "Decoded sentence: Co       \n",
      "\n",
      "-\n",
      "Input sentence: Allowed Amount\n",
      "GT sentence: Allowed Amount\n",
      "\n",
      "Decoded sentence: Sooooo oo e e\n",
      "\n",
      "-\n",
      "Input sentence: Patient 3\"! RN:\n",
      "GT sentence: Patient MRN: \n",
      "\n",
      "Decoded sentence: Eo             \n",
      "\n",
      "-\n",
      "Input sentence: Other Family Member:\n",
      "GT sentence: Other Family Member:\n",
      "\n",
      "Decoded sentence: Sor                 \n",
      "\n",
      "-\n",
      "Input sentence: Neuro\n",
      "GT sentence: Neuro\n",
      "\n",
      "Decoded sentence: So   \n",
      "\n",
      "-\n",
      "Input sentence: D. Signature of Attending Physician\n",
      "GT sentence: D. Signature of Attending Physician\n",
      "\n",
      "Decoded sentence: Soo o                               \n",
      "\n",
      "-\n",
      "Input sentence: Forwarding Service Requested\n",
      "GT sentence: Forwarding Service Requested\n",
      "\n",
      "Decoded sentence: Core                        \n",
      "\n",
      "-\n",
      "Input sentence: Last Name Suffix First Name MI\n",
      "GT sentence: Last Name Suffix First Name  MI\n",
      "\n",
      "Decoded sentence: Eor                           \n",
      "\n",
      "-\n",
      "Input sentence: Other Carrier Padi\n",
      "GT sentence: Other Carrier Paid\n",
      "\n",
      "Decoded sentence: Sor               \n",
      "\n",
      "-\n",
      "Input sentence: hddle Name/initial:\n",
      "GT sentence: Middle Name/Initial:\n",
      "\n",
      "Decoded sentence: Eor                 \n",
      "\n",
      "-\n",
      "Input sentence: Reason\n",
      "GT sentence: Reason\n",
      "\n",
      "Decoded sentence: So     \n",
      "\n",
      "-\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Eore e e e \n",
      "\n",
      "-\n",
      "Input sentence: Location Temp Time completed at - n/a\n",
      "GT sentence: Location Temp Time completed at - n/a\n",
      "\n",
      "Decoded sentence: Coo                                  \n",
      "\n",
      "-\n",
      "Input sentence: Edsur Lmtm_JomiJ.MDcPhysdanJ\n",
      "GT sentence: Editor: Larkin, John J. MD (Physician)\n",
      "\n",
      "Decoded sentence: Eoo                         \n",
      "\n",
      "-\n",
      "Input sentence: Provider First Name: devin\n",
      "GT sentence: Provider First Name: devin\n",
      "\n",
      "Decoded sentence: Sore                      \n",
      "\n",
      "-\n",
      "Input sentence: Caudeil. Susannah, RN\n",
      "GT sentence: Caudell, Susannah, RN\n",
      "\n",
      "Decoded sentence: Eo                   \n",
      "\n",
      "-\n",
      "Input sentence: Weight: 128 pounds\n",
      "GT sentence: Weight: 128 pounds\n",
      "\n",
      "Decoded sentence: Sore              \n",
      "\n",
      "-\n",
      "Input sentence: AantAthpn; aninnal Medical Center\n",
      "GT sentence: ARMC-Athens Regional Medical Center\n",
      "\n",
      "Decoded sentence: Soo                                \n",
      "\n",
      "-\n",
      "Input sentence: Total Patient Responsibility\n",
      "GT sentence: Total Patient Responsibility\n",
      "\n",
      "Decoded sentence: Soo o                        \n",
      "\n",
      "-\n",
      "Input sentence: grade 3 opening to valgus at 30 degrees\n",
      "GT sentence: grade 3 opening to valgus at 30 degrees\n",
      "\n",
      "Decoded sentence: Sore                                    \n",
      "\n",
      "-\n",
      "Input sentence: - ANION GAP (Cale) - 16.0 Range: 0.025.0\n",
      "GT sentence:  ANION GAP (Calc) - 16.0 Range: 0.0-25.0\n",
      "\n",
      "Decoded sentence: E                                         \n",
      "\n",
      "-\n",
      "Input sentence: None available,\n",
      "GT sentence: None available.\n",
      "\n",
      "Decoded sentence: Cor            \n",
      "\n",
      "-\n",
      "Input sentence: Spouse 0n 8!. Off-Job Acc April 1, 2017\n",
      "GT sentence: Spouse On & Off-Job Acc April 1, 2017\n",
      "\n",
      "Decoded sentence: Soo                                    \n",
      "\n",
      "-\n",
      "Input sentence: Pan- 1\n",
      "GT sentence: Page 3\n",
      "\n",
      "Decoded sentence: Eo    \n",
      "\n",
      "-\n",
      "Input sentence: Patient MRN:\n",
      "GT sentence: Patient MRN:\n",
      "\n",
      "Decoded sentence: Eo          \n",
      "\n",
      "-\n",
      "Input sentence: Customer #:\n",
      "GT sentence: Customer #:\n",
      "\n",
      "Decoded sentence: Eore       \n",
      "\n",
      "-\n",
      "Input sentence: CL-1116 (11114)\n",
      "GT sentence: CL-1116 (11/14)\n",
      "\n",
      "Decoded sentence: Eo            \n",
      "\n",
      "-\n",
      "Input sentence: Medical Provider Roles:Primary Care\n",
      "GT sentence: Medical Provider Roles: Primary Care\n",
      "\n",
      "Decoded sentence: Sooo  o  o                          \n",
      "\n",
      "-\n",
      "Input sentence: Ins: MaineCare FQHC\n",
      "GT sentence: Ins: MaineCare FQHC\n",
      "\n",
      "Decoded sentence: Eor                \n",
      "\n",
      "-\n",
      "Input sentence: Resident State on LDW  NC\n",
      "GT sentence: Resident State on LDW - NC\n",
      "\n",
      "Decoded sentence: Eo                        \n",
      "\n",
      "-\n",
      "Input sentence: Room No:\n",
      "GT sentence: Room No:\n",
      "\n",
      "Decoded sentence: Co      \n",
      "\n",
      "-\n",
      "Input sentence: after Tax:\n",
      "GT sentence: After Tax:\n",
      "\n",
      "Decoded sentence: Eo        \n",
      "\n",
      "-\n",
      "Input sentence: Medical Provider Roles: Treating\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "\n",
      "Decoded sentence: Soo o  o                         \n",
      "\n",
      "-\n",
      "Input sentence: PART I: To BE COMPLETED BY INSUREDIPATIENII\n",
      "GT sentence: PART I: TO BE COMPLETED BY INSURED/PATIENT\n",
      "\n",
      "Decoded sentence: EAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEN\n",
      "\n",
      "-\n",
      "Input sentence: Primary [CD Code:\n",
      "GT sentence: Primary ICD Code:\n",
      "\n",
      "Decoded sentence: Pore    e    e  \n",
      "\n",
      "-\n",
      "Input sentence: RESULT STATUS: Auth (Veried)\n",
      "GT sentence: RESULT STATUS: Auth (Verified)\n",
      "\n",
      "Decoded sentence: EEEE E                       \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Check (/3 only those that apply\n",
      "GT sentence: Check () only those that apply\n",
      "\n",
      "Decoded sentence: Soo  o                         \n",
      "\n",
      "-\n",
      "Input sentence: . R42 Dizziness and giddiness\n",
      "GT sentence:  R42 Dizziness and giddiness\n",
      "\n",
      "Decoded sentence: Soo                          \n",
      "\n",
      "-\n",
      "Input sentence: Report Right wrist.\n",
      "GT sentence: Report Right wrist.\n",
      "\n",
      "Decoded sentence: Sooo o o o o  o    \n",
      "\n",
      "-\n",
      "Input sentence: My Spouse: f\"\n",
      "GT sentence: My Spouse: \n",
      "\n",
      "Decoded sentence: Eor           \n",
      "\n",
      "-\n",
      "Input sentence: Original Print Date: -\n",
      "GT sentence: Original Print Date: \n",
      "\n",
      "Decoded sentence: Sore                  \n",
      "\n",
      "-\n",
      "Input sentence: :Fax Number\n",
      "GT sentence: Fax Number \n",
      "\n",
      "Decoded sentence: Eor        \n",
      "\n",
      "-\n",
      "Input sentence: 8. Pennsaid.\n",
      "GT sentence: 8. Pennsaid.\n",
      "\n",
      "Decoded sentence: Eor         \n",
      "\n",
      "-\n",
      "Input sentence: Business Telephone:\n",
      "GT sentence: Business Telephone:\n",
      "\n",
      "Decoded sentence: Sooo o o e e e e e\n",
      "\n",
      "-\n",
      "Input sentence: Health insurance provider  bcbs\n",
      "GT sentence: Health insurance provider - bcbs\n",
      "\n",
      "Decoded sentence: Soo o  o                         \n",
      "\n",
      "-\n",
      "Input sentence: Dr. Jon .J. Dewitte, M.D.;\n",
      "GT sentence: Dr. Jon J. Dewitte, M.D.;\n",
      "\n",
      "Decoded sentence: Eo                        \n",
      "\n",
      "-\n",
      "Input sentence: Policg'h old er: Owner Information\n",
      "GT sentence: Policyholder/Owner Information\n",
      "\n",
      "Decoded sentence: Sooo o o o  o  o  o  o  o  o      \n",
      "\n",
      "-\n",
      "Input sentence: ins: Maine-care\n",
      "GT sentence: Ins: Mainecare\n",
      "\n",
      "Decoded sentence: Sor            \n",
      "\n",
      "-\n",
      "Input sentence: Supervnsor Work Phone:\n",
      "GT sentence: Supervisor Work Phone:\n",
      "\n",
      "Decoded sentence: Soo o  o    e    e    \n",
      "\n",
      "-\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "\n",
      "Decoded sentence: Eo     \n",
      "\n",
      "-\n",
      "Input sentence: The Benets Center\n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: Eore              \n",
      "\n",
      "-\n",
      "Input sentence: GUARANT'UH NAME AND. ADDRESS\n",
      "GT sentence: GUARANTOR NAME AND ADDRESS\n",
      "\n",
      "Decoded sentence: EAEEEEEEEEAEAEAEAEAEAEAEAA\n",
      "\n",
      "-\n",
      "Input sentence: Commercial Insurance Adjustment\n",
      "GT sentence: Commercial Insurance Adjustment\n",
      "\n",
      "Decoded sentence: Soo  o                          \n",
      "\n",
      "-\n",
      "Input sentence: Socral Security Number\n",
      "GT sentence: Social Security Number\n",
      "\n",
      "Decoded sentence: Sor                   \n",
      "\n",
      "-\n",
      "Input sentence: Language interpreter used: No\n",
      "GT sentence: Language interpreter used: No\n",
      "\n",
      "Decoded sentence: Sore                         \n",
      "\n",
      "-\n",
      "Input sentence: S lute riProx-inee:\n",
      "GT sentence: State/Province:\n",
      "\n",
      "Decoded sentence: Sore e e e e e e  \n",
      "\n",
      "-\n",
      "Input sentence: Flrat Chorre Health Network, Inc.\n",
      "GT sentence: First Choice Health Network, Inc.\n",
      "\n",
      "Decoded sentence: Sore   o                         \n",
      "\n",
      "-\n",
      "Input sentence: Provider Me: \n",
      "GT sentence: Provider No:\n",
      "\n",
      "Decoded sentence: Eore e e e e\n",
      "\n",
      "-\n",
      "Input sentence: Details\n",
      "GT sentence: Details\n",
      "\n",
      "Decoded sentence: Eo     \n",
      "\n",
      "-\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "\n",
      "Decoded sentence: Eo     \n",
      "\n",
      "-\n",
      "Input sentence: OPERATION RECORD\n",
      "GT sentence: OPERATION RECORD\n",
      "\n",
      "Decoded sentence: EAEEEEEAEAEAAA \n",
      "\n",
      "-\n",
      "Input sentence: Location : WCINYP - Slarr\n",
      "GT sentence: Location : WCINYP - Starr \n",
      "\n",
      "Decoded sentence: Co                       \n",
      "\n",
      "-\n",
      "Input sentence: ( Treatment!) lest\n",
      "GT sentence: Treatment Dates:\n",
      "\n",
      "Decoded sentence: Eore              \n",
      "\n",
      "-\n",
      "Input sentence: DOB: :Select...\n",
      "GT sentence: DOB: Select...\n",
      "\n",
      "Decoded sentence: Eo             \n",
      "\n",
      "-\n",
      "Input sentence: ACCIDENT DETAFLS\n",
      "GT sentence: ACCIDENT DETAILS\n",
      "\n",
      "Decoded sentence: EAEEEEAEAAAAAA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Risk Factors\n",
      "GT sentence: Risk Factors\n",
      "\n",
      "Decoded sentence: EAEEEEEEEEEEEEAEAEAEAEA \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f716fcb8240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f727828f208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "seq_index = 150\n",
    "target_text = target_texts[seq_index][1:-1]\n",
    "text = input_texts[seq_index]\n",
    "print('-')\n",
    "print('Input sentence:', text)\n",
    "print('GT sentence:', target_text)\n",
    "print('Decoded sentence:', decoded_sentence) \n",
    "decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: O C . ACCIDENT CLAIM FORM\n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f716fe9c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f716ff54ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence: EAEEEEEEEEEEEEAEAEAEAEA \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_index = 150\n",
    "target_text = test_target_texts[seq_index][1:-1]\n",
    "text = test_input_texts[seq_index]\n",
    "print('-')\n",
    "print('Input sentence:', text)\n",
    "print('GT sentence:', target_text)\n",
    "print('Decoded sentence:', decoded_sentence)   \n",
    "decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on separate tesseract corrected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain transfer from noisy spelling mistakes to OCR corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on noisy spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_transfer.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on OCR correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "# Keep test data from the corrected OCR, as this what we care about\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# Vectorize train data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "# Vectorize test data\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.001# Reduce the learning rate for fine tuning\n",
    "model.load_weights('best_model_transfer.hdf5')\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_transfer.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
