{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences_data(input_texts, target_labels, max_sents_per_doc, max_words_per_sent, max_chars_per_word, \n",
    "                             num_classes, char2int):\n",
    "\n",
    "    #print(target_labels[0:100])\n",
    "    '''\n",
    "    hier_input_data = np.zeros((len(input_texts), \n",
    "                                max_sents_per_doc, \n",
    "                                max_words_per_sent, \n",
    "                                max_chars_per_word), dtype='float32')\n",
    "    \n",
    "        \n",
    "    hier_target_data = np.zeros((len(input_texts), num_classes), dtype='float32')\n",
    "    '''\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    if(target_labels == None):\n",
    "        target_labels = np.zeros(len(input_texts), dtype='int32')\n",
    "        \n",
    "\n",
    "    for i, (input_text, target_label) in enumerate(zip(input_texts, target_labels)):\n",
    "        hier_input_data = np.zeros( (max_sents_per_doc, \n",
    "                                    max_words_per_sent, \n",
    "                                    max_chars_per_word), dtype='float32')\n",
    "\n",
    "\n",
    "        hier_target_data = np.zeros(num_classes, dtype='float32')        \n",
    "        #sents_lst = sent_tokenize(clean_str(BeautifulSoup(input_text).get_text())) # TODO: Move to clean str\n",
    "        sents_lst = sent_tokenize(input_text)\n",
    "        \n",
    "        \n",
    "        if len(sents_lst) > max_sents_per_doc:\n",
    "            #print('sents_per_doc', len(sents_lst))\n",
    "            continue\n",
    "        \n",
    "        for j, sent in enumerate(sents_lst):\n",
    "                \n",
    "            words_lst = word_tokenize(sent)\n",
    "            \n",
    "            if(len(words_lst) > max_words_per_sent):\n",
    "                #print('words_per_sent', len(words_lst))\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            for k, word in enumerate(words_lst):\n",
    "                \n",
    "                \n",
    "                if(len(word) > max_chars_per_word):\n",
    "                    #print('chars_per_word', len(word))\n",
    "                    continue\n",
    "                \n",
    "                for l, char in enumerate(word):\n",
    "                    # c0..cn\n",
    "                    if(char in char2int):\n",
    "                        hier_input_data[j, k, l] = char2int[char]\n",
    "                        try:\n",
    "                            #print(target_label)\n",
    "                            hier_target_data[target_label] = 1\n",
    "                            \n",
    "                            #print(hier_target_data[i,:])\n",
    "                        except:\n",
    "                            print(target_label)\n",
    "        input_data.append(hier_input_data)\n",
    "        target_data.append(hier_target_data)\n",
    "                \n",
    "    return np.array(input_data), np.array(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars2word_model_simple_BiLSTM(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    " \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len, \n",
    "                           latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    #print(inputs.shape)\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "        \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_sentence_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_sent2doc_model(encoder_sentence_embedding_model, \n",
    "                         max_sents_seq_len, \n",
    "                         max_words_seq_len, \n",
    "                         max_char_seq_len, \n",
    "                         word2sent_latent_dim,\n",
    "                         sent2doc_latent_dim):\n",
    "    \n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    \n",
    "    sents_states = []\n",
    "    \n",
    "    for s in range(max_sents_seq_len):\n",
    "        \n",
    "        encoder_words_inputs = Lambda(lambda x: x[:,s,:,:])(inputs)\n",
    "        #print(encoder_words_inputs.shape)\n",
    "        encoder_words_outputs = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        encoder_words_outputs = Reshape((1,word2sent_latent_dim*2))(encoder_words_outputs)\n",
    "        #_, h, c = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        '''\n",
    "        input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "        encoder_inputs_ = input_words   \n",
    "        #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "        encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "        encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "\n",
    "        encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #encoder_words_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        #encoder_words_states = Reshape((1,word2sent_latent_dim*4))(encoder_words_states)\n",
    "        #print(encoder_words_outputs.shape)\n",
    "        sents_states.append(encoder_words_outputs)\n",
    "    #print(sents_states)\n",
    "    input_sents = Concatenate(axis=-2)(sents_states)\n",
    "    #print(input_sents.shape)\n",
    "    encoder_inputs_ = input_sents   \n",
    "    encoder = Bidirectional(LSTM(sent2doc_latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    \n",
    "    encoder_document_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "    '''\n",
    "    preds = Dense(2, activation='softmax')(encoder_embedding_output)\n",
    "    model = Model(inputs, preds)\n",
    "    '''\n",
    "    #return model, encoder_document_embedding_model\n",
    "    return encoder_document_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_senti_model(encoder_document_embedding_model,\n",
    "                           max_sents_seq_len, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len):\n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    encoder_embedding_output = encoder_document_embedding_model(inputs)\n",
    "    preds = Dense(2, activation='softmax')(encoder_embedding_output)\n",
    "    model = Model(inputs, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'imdb/labeledTrainData.tsv'\n",
    "data_train = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'imdb/testData.tsv'\n",
    "data_test = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_test.shape)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      With all this stuff going down at the moment w...\n",
       "1      \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2      The film starts with a manager (Nicholas Bell)...\n",
       "3      It must be assumed that those who praised this...\n",
       "4      Superbly trashy and wondrously unpretentious 8...\n",
       "5      I dont know why people think this is such a ba...\n",
       "6      This movie could have been very good, but come...\n",
       "7      I watched this video at a friend's house. I'm ...\n",
       "8      A friend of mine bought this film for £1, and ...\n",
       "9      <br /><br />This movie is full of references. ...\n",
       "10     What happens when an army of wetbacks, towelhe...\n",
       "11     Although I generally do not like remakes belie...\n",
       "12     \\Mr. Harvey Lights a Candle\\\" is anchored by a...\n",
       "13     I had a feeling that after \\Submerged\\\", this ...\n",
       "14     note to George Litman, and others: the Mystery...\n",
       "15     Stephen King adaptation (scripted by King hims...\n",
       "16     `The Matrix' was an exciting summer blockbuste...\n",
       "17     Ulli Lommel's 1980 film 'The Boogey Man' is no...\n",
       "18     This movie is one among the very few Indian mo...\n",
       "19     Most people, especially young people, may not ...\n",
       "20     \\Soylent Green\\\" is one of the best and most d...\n",
       "21     Michael Stearns plays Mike, a sexually frustra...\n",
       "22     This happy-go-luck 1939 military swashbuckler,...\n",
       "23     I would love to have that two hours of my life...\n",
       "24     The script for this movie was probably found i...\n",
       "25     Looking for Quo Vadis at my local video store,...\n",
       "26     Note to all mad scientists everywhere: if you'...\n",
       "27     What the ........... is this ? This must, with...\n",
       "28     Intrigued by the synopsis (every gay video the...\n",
       "29     Would anyone really watch this RUBBISH if it d...\n",
       "                             ...                        \n",
       "970    This movie was disaster at Box Office, and the...\n",
       "971    8 Simple Rules is a funny show but it also has...\n",
       "972    This movie was strange... I watched it while i...\n",
       "973    *** Contains Spoilers ***<br /><br />I did not...\n",
       "974    ...for this movie defines a new low in Bollywo...\n",
       "975    This dreadful film assembles every Asian stere...\n",
       "976    The original movie, The Odd Couple, has some w...\n",
       "977    Gédéon and Jules Naudet wanted to film a docum...\n",
       "978    \\Hotel du Nord \\\" is the only Carné movie from...\n",
       "979    One of Boris Karloff's real clinkers. Essentia...\n",
       "980    It is not every film's job to stimulate you su...\n",
       "981    LOL! Not a bad way to start it. I thought this...\n",
       "982    Modern viewers know this little film primarily...\n",
       "983    Like most comments I saw this film under the n...\n",
       "984    Diana Guzman is an angry young woman. Survivin...\n",
       "985    Rated PG-13 for violence, brief sexual humor a...\n",
       "986    First of all yes I'm white, so I try to tread ...\n",
       "987    A film that is so much a 30's Warners film in ...\n",
       "988    Though I'm not the biggest fan of wirework bas...\n",
       "989    I have to totally disagree with the other comm...\n",
       "990    I picked this movie up to replace the dismal c...\n",
       "991    Usually, any film with Sylvester Stallone is u...\n",
       "992    This is a VERY entertaining movie. A few of th...\n",
       "993    Think Pierce Brosnan and you think suave, dapp...\n",
       "994    A new way to enjoy Goldsworthy's work, Rivers ...\n",
       "995    The only thing I remember about this movie are...\n",
       "996    This is a kind of movie that will stay with yo...\n",
       "997    I just didn't get this movie...Was it a musica...\n",
       "998    Granting the budget and time constraints of se...\n",
       "999    This move was on TV last night. I guess as a t...\n",
       "Name: review, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = data_train.review  + data_test.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "sents_per_docs_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "\n",
    "for text in all_texts:\n",
    "    \n",
    "    sents = sent_tokenize(clean_str(BeautifulSoup(text).get_text()))\n",
    "    sents_per_docs_lengths.append(len(sents))\n",
    "    for sent in sents:       \n",
    "    \n",
    "        words = word_tokenize(sent)\n",
    "        words_per_sents_lengths.append(len(words))\n",
    "        for word in words:\n",
    "            chars_per_words_lengths.append(len(word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADoJJREFUeJzt3V2MXPV9h/HnWxZCQhqZlzVyMapBsgioEi9aIVKqqIXQEhLFvgCJKEp94co3aUqaSInTXuWmAqkKSaUIyQISt6KU1CG1RSJaywFFlVqHdaC8xKQmNAUXB28anKS5KKH59WKO6cZad2ZnZ3Z3/vt8pNXMOXuG/R2O9Wj898xsqgpJ0uT7lZUeQJI0GgZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEVPL+cMuuOCC2rRp03L+SEmaeIcOHfphVU33O25Zg75p0yZmZ2eX80dK0sRL8u+DHOeSiyQ1wqBLUiMGCnqSdUn2JHk+yeEk70pyXpL9SY50t+eOe1hJ0ukN+gz988CjVfVO4ErgMLATOFBVm4ED3bYkaYX0DXqSdwDvBu4DqKrXq+oEsAXY3R22G9g6riElSf0N8gz9UmAO+GKSJ5Pcm+Qc4MKqOgbQ3a5f6MFJdiSZTTI7Nzc3ssElSb9skKBPAdcA91TV1cDPWMTySlXtqqqZqpqZnu77MkpJ0pAGCfpR4GhVHey299AL/KtJNgB0t8fHM6IkaRB9g15VPwBeTnJZt+tG4DvAPmBbt28bsHcsE0qSBjLoq1w+CjyQ5GngKuDPgDuBm5IcAW7qtteMTTu/ttIjSNIvGeit/1X1FDCzwLduHO04kqRh+U5RSWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQZekRhh0SWqEQR+T+b8Aw1+GIWk5GHRJaoRBl6RGGHRJaoRB78P1b0mTwqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1YqCgJ/l+kmeSPJVkttt3XpL9SY50t+eOd9Tls9iXKvrSRkmrwWKeof9OVV1VVTPd9k7gQFVtBg5025KkFbKUJZctwO7u/m5g69LHkSQNa9CgF/APSQ4l2dHtu7CqjgF0t+sXemCSHUlmk8zOzc0tfWJJ0oKmBjzu+qp6Jcl6YH+S5wf9AVW1C9gFMDMzU0PMKEkawEDP0Kvqle72OPBV4Frg1SQbALrb4+MaUpLUX9+gJzknya+evA/8LvAssA/Y1h22Ddg7riElSf0NsuRyIfDVJCeP/+uqejTJE8CXk2wHXgJuG9+YkqR++ga9ql4Erlxg/38CN45jKEnS4vlOUUlqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYMHPQkZyR5Mskj3fYlSQ4mOZLkoSRnjW9MSVI/i3mGfgdweN72XcDdVbUZeA3YPsrBJEmLM1DQk2wE3gfc220HuAHY0x2yG9g6jgElSYMZ9Bn654BPAr/ots8HTlTVG932UeCihR6YZEeS2SSzc3NzSxp23Dbt/NpKjyBJQ+sb9CTvB45X1aH5uxc4tBZ6fFXtqqqZqpqZnp4eckxJUj9TAxxzPfCBJLcAZwPvoPeMfV2Sqe5Z+kbglfGNKUnqp+8z9Kr6dFVtrKpNwO3AN6rqQ8BjwK3dYduAvWObUpLU11Jeh/4p4ONJXqC3pn7faEZqm+v0ksZlkCWXN1XV48Dj3f0XgWtHP5IkaRi+U1SSGmHQJakRBl2SGmHQJakRBl2SGmHQR8iXJEpaSQZdkhph0CWpEQZ9BZ1conGpRtIoGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJakTfoCc5O8m3kvxLkueSfKbbf0mSg0mOJHkoyVnjH3f0Jvm3BU3y7JJGb5Bn6P8N3FBVVwJXATcnuQ64C7i7qjYDrwHbxzemJKmfvkGvnv/qNs/svgq4AdjT7d8NbB3LhJKkgQy0hp7kjCRPAceB/cD3gBNV9UZ3yFHgovGMKEkaxEBBr6r/qaqrgI3AtcDlCx220GOT7Egym2R2bm5u+EnXONfLJfWzqFe5VNUJ4HHgOmBdkqnuWxuBV07zmF1VNVNVM9PT00uZVZL0/xjkVS7TSdZ1998KvAc4DDwG3Nodtg3YO64hJUn9DfIMfQPwWJKngSeA/VX1CPAp4ONJXgDOB+4b35hrh0srkoY11e+AqnoauHqB/S/SW0+XJK0CvlNUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhqxZoLuW+oltW7NBF2SWmfQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJ9CwnxzpJ05KbTPoktQIgy5Jjegb9CQXJ3ksyeEkzyW5o9t/XpL9SY50t+eOf9y1xSUSSYsxyDP0N4BPVNXlwHXAR5JcAewEDlTVZuBAty1JWiF9g15Vx6rq2939nwKHgYuALcDu7rDdwNZxDSlJ6m9Ra+hJNgFXAweBC6vqGPSiD6wf9XCSpMENHPQkbwe+Anysqn6yiMftSDKbZHZubm6YGSVJAxgo6EnOpBfzB6rq4W73q0k2dN/fABxf6LFVtauqZqpqZnp6ehQzS5IWMMirXALcBxyuqs/O+9Y+YFt3fxuwd/TjSZIGNTXAMdcDHwaeSfJUt+9PgDuBLyfZDrwE3DaeESVJg+gb9Kr6RyCn+faNox1HkjQs3ykqSY0w6JLUCIPeOD8+QFo7DLokNcKgS1IjDHojXFqRZNAlqREGXZIaYdAlqRFNB911ZUlrSdNBl6S1xKBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiP6Bj3J/UmOJ3l23r7zkuxPcqS7PXe8Y2pc/CUgUjsGeYb+JeDmU/btBA5U1WbgQLctSVpBfYNeVd8EfnTK7i3A7u7+bmDriOeSJC3SsGvoF1bVMYDudv3oRpIkDWPs/yiaZEeS2SSzc3Nz4/5xkrRmDRv0V5NsAOhuj5/uwKraVVUzVTUzPT095I+TJPUzbND3Adu6+9uAvaMZR5I0rEFetvgg8E/AZUmOJtkO3AnclOQIcFO3vWqs9ZfirfXzl9aqqX4HVNUHT/OtG0c8iyRpCXynqCQ1wqBLUiOaCbrrxuMxyP9X/99Lq0MzQZektc6gS1IjDLre5NKJNNkMuiQ1wqBLUiMMuiQ1wqBrYItZY59/rGvz0vIw6JLUCIMuSY0w6GuQ7/6U2mTQJakRBl2SGmHQJakRBl0jNcq1d9fxpcUx6JLUCIMuSY0w6FoxCy2puMwiDc+gS1IjDLokNcKgS1IjJi7orrGuDsNeh9M9bthPcpT0fyYu6JKkhRl0SWrEkoKe5OYk303yQpKdoxpqIf41e23rt1Sz2F+osdDjltsolp/GOYcmz9BBT3IG8AXgvcAVwAeTXDGqwSRJi7OUZ+jXAi9U1YtV9TrwN8CW0YwlSVqspQT9IuDledtHu32SpBWQqhrugcltwO9V1R902x8Grq2qj55y3A5gR7d5GfDdPv/pC4AfDjXUZPD8JpvnN9km9fx+vaqm+x00tYQfcBS4eN72RuCVUw+qql3ArkH/o0lmq2pmCXOtap7fZPP8Jlvr57eUJZcngM1JLklyFnA7sG80Y0mSFmvoZ+hV9UaSPwT+HjgDuL+qnhvZZJKkRVnKkgtV9XXg6yOa5aSBl2cmlOc32Ty/ydb0+Q39j6KSpNXFt/5LUiNWVdCX86MElkOSi5M8luRwkueS3NHtPy/J/iRHuttzV3rWYSU5I8mTSR7pti9JcrA7t4e6fzCfSEnWJdmT5PnuGr6rsWv3x92fy2eTPJjk7Em+fknuT3I8ybPz9i14vdLzF11rnk5yzcpNPjqrJuiNfpTAG8Anqupy4DrgI9057QQOVNVm4EC3PanuAA7P274LuLs7t9eA7Ssy1Wh8Hni0qt4JXEnvPJu4dkkuAv4ImKmq36D3wobbmezr9yXg5lP2ne56vRfY3H3tAO5ZphnHatUEnQY/SqCqjlXVt7v7P6UXhIvondfu7rDdwNaVmXBpkmwE3gfc220HuAHY0x0yyef2DuDdwH0AVfV6VZ2gkWvXmQLemmQKeBtwjAm+flX1TeBHp+w+3fXaAvxl9fwzsC7JhuWZdHxWU9Cb/iiBJJuAq4GDwIVVdQx60QfWr9xkS/I54JPAL7rt84ETVfVGtz3J1/BSYA74YrekdG+Sc2jk2lXVfwB/DrxEL+Q/Bg7RzvU76XTXq8nerKagZ4F9TbwEJ8nbga8AH6uqn6z0PKOQ5P3A8ao6NH/3AodO6jWcAq4B7qmqq4GfMaHLKwvp1pK3AJcAvwacQ28Z4lSTev36aenP6ptWU9AH+iiBSZPkTHoxf6CqHu52v3ryr3fd7fGVmm8Jrgc+kOT79JbHbqD3jH1d91d4mOxreBQ4WlUHu+099ALfwrUDeA/wb1U1V1U/Bx4GfpN2rt9Jp7teTfZmNQW9uY8S6NaU7wMOV9Vn531rH7Ctu78N2Lvcsy1VVX26qjZW1SZ61+obVfUh4DHg1u6wiTw3gKr6AfByksu6XTcC36GBa9d5Cbguydu6P6cnz6+J6zfP6a7XPuD3u1e7XAf8+OTSzESrqlXzBdwC/CvwPeBPV3qeEZzPb9H7a9zTwFPd1y301poPAEe62/NWetYlnudvA4909y8FvgW8APwt8JaVnm8J53UVMNtdv78Dzm3p2gGfAZ4HngX+CnjLJF8/4EF6/x7wc3rPwLef7nrRW3L5QteaZ+i92mfFz2GpX75TVJIasZqWXCRJS2DQJakRBl2SGmHQJakRBl2SGmHQJakRBl2SGmHQJakR/wvCsnQOWRECugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49b04e7cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_s = plt.hist(sents_per_docs_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.800000000000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.45303783595946"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f498c413320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.511868686868688"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.209876214472867"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f498c4a2cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1639106377503046"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5984862894612575"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "all_texts = list(all_texts.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "train_texts = list(data_train.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))\n",
    "test_texts = list(data_test.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "#np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in all_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1000\n",
      "Number of unique input tokens: 108\n",
      "Number of unique output tokens: 108\n",
      "Max sequence length for inputs: 13235\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(all_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train review model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SENTS_PER_DOC = 20\n",
      "\n",
      "MAX_WORDS_PER_SENT = 26\n",
      "\n",
      "MAX_CHARS_PER_WORD = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTS_PER_DOC = int(np.mean(sents_per_docs_lengths)) + 1\n",
    "MAX_WORDS_PER_SENT = int(np.mean(words_per_sents_lengths)) + 1\n",
    "MAX_CHARS_PER_WORD = int(np.mean(chars_per_words_lengths)) + 1\n",
    "\n",
    "#MAX_SENTS_PER_DOC = 10\n",
    "#MAX_WORDS_PER_SENT = 40\n",
    "#MAX_CHARS_PER_WORD = 20\n",
    "print('MAX_SENTS_PER_DOC = ' + str(MAX_SENTS_PER_DOC) + '\\n')\n",
    "print('MAX_WORDS_PER_SENT = ' + str(MAX_WORDS_PER_SENT) + '\\n')\n",
    "print('MAX_CHARS_PER_WORD = ' + str(MAX_CHARS_PER_WORD) + '\\n')\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data, train_targets = vectorize_sentences_data(input_texts=train_texts, \n",
    "                                                               target_labels=list(data_train.sentiment), \n",
    "                                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                                               num_classes=NUM_CLASSES, \n",
    "                                                               char2int=char2int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 457.  413.]\n"
     ]
    }
   ],
   "source": [
    "print(train_targets.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i, t in enumerate(zip(train_texts, list(data_train.sentiment))):\\n    print(t)\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i, t in enumerate(zip(train_texts, list(data_train.sentiment))):\n",
    "    print(t)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(data_train.sentiment)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data, _ = vectorize_sentences_data(input_texts=test_texts, \n",
    "                                               target_labels=None, \n",
    "                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                               num_classes=NUM_CLASSES, \n",
    "                                               char2int=char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 20, 26, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 20, 26, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n",
      "  if sys.path[0] == '':\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 20, 26, 5)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_59 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_60 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_61 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_62 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_63 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_64 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_66 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_67 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 26, 5)        0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_10 (Model)                (None, 1024)         4957584     lambda_49[0][0]                  \n",
      "                                                                 lambda_50[0][0]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_52[0][0]                  \n",
      "                                                                 lambda_53[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "                                                                 lambda_55[0][0]                  \n",
      "                                                                 lambda_56[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "                                                                 lambda_58[0][0]                  \n",
      "                                                                 lambda_59[0][0]                  \n",
      "                                                                 lambda_60[0][0]                  \n",
      "                                                                 lambda_61[0][0]                  \n",
      "                                                                 lambda_62[0][0]                  \n",
      "                                                                 lambda_63[0][0]                  \n",
      "                                                                 lambda_64[0][0]                  \n",
      "                                                                 lambda_65[0][0]                  \n",
      "                                                                 lambda_66[0][0]                  \n",
      "                                                                 lambda_67[0][0]                  \n",
      "                                                                 lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_41 (Reshape)            (None, 1, 1024)      0           model_10[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_42 (Reshape)            (None, 1, 1024)      0           model_10[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_43 (Reshape)            (None, 1, 1024)      0           model_10[3][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_44 (Reshape)            (None, 1, 1024)      0           model_10[4][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_45 (Reshape)            (None, 1, 1024)      0           model_10[5][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_46 (Reshape)            (None, 1, 1024)      0           model_10[6][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_47 (Reshape)            (None, 1, 1024)      0           model_10[7][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_48 (Reshape)            (None, 1, 1024)      0           model_10[8][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_49 (Reshape)            (None, 1, 1024)      0           model_10[9][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_50 (Reshape)            (None, 1, 1024)      0           model_10[10][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_51 (Reshape)            (None, 1, 1024)      0           model_10[11][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_52 (Reshape)            (None, 1, 1024)      0           model_10[12][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_53 (Reshape)            (None, 1, 1024)      0           model_10[13][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_54 (Reshape)            (None, 1, 1024)      0           model_10[14][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_55 (Reshape)            (None, 1, 1024)      0           model_10[15][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_56 (Reshape)            (None, 1, 1024)      0           model_10[16][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_57 (Reshape)            (None, 1, 1024)      0           model_10[17][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_58 (Reshape)            (None, 1, 1024)      0           model_10[18][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_59 (Reshape)            (None, 1, 1024)      0           model_10[19][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_60 (Reshape)            (None, 1, 1024)      0           model_10[20][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 20, 1024)     0           reshape_41[0][0]                 \n",
      "                                                                 reshape_42[0][0]                 \n",
      "                                                                 reshape_43[0][0]                 \n",
      "                                                                 reshape_44[0][0]                 \n",
      "                                                                 reshape_45[0][0]                 \n",
      "                                                                 reshape_46[0][0]                 \n",
      "                                                                 reshape_47[0][0]                 \n",
      "                                                                 reshape_48[0][0]                 \n",
      "                                                                 reshape_49[0][0]                 \n",
      "                                                                 reshape_50[0][0]                 \n",
      "                                                                 reshape_51[0][0]                 \n",
      "                                                                 reshape_52[0][0]                 \n",
      "                                                                 reshape_53[0][0]                 \n",
      "                                                                 reshape_54[0][0]                 \n",
      "                                                                 reshape_55[0][0]                 \n",
      "                                                                 reshape_56[0][0]                 \n",
      "                                                                 reshape_57[0][0]                 \n",
      "                                                                 reshape_58[0][0]                 \n",
      "                                                                 reshape_59[0][0]                 \n",
      "                                                                 reshape_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) [(None, 20, 1024), ( 6295552     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1024)         0           bidirectional_9[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 11,253,136\n",
      "Trainable params: 11,241,472\n",
      "Non-trainable params: 11,664\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 20, 26, 5)         0         \n",
      "_________________________________________________________________\n",
      "model_11 (Model)             (None, 1024)              11253136  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 11,255,186\n",
      "Trainable params: 11,243,522\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "char2word_latent_dim = 256\n",
    "word2sent_latent_dim = 512\n",
    "sent2doc_latent_dim = 512\n",
    "char_vocab_size = len(char2int)\n",
    "\n",
    "#MAX_SENTS_PER_DOC = 21#11\n",
    "#MAX_WORDS_PER_SENT = 26#24\n",
    "#MAX_CHARS_PER_WORD = 5\n",
    "#_, _, _, encoder_word_embedding_model = build_chars2word_model(num_encoder_tokens=char_vocab_size, latent_dim=chars2word_latent_dim)\n",
    "encoder_word_embedding_model = build_chars2word_model_simple_BiLSTM(num_encoder_tokens=char_vocab_size, latent_dim=char2word_latent_dim)\n",
    "#print(encoder_word_embedding_model.summary())\n",
    "'''\n",
    "_, _, _, encoder_sentence_embedding_model = build_words2sent_model(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD,\n",
    "                                                                   latent_dim=words2sent_latent_dim)\n",
    "'''\n",
    "encoder_sentence_embedding_model = build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                                   latent_dim=word2sent_latent_dim)\n",
    "#print(encoder_sentence_embedding_model.summary())\n",
    "\n",
    "encoder_document_embedding_model = build_sent2doc_model(encoder_sentence_embedding_model, \n",
    "                                                 max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                                 max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                 max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                 word2sent_latent_dim=word2sent_latent_dim,\n",
    "                                                 sent2doc_latent_dim=sent2doc_latent_dim)\n",
    "print(encoder_document_embedding_model.summary())\n",
    "model = build_hier_senti_model(encoder_document_embedding_model=encoder_document_embedding_model,\n",
    "                                max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                max_char_seq_len=MAX_CHARS_PER_WORD)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 735 samples, validate on 184 samples\n",
      "Epoch 1/100\n",
      "735/735 [==============================] - 181s 247ms/step - loss: 0.6545 - categorical_accuracy: 0.5510 - val_loss: 0.6629 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.54891, saving model to best_hier_senti_model-20-26.hdf5\n",
      "Epoch 2/100\n",
      "735/735 [==============================] - 98s 133ms/step - loss: 0.6544 - categorical_accuracy: 0.5510 - val_loss: 0.6629 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 3/100\n",
      "735/735 [==============================] - 97s 133ms/step - loss: 0.6544 - categorical_accuracy: 0.5510 - val_loss: 0.6629 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 4/100\n",
      "735/735 [==============================] - 97s 132ms/step - loss: 0.6543 - categorical_accuracy: 0.5510 - val_loss: 0.6628 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 5/100\n",
      "735/735 [==============================] - 97s 133ms/step - loss: 0.6543 - categorical_accuracy: 0.5510 - val_loss: 0.6627 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 6/100\n",
      "735/735 [==============================] - 98s 133ms/step - loss: 0.6542 - categorical_accuracy: 0.5510 - val_loss: 0.6627 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 7/100\n",
      "735/735 [==============================] - 97s 132ms/step - loss: 0.6542 - categorical_accuracy: 0.5510 - val_loss: 0.6626 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 8/100\n",
      "735/735 [==============================] - 97s 132ms/step - loss: 0.6541 - categorical_accuracy: 0.5510 - val_loss: 0.6625 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 9/100\n",
      "735/735 [==============================] - 98s 133ms/step - loss: 0.6540 - categorical_accuracy: 0.5510 - val_loss: 0.6625 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 10/100\n",
      "735/735 [==============================] - 97s 132ms/step - loss: 0.6540 - categorical_accuracy: 0.5510 - val_loss: 0.6624 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 11/100\n",
      "735/735 [==============================] - 97s 132ms/step - loss: 0.6540 - categorical_accuracy: 0.5510 - val_loss: 0.6623 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 12/100\n",
      "735/735 [==============================] - 98s 133ms/step - loss: 0.6538 - categorical_accuracy: 0.5510 - val_loss: 0.6623 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 13/100\n",
      "735/735 [==============================] - 97s 132ms/step - loss: 0.6537 - categorical_accuracy: 0.5510 - val_loss: 0.6622 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 14/100\n",
      "735/735 [==============================] - 75s 102ms/step - loss: 0.6537 - categorical_accuracy: 0.5510 - val_loss: 0.6622 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 15/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6536 - categorical_accuracy: 0.5510 - val_loss: 0.6621 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 16/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6535 - categorical_accuracy: 0.5510 - val_loss: 0.6620 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 17/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6534 - categorical_accuracy: 0.5510 - val_loss: 0.6620 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 18/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6534 - categorical_accuracy: 0.5510 - val_loss: 0.6619 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 19/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6533 - categorical_accuracy: 0.5510 - val_loss: 0.6619 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 20/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6532 - categorical_accuracy: 0.5510 - val_loss: 0.6618 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 21/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6531 - categorical_accuracy: 0.5510 - val_loss: 0.6619 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 22/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6530 - categorical_accuracy: 0.5510 - val_loss: 0.6619 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 23/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6529 - categorical_accuracy: 0.5510 - val_loss: 0.6619 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 24/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6528 - categorical_accuracy: 0.5510 - val_loss: 0.6620 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 25/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6528 - categorical_accuracy: 0.5510 - val_loss: 0.6620 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 26/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6526 - categorical_accuracy: 0.5510 - val_loss: 0.6621 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 27/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6525 - categorical_accuracy: 0.5510 - val_loss: 0.6623 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 28/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6523 - categorical_accuracy: 0.5510 - val_loss: 0.6624 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 29/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6521 - categorical_accuracy: 0.5510 - val_loss: 0.6626 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 30/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6519 - categorical_accuracy: 0.5510 - val_loss: 0.6630 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 31/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6516 - categorical_accuracy: 0.5510 - val_loss: 0.6634 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 32/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6513 - categorical_accuracy: 0.5510 - val_loss: 0.6641 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 33/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6512 - categorical_accuracy: 0.5510 - val_loss: 0.6645 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 34/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6504 - categorical_accuracy: 0.5510 - val_loss: 0.6659 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6501 - categorical_accuracy: 0.5510 - val_loss: 0.6669 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 36/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6496 - categorical_accuracy: 0.5510 - val_loss: 0.6687 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 37/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6494 - categorical_accuracy: 0.5510 - val_loss: 0.6706 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 38/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6492 - categorical_accuracy: 0.5510 - val_loss: 0.6730 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 39/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6489 - categorical_accuracy: 0.5510 - val_loss: 0.6721 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 40/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6489 - categorical_accuracy: 0.5510 - val_loss: 0.6746 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 41/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6486 - categorical_accuracy: 0.5510 - val_loss: 0.6753 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 42/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6487 - categorical_accuracy: 0.5510 - val_loss: 0.6770 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 43/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6486 - categorical_accuracy: 0.5510 - val_loss: 0.6774 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 44/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6485 - categorical_accuracy: 0.5510 - val_loss: 0.6773 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 45/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6485 - categorical_accuracy: 0.5510 - val_loss: 0.6783 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 46/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6486 - categorical_accuracy: 0.5510 - val_loss: 0.6814 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 47/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6484 - categorical_accuracy: 0.5510 - val_loss: 0.6805 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 48/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6484 - categorical_accuracy: 0.5510 - val_loss: 0.6805 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 49/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6483 - categorical_accuracy: 0.5510 - val_loss: 0.6828 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 50/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6483 - categorical_accuracy: 0.5510 - val_loss: 0.6816 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 51/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6483 - categorical_accuracy: 0.5510 - val_loss: 0.6843 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 52/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6483 - categorical_accuracy: 0.5510 - val_loss: 0.6842 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 53/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6483 - categorical_accuracy: 0.5510 - val_loss: 0.6867 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 54/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6482 - categorical_accuracy: 0.5510 - val_loss: 0.6882 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 55/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6482 - categorical_accuracy: 0.5510 - val_loss: 0.6868 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 56/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6482 - categorical_accuracy: 0.5510 - val_loss: 0.6857 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 57/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6482 - categorical_accuracy: 0.5510 - val_loss: 0.6880 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 58/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6482 - categorical_accuracy: 0.5510 - val_loss: 0.6887 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00058: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 59/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6484 - categorical_accuracy: 0.5510 - val_loss: 0.6891 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 60/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6481 - categorical_accuracy: 0.5510 - val_loss: 0.6897 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 61/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6481 - categorical_accuracy: 0.5510 - val_loss: 0.6896 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 62/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6482 - categorical_accuracy: 0.5510 - val_loss: 0.6901 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 63/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6481 - categorical_accuracy: 0.5510 - val_loss: 0.6919 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 64/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6480 - categorical_accuracy: 0.5510 - val_loss: 0.6922 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 65/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6481 - categorical_accuracy: 0.5510 - val_loss: 0.6924 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 66/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6481 - categorical_accuracy: 0.5510 - val_loss: 0.6944 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 67/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6480 - categorical_accuracy: 0.5510 - val_loss: 0.6942 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 68/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6479 - categorical_accuracy: 0.5510 - val_loss: 0.6959 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 69/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6480 - categorical_accuracy: 0.5510 - val_loss: 0.6935 - val_categorical_accuracy: 0.5489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 70/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6480 - categorical_accuracy: 0.5510 - val_loss: 0.6955 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00070: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 71/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6479 - categorical_accuracy: 0.5510 - val_loss: 0.6971 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 72/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6480 - categorical_accuracy: 0.5510 - val_loss: 0.6979 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 73/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6479 - categorical_accuracy: 0.5510 - val_loss: 0.6968 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 74/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6480 - categorical_accuracy: 0.5510 - val_loss: 0.6987 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00074: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 75/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6478 - categorical_accuracy: 0.5510 - val_loss: 0.6972 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00075: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 76/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6478 - categorical_accuracy: 0.5510 - val_loss: 0.6987 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00076: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 77/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6478 - categorical_accuracy: 0.5510 - val_loss: 0.7002 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 78/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6477 - categorical_accuracy: 0.5510 - val_loss: 0.7005 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00078: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 79/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6478 - categorical_accuracy: 0.5510 - val_loss: 0.7019 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 80/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6477 - categorical_accuracy: 0.5510 - val_loss: 0.7019 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 81/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6478 - categorical_accuracy: 0.5510 - val_loss: 0.7038 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00081: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 82/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6476 - categorical_accuracy: 0.5510 - val_loss: 0.7043 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00082: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 83/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6477 - categorical_accuracy: 0.5510 - val_loss: 0.7048 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00083: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 84/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6477 - categorical_accuracy: 0.5510 - val_loss: 0.7040 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00084: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 85/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6476 - categorical_accuracy: 0.5510 - val_loss: 0.7055 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 86/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6476 - categorical_accuracy: 0.5510 - val_loss: 0.7068 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 87/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6475 - categorical_accuracy: 0.5510 - val_loss: 0.7068 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00087: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 88/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6475 - categorical_accuracy: 0.5510 - val_loss: 0.7083 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00088: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 89/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6475 - categorical_accuracy: 0.5510 - val_loss: 0.7087 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 90/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6474 - categorical_accuracy: 0.5510 - val_loss: 0.7099 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00090: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 91/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6474 - categorical_accuracy: 0.5510 - val_loss: 0.7104 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 92/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6474 - categorical_accuracy: 0.5510 - val_loss: 0.7122 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00092: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 93/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6473 - categorical_accuracy: 0.5510 - val_loss: 0.7121 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00093: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 94/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6473 - categorical_accuracy: 0.5510 - val_loss: 0.7121 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 95/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6473 - categorical_accuracy: 0.5510 - val_loss: 0.7127 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 96/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6472 - categorical_accuracy: 0.5510 - val_loss: 0.7136 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00096: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 97/100\n",
      "735/735 [==============================] - 69s 94ms/step - loss: 0.6473 - categorical_accuracy: 0.5510 - val_loss: 0.7135 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 98/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6471 - categorical_accuracy: 0.5510 - val_loss: 0.7151 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 99/100\n",
      "735/735 [==============================] - 68s 93ms/step - loss: 0.6472 - categorical_accuracy: 0.5510 - val_loss: 0.7158 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.54891\n",
      "Epoch 100/100\n",
      "735/735 [==============================] - 69s 93ms/step - loss: 0.6471 - categorical_accuracy: 0.5524 - val_loss: 0.7158 - val_categorical_accuracy: 0.5489\n",
      "\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.54891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f49845aa080>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 100\n",
    "lr = 0.00001\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_senti_model-{}-{}.hdf5\".format(MAX_SENTS_PER_DOC,MAX_WORDS_PER_SENT,MAX_CHARS_PER_WORD) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit(train_input_data, train_targets,\n",
    "          #validation_data=(test_input_data, test_targets)\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with all this stuff going down at the moment with mj ive started listening to his music, watching the odd documentary here and there, watched the wiz and watched moonwalker again. maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. some of it has subtle messages about mjs feeling towards the press and also the obvious message of drugs are bad mkay.visually impressive but of course this is all about michael jackson so unless you remotely like mj in anyway then you are going to hate this and find it boring. some may call mj an egotist for consenting to the making of this movie but mj and most of his fans would say that he made it for the fans which if true is really nice of him.the actual feature film bit when it finally starts is only on for 20 minutes or so excluding the smooth criminal sequence and joe pesci is convincing as a psychopathic all powerful drug lord. why he wants mj dead so bad is beyond me. because mj overheard his plans? nah, joe pescis character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates mjs music.lots of cool things in this like mj turning into a car and a robot and the whole speed demon sequence. also, the director must have had the patience of a saint when it came to filming the kiddy bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.bottom line, this movie is for people who like mj on one level or another (which i think is most people). if not, then stay away. it does try and give off a wholesome message and ironically mjs bestest buddy in this movie is a girl! michael jackson is truly one of the most talented people ever to grace this planet but is he guilty? well, with all the attention ive gave this subject....hmmm well i dont know because people can be different behind closed doors, i know this for a fact. he is either an extremely nice but stupid guy or one of the most sickest liars. i hope he is not the latter.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-9615897530e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_input' is not defined"
     ]
    }
   ],
   "source": [
    "for i, rev in enumerate(train_texts):\n",
    "    print(rev)\n",
    "    train_input = train_input_data[i].copy()\n",
    "    train_input = np.reshape(train_input, (1,train_input.shape[0], train_input.shape[1], train_input.shape[2]))\n",
    "    prediction = model.predict(train_input)\n",
    "    print('Prediction: ', prediction)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    print('Sentiment: ' + str(sentiment))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rev in enumerate(test_texts):\n",
    "    print(rev)\n",
    "    test_input = test_input_data[i].copy()\n",
    "    test_input = np.reshape(test_input, (1,test_input.shape[0], test_input.shape[1], test_input.shape[2]))\n",
    "    prediction = model.predict(test_input)\n",
    "    print('Prediction: ', prediction)\n",
    "    sentiment = np.argmax(prediction)\n",
    "    print('Sentiment: ' + str(sentiment))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
