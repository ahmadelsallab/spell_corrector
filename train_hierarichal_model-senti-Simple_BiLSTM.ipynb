{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences_data(input_texts, target_labels, max_sents_per_doc, max_words_per_sent, max_chars_per_word, \n",
    "                             num_classes, char2int):\n",
    "\n",
    "    \n",
    "    \n",
    "    hier_input_data = np.zeros((len(input_texts), \n",
    "                                max_sents_per_doc, \n",
    "                                max_words_per_sent, \n",
    "                                max_chars_per_word), dtype='float32')\n",
    "    \n",
    "        \n",
    "    hier_target_data = np.zeros((len(input_texts), num_classes), dtype='float32')\n",
    "    if(target_labels == None):\n",
    "        target_labels = np.zeros(len(input_texts), dtype='int32')\n",
    "    \n",
    "    for i, (input_text, target_label) in enumerate(zip(input_texts, target_labels)):\n",
    "        #sents_lst = sent_tokenize(clean_str(BeautifulSoup(input_text).get_text())) # TODO: Move to clean str\n",
    "        sents_lst = sent_tokenize(input_text)\n",
    "        \n",
    "        \n",
    "        if len(sents_lst) > max_sents_per_doc:\n",
    "            continue\n",
    "        \n",
    "        for j, sent in enumerate(sents_lst):\n",
    "                \n",
    "            words_lst = word_tokenize(input_text)\n",
    "            \n",
    "            if(len(words_lst) > max_words_per_sent):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            for k, word in enumerate(words_lst):\n",
    "                \n",
    "                \n",
    "                if(len(word) > max_chars_per_word):\n",
    "                    continue\n",
    "                \n",
    "                for l, char in enumerate(word):\n",
    "                    # c0..cn\n",
    "                    if(char in char2int):\n",
    "                        hier_input_data[i, j, k, l] = char2int[char]\n",
    "                        try:\n",
    "                            hier_target_data[i, target_label] = 1\n",
    "                        except:\n",
    "                            print(target_label)\n",
    "\n",
    "                \n",
    "    return hier_input_data, hier_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars2word_model_simple_BiLSTM(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    " \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len, \n",
    "                           latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    #print(inputs.shape)\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "        \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_sentence_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_sent2doc_model(encoder_sentence_embedding_model, \n",
    "                         max_sents_seq_len, \n",
    "                         max_words_seq_len, \n",
    "                         max_char_seq_len, \n",
    "                         word2sent_latent_dim,\n",
    "                         sent2doc_latent_dim):\n",
    "    \n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    \n",
    "    sents_states = []\n",
    "    \n",
    "    for s in range(max_sents_seq_len):\n",
    "        \n",
    "        encoder_words_inputs = Lambda(lambda x: x[:,s,:,:])(inputs)\n",
    "        #print(encoder_words_inputs.shape)\n",
    "        encoder_words_outputs = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        encoder_words_outputs = Reshape((1,word2sent_latent_dim*2))(encoder_words_outputs)\n",
    "        #_, h, c = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        '''\n",
    "        input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "        encoder_inputs_ = input_words   \n",
    "        #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "        encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "        encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "\n",
    "        encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #encoder_words_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        #encoder_words_states = Reshape((1,word2sent_latent_dim*4))(encoder_words_states)\n",
    "        #print(encoder_words_outputs.shape)\n",
    "        sents_states.append(encoder_words_outputs)\n",
    "    #print(sents_states)\n",
    "    input_sents = Concatenate(axis=-2)(sents_states)\n",
    "    #print(input_sents.shape)\n",
    "    encoder_inputs_ = input_sents   \n",
    "    encoder = Bidirectional(LSTM(sent2doc_latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    \n",
    "    encoder_document_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "    '''\n",
    "    preds = Dense(2, activation='softmax')(encoder_embedding_output)\n",
    "    model = Model(inputs, preds)\n",
    "    '''\n",
    "    #return model, encoder_document_embedding_model\n",
    "    return encoder_document_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_senti_model(encoder_document_embedding_model,\n",
    "                           max_sents_seq_len, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len):\n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    encoder_embedding_output = encoder_document_embedding_model(inputs)\n",
    "    preds = Dense(2, activation='softmax')(encoder_embedding_output)\n",
    "    model = Model(inputs, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'imdb/labeledTrainData.tsv'\n",
    "data_train = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'imdb/testData.tsv'\n",
    "data_test = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_test.shape)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      With all this stuff going down at the moment w...\n",
       "1      \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2      The film starts with a manager (Nicholas Bell)...\n",
       "3      It must be assumed that those who praised this...\n",
       "4      Superbly trashy and wondrously unpretentious 8...\n",
       "5      I dont know why people think this is such a ba...\n",
       "6      This movie could have been very good, but come...\n",
       "7      I watched this video at a friend's house. I'm ...\n",
       "8      A friend of mine bought this film for £1, and ...\n",
       "9      <br /><br />This movie is full of references. ...\n",
       "10     What happens when an army of wetbacks, towelhe...\n",
       "11     Although I generally do not like remakes belie...\n",
       "12     \\Mr. Harvey Lights a Candle\\\" is anchored by a...\n",
       "13     I had a feeling that after \\Submerged\\\", this ...\n",
       "14     note to George Litman, and others: the Mystery...\n",
       "15     Stephen King adaptation (scripted by King hims...\n",
       "16     `The Matrix' was an exciting summer blockbuste...\n",
       "17     Ulli Lommel's 1980 film 'The Boogey Man' is no...\n",
       "18     This movie is one among the very few Indian mo...\n",
       "19     Most people, especially young people, may not ...\n",
       "20     \\Soylent Green\\\" is one of the best and most d...\n",
       "21     Michael Stearns plays Mike, a sexually frustra...\n",
       "22     This happy-go-luck 1939 military swashbuckler,...\n",
       "23     I would love to have that two hours of my life...\n",
       "24     The script for this movie was probably found i...\n",
       "25     Looking for Quo Vadis at my local video store,...\n",
       "26     Note to all mad scientists everywhere: if you'...\n",
       "27     What the ........... is this ? This must, with...\n",
       "28     Intrigued by the synopsis (every gay video the...\n",
       "29     Would anyone really watch this RUBBISH if it d...\n",
       "                             ...                        \n",
       "970    This movie was disaster at Box Office, and the...\n",
       "971    8 Simple Rules is a funny show but it also has...\n",
       "972    This movie was strange... I watched it while i...\n",
       "973    *** Contains Spoilers ***<br /><br />I did not...\n",
       "974    ...for this movie defines a new low in Bollywo...\n",
       "975    This dreadful film assembles every Asian stere...\n",
       "976    The original movie, The Odd Couple, has some w...\n",
       "977    Gédéon and Jules Naudet wanted to film a docum...\n",
       "978    \\Hotel du Nord \\\" is the only Carné movie from...\n",
       "979    One of Boris Karloff's real clinkers. Essentia...\n",
       "980    It is not every film's job to stimulate you su...\n",
       "981    LOL! Not a bad way to start it. I thought this...\n",
       "982    Modern viewers know this little film primarily...\n",
       "983    Like most comments I saw this film under the n...\n",
       "984    Diana Guzman is an angry young woman. Survivin...\n",
       "985    Rated PG-13 for violence, brief sexual humor a...\n",
       "986    First of all yes I'm white, so I try to tread ...\n",
       "987    A film that is so much a 30's Warners film in ...\n",
       "988    Though I'm not the biggest fan of wirework bas...\n",
       "989    I have to totally disagree with the other comm...\n",
       "990    I picked this movie up to replace the dismal c...\n",
       "991    Usually, any film with Sylvester Stallone is u...\n",
       "992    This is a VERY entertaining movie. A few of th...\n",
       "993    Think Pierce Brosnan and you think suave, dapp...\n",
       "994    A new way to enjoy Goldsworthy's work, Rivers ...\n",
       "995    The only thing I remember about this movie are...\n",
       "996    This is a kind of movie that will stay with yo...\n",
       "997    I just didn't get this movie...Was it a musica...\n",
       "998    Granting the budget and time constraints of se...\n",
       "999    This move was on TV last night. I guess as a t...\n",
       "Name: review, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = data_train.review  + data_test.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "sents_per_docs_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "\n",
    "for text in all_texts:\n",
    "    \n",
    "    sents = sent_tokenize(clean_str(BeautifulSoup(text).get_text()))\n",
    "    sents_per_docs_lengths.append(len(sents))\n",
    "    for sent in sents:       \n",
    "    \n",
    "        words = word_tokenize(sent)\n",
    "        words_per_sents_lengths.append(len(words))\n",
    "        for word in words:\n",
    "            chars_per_words_lengths.append(len(word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_s = plt.hist(sents_per_docs_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.45303783595946"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.517272727272726"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.212834513980894"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.163028805997918"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.598470412425096"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/series.py:3194: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 3194 of the file /usr/local/lib/python3.6/site-packages/pandas/core/series.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  mapped = lib.map_infer(values, f, convert=convert_dtype)\n"
     ]
    }
   ],
   "source": [
    "all_texts = list(all_texts.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/series.py:3194: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 3194 of the file /usr/local/lib/python3.6/site-packages/pandas/core/series.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  mapped = lib.map_infer(values, f, convert=convert_dtype)\n"
     ]
    }
   ],
   "source": [
    "train_texts = list(data_train.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))\n",
    "test_texts = list(data_test.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "#np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in all_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1000\n",
      "Number of unique input tokens: 108\n",
      "Number of unique output tokens: 108\n",
      "Max sequence length for inputs: 13235\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(all_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'w': 4,\n",
       " 'i': 5,\n",
       " 't': 6,\n",
       " 'h': 7,\n",
       " 'a': 8,\n",
       " 'l': 9,\n",
       " 's': 10,\n",
       " 'u': 11,\n",
       " 'f': 12,\n",
       " 'g': 13,\n",
       " 'o': 14,\n",
       " 'n': 15,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'm': 18,\n",
       " 'j': 19,\n",
       " 'v': 20,\n",
       " 'r': 21,\n",
       " 'c': 22,\n",
       " ',': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " 'k': 26,\n",
       " '.': 27,\n",
       " 'b': 28,\n",
       " 'p': 29,\n",
       " '2': 30,\n",
       " '0': 31,\n",
       " 'x': 32,\n",
       " 'q': 33,\n",
       " '?': 34,\n",
       " '(': 35,\n",
       " ')': 36,\n",
       " '!': 37,\n",
       " ';': 38,\n",
       " ':': 39,\n",
       " '¨': 40,\n",
       " '-': 41,\n",
       " '1': 42,\n",
       " '6': 43,\n",
       " '9': 44,\n",
       " '5': 45,\n",
       " '8': 46,\n",
       " '\\x96': 47,\n",
       " '4': 48,\n",
       " '/': 49,\n",
       " '7': 50,\n",
       " '$': 51,\n",
       " '£': 52,\n",
       " '*': 53,\n",
       " '´': 54,\n",
       " '&': 55,\n",
       " '3': 56,\n",
       " '`': 57,\n",
       " 'é': 58,\n",
       " '=': 59,\n",
       " '%': 60,\n",
       " 'ê': 61,\n",
       " '[': 62,\n",
       " ']': 63,\n",
       " '\\x85': 64,\n",
       " '~': 65,\n",
       " '+': 66,\n",
       " '#': 67,\n",
       " 'è': 68,\n",
       " 'ó': 69,\n",
       " '\\x84': 70,\n",
       " '@': 71,\n",
       " '_': 72,\n",
       " '\\x97': 73,\n",
       " 'ı': 74,\n",
       " 'á': 75,\n",
       " 'ä': 76,\n",
       " '’': 77,\n",
       " '–': 78,\n",
       " '‘': 79,\n",
       " 'ï': 80,\n",
       " 'à': 81,\n",
       " '½': 82,\n",
       " '®': 83,\n",
       " 'â': 84,\n",
       " 'ç': 85,\n",
       " 'ö': 86,\n",
       " 'ù': 87,\n",
       " 'ü': 88,\n",
       " '^': 89,\n",
       " '<': 90,\n",
       " '>': 91,\n",
       " 'æ': 92,\n",
       " '¡': 93,\n",
       " '，': 94,\n",
       " '、': 95,\n",
       " '{': 96,\n",
       " '}': 97,\n",
       " 'ã': 98,\n",
       " '“': 99,\n",
       " '”': 100,\n",
       " 'ñ': 101,\n",
       " '\\x91': 102,\n",
       " '…': 103,\n",
       " 'ø': 104,\n",
       " '|': 105,\n",
       " '·': 106,\n",
       " 'ô': 107}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'w',\n",
       " 5: 'i',\n",
       " 6: 't',\n",
       " 7: 'h',\n",
       " 8: 'a',\n",
       " 9: 'l',\n",
       " 10: 's',\n",
       " 11: 'u',\n",
       " 12: 'f',\n",
       " 13: 'g',\n",
       " 14: 'o',\n",
       " 15: 'n',\n",
       " 16: 'd',\n",
       " 17: 'e',\n",
       " 18: 'm',\n",
       " 19: 'j',\n",
       " 20: 'v',\n",
       " 21: 'r',\n",
       " 22: 'c',\n",
       " 23: ',',\n",
       " 24: 'y',\n",
       " 25: 'z',\n",
       " 26: 'k',\n",
       " 27: '.',\n",
       " 28: 'b',\n",
       " 29: 'p',\n",
       " 30: '2',\n",
       " 31: '0',\n",
       " 32: 'x',\n",
       " 33: 'q',\n",
       " 34: '?',\n",
       " 35: '(',\n",
       " 36: ')',\n",
       " 37: '!',\n",
       " 38: ';',\n",
       " 39: ':',\n",
       " 40: '¨',\n",
       " 41: '-',\n",
       " 42: '1',\n",
       " 43: '6',\n",
       " 44: '9',\n",
       " 45: '5',\n",
       " 46: '8',\n",
       " 47: '\\x96',\n",
       " 48: '4',\n",
       " 49: '/',\n",
       " 50: '7',\n",
       " 51: '$',\n",
       " 52: '£',\n",
       " 53: '*',\n",
       " 54: '´',\n",
       " 55: '&',\n",
       " 56: '3',\n",
       " 57: '`',\n",
       " 58: 'é',\n",
       " 59: '=',\n",
       " 60: '%',\n",
       " 61: 'ê',\n",
       " 62: '[',\n",
       " 63: ']',\n",
       " 64: '\\x85',\n",
       " 65: '~',\n",
       " 66: '+',\n",
       " 67: '#',\n",
       " 68: 'è',\n",
       " 69: 'ó',\n",
       " 70: '\\x84',\n",
       " 71: '@',\n",
       " 72: '_',\n",
       " 73: '\\x97',\n",
       " 74: 'ı',\n",
       " 75: 'á',\n",
       " 76: 'ä',\n",
       " 77: '’',\n",
       " 78: '–',\n",
       " 79: '‘',\n",
       " 80: 'ï',\n",
       " 81: 'à',\n",
       " 82: '½',\n",
       " 83: '®',\n",
       " 84: 'â',\n",
       " 85: 'ç',\n",
       " 86: 'ö',\n",
       " 87: 'ù',\n",
       " 88: 'ü',\n",
       " 89: '^',\n",
       " 90: '<',\n",
       " 91: '>',\n",
       " 92: 'æ',\n",
       " 93: '¡',\n",
       " 94: '，',\n",
       " 95: '、',\n",
       " 96: '{',\n",
       " 97: '}',\n",
       " 98: 'ã',\n",
       " 99: '“',\n",
       " 100: '”',\n",
       " 101: 'ñ',\n",
       " 102: '\\x91',\n",
       " 103: '…',\n",
       " 104: 'ø',\n",
       " 105: '|',\n",
       " 106: '·',\n",
       " 107: 'ô'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train review model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SENTS_PER_DOC = 20\n",
      "\n",
      "MAX_WORDS_PER_SENT = 26\n",
      "\n",
      "MAX_CHARS_PER_WORD = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_SENTS_PER_DOC = int(np.mean(sents_per_docs_lengths)) + 1\n",
    "MAX_WORDS_PER_SENT = int(np.mean(words_per_sents_lengths)) + 1\n",
    "MAX_CHARS_PER_WORD = int(np.mean(chars_per_words_lengths)) + 1\n",
    "\n",
    "#MAX_SENTS_PER_DOC = 10\n",
    "#MAX_WORDS_PER_SENT = 40\n",
    "#MAX_CHARS_PER_WORD = 20\n",
    "print('MAX_SENTS_PER_DOC = ' + str(MAX_SENTS_PER_DOC) + '\\n')\n",
    "print('MAX_WORDS_PER_SENT = ' + str(MAX_WORDS_PER_SENT) + '\\n')\n",
    "print('MAX_CHARS_PER_WORD = ' + str(MAX_CHARS_PER_WORD) + '\\n')\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data, train_targets = vectorize_sentences_data(input_texts=train_texts, \n",
    "                                                               target_labels=list(data_train.sentiment), \n",
    "                                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                                               num_classes=NUM_CLASSES, \n",
    "                                                               char2int=char2int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data, _ = vectorize_sentences_data(input_texts=test_texts, \n",
    "                                               target_labels=None, \n",
    "                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                               num_classes=NUM_CLASSES, \n",
    "                                               char2int=char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 26, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 26, 5)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 108)         11664     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection [(None, None, 256), (None 242688    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 254,352\n",
      "Trainable params: 242,688\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 26, 5)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 26, 256)           254352    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection [(None, 26, 128), (None,  164352    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 418,704\n",
      "Trainable params: 407,040\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 20, 26, 5)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 26, 5)        0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 128)          418704      lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 128)       0           model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 128)       0           model_2[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 128)       0           model_2[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 128)       0           model_2[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 128)       0           model_2[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 128)       0           model_2[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 128)       0           model_2[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 128)       0           model_2[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 128)       0           model_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 128)       0           model_2[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1, 128)       0           model_2[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 128)       0           model_2[12][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 128)       0           model_2[13][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 128)       0           model_2[14][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 1, 128)       0           model_2[15][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 1, 128)       0           model_2[16][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 1, 128)       0           model_2[17][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 1, 128)       0           model_2[18][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 1, 128)       0           model_2[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 1, 128)       0           model_2[20][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 128)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "                                                                 reshape_16[0][0]                 \n",
      "                                                                 reshape_17[0][0]                 \n",
      "                                                                 reshape_18[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "                                                                 reshape_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 20, 64), (No 41216       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 64)           0           bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 459,920\n",
      "Trainable params: 448,256\n",
      "Non-trainable params: 11,664\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 20, 26, 5)         0         \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 64)                459920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 460,050\n",
      "Trainable params: 448,386\n",
      "Non-trainable params: 11,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "char2word_latent_dim = 128\n",
    "word2sent_latent_dim = 64\n",
    "sent2doc_latent_dim = 32\n",
    "char_vocab_size = len(char2int)\n",
    "\n",
    "#MAX_SENTS_PER_DOC = 11\n",
    "#MAX_WORDS_PER_SENT = 24\n",
    "#MAX_CHARS_PER_WORD = 5\n",
    "#_, _, _, encoder_word_embedding_model = build_chars2word_model(num_encoder_tokens=char_vocab_size, latent_dim=chars2word_latent_dim)\n",
    "encoder_word_embedding_model = build_chars2word_model_simple_BiLSTM(num_encoder_tokens=char_vocab_size, latent_dim=char2word_latent_dim)\n",
    "print(encoder_word_embedding_model.summary())\n",
    "'''\n",
    "_, _, _, encoder_sentence_embedding_model = build_words2sent_model(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD,\n",
    "                                                                   latent_dim=words2sent_latent_dim)\n",
    "'''\n",
    "encoder_sentence_embedding_model = build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                                   latent_dim=word2sent_latent_dim)\n",
    "print(encoder_sentence_embedding_model.summary())\n",
    "\n",
    "encoder_document_embedding_model = build_sent2doc_model(encoder_sentence_embedding_model, \n",
    "                                                 max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                                 max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                 max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                 word2sent_latent_dim=word2sent_latent_dim,\n",
    "                                                 sent2doc_latent_dim=sent2doc_latent_dim)\n",
    "print(encoder_document_embedding_model.summary())\n",
    "model = build_hier_senti_model(encoder_document_embedding_model=encoder_document_embedding_model,\n",
    "                                max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                max_char_seq_len=MAX_CHARS_PER_WORD)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 0.0000e+00 - categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 1.00000, saving model to best_hier_senti_model-20-26.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fccb42202b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_senti_model-{}-{}.hdf5\".format(MAX_SENTS_PER_DOC,MAX_WORDS_PER_SENT,MAX_CHARS_PER_WORD) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit(train_input_data, train_targets,\n",
    "          #validation_data=(test_input_data, test_targets)\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturally in a film whos main themes are of mortality, nostalgia, and loss of innocence it is perhaps not surprising that it is rated more highly by older viewers than younger ones. however there is a craftsmanship and completeness to the film which anyone can enjoy. the pace is steady and constant, the characters full and engaging, the relationships and interactions natural showing that you do not need floods of tears to show emotion, screams to show fear, shouting to show dispute or violence to show anger. naturally joyces short story lends the film a ready made structure as perfect as a polished diamond, but the small changes huston makes such as the inclusion of the poem fit in neatly. it is truly a masterpiece of tact, subtlety and overwhelming beauty.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this movie is a disaster within a disaster film. it is full of great action scenes, which are only meaningful if you throw away all sense of reality. lets see, word to the wise, lava burns you; steam burns you. you cant stand next to lava. diverting a minor lava flow is difficult, let alone a significant one. scares me to think that some might actually believe what they saw in this movie.even worse is the significant amount of talent that went into making this film. i mean the acting is actually very good. the effects are above average. hard to believe somebody read the scripts for this and allowed all this talent to be wasted. i guess my suggestion would be that if this movie is about to start on tv ... look away! it is like a train wreck: it is so awful that once you know what is coming, you just have to watch. look away and spend your time on more meaningful content.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "all in all, this is a movie for kids. we saw it tonight and my child loved it. at one point my kids excitement was so great that sitting was impossible. however, i am a great fan of a.a. milnes books which are very subtle and hide a wry intelligence behind the childlike quality of its leading characters. this film was not subtle. it seems a shame that disney cannot see the benefit of making movies from more of the stories contained in those pages, although perhaps, it doesnt have the permission to use them. i found myself wishing the theater was replaying winnie-the-pooh and tigger too, instead. the characters voices were very good. i was only really bothered by kanga. the music, however, was twice as loud in parts than the dialog, and incongruous to the film.as for the story, it was a bit preachy and militant in tone. overall, i was disappointed, but i would go again just to see the same excitement on my childs face.i liked lumpys laugh....\n",
      "Sentiment: [[0.5 0.5]]\n",
      "afraid of the dark left me with the impression that several different screenplays were written, all too short for a feature length film, then spliced together clumsily into this frankensteins monster.at his best, the protagonist, lucas, is creepy. as hard as it is to draw a bead on the secondary characters, theyre far more sympathetic.afraid of the dark could have achieved mediocrity had it taken just one approach and seen it through -- and had it made lucas simply psychotic and confused instead of ghoulish and off-putting. i wanted to see him packed off into an asylum so the rest of the characters could have a normal life.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "a very accurate depiction of small time mob life filmed in new jersey. the story, characters and script are believable but the acting drops the ball. still, its worth watching, especially for the strong images, some still with me even though i first viewed this 25 years ago.a young hood steps up and starts doing bigger things (tries to) but these things keep going wrong, leading the local boss to suspect that his end is being skimmed off, not a good place to be if you enjoy your health, or life.this is the film that introduced joe pesce to martin scorsese. also present is that perennial screen wise guy, frank vincent. strong on characterizations and visuals. sound muddled and much of the acting is amateurish, but a great story.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "...as valuable as king tuts tomb! (ok, maybe not that valuable, but worth hunting down if you can). i notice no one has commented on this movie for some years, and i hope a fresh post will spark some new comments. this is a film that i remembered only snippets of from childhood, and only saw recently when i tired of waiting for fox to honour its own past, and hunted down the korean dvd (in english, but with unremovable korean subtitles). i wont go through another long plot description - suffice to say that seeing it for the first time in its proper widescreen format left me agape at the vistas and the scope of the film. the matte paintings still hold up, and the palace sets are truly breathtaking. but it is the smaller scale details that lend this film its depth and richness, offering a glimpse into the lifestyles of egypts poor as well as its elite. the bazaars, hovels, docks, embalming houses, and taverns are as fascinating as pharaohs throne room. while errors abound on the large scale (most notably the dynastic succession), the details are more meticulously researched than the vast majority of hollywoods films. visually, its not without its flaws - the interiors are often too overly lit and colourful to blend seamlessly with the exteriors. nevertheless, this is a movie that should be credited for being as audacious in the small as it is in the large. tedious? in parts, absolutely. overacted? underacted? yes, both - though understated might be a more apt description. too long? absolutely not. i wished they had spent more time with sinuhes experiences in the house of death, and among the hittites, and less with his romance with nefer, though. historically inaccurate? yes, that too, but so was shakespeare. nobody chastises him for it. i appreciate historical accuracy as much as the next guy, but ultimately it has to be remembered that cinema is theater, not a history lesson.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this has to be one of the biggest misfires ever...the script was nice and could have ended a lot better.the actors should have played better and maybe then i would have given this movie a slightly better grade. maybe hollywood should remake this movie with some little better actors and better director.sorry guys for disappointment but the movie is bad.if i had to re-watch it it would be like torture. i dont want to spoil everyones opinion with mine so..my advice is watch the movie first..see if u like it and after vote(do not vote before you watch it ! ) and by the way... have fun watching it ! dont just peek...watch it till the end :))))))))) !!\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this is one of those movies i watched, and wondered, why did i watch it? what did i find so interesting about it? being a truck driver myself, i didnt find it very realistic. no, ive never used a lot lizard, nor have i ever seen, nor heard about one traveling around the country in a brand new seventy thousand dollar rv, either.same thing about a pimp whom has never sampled the lady in question (until the end of the movie, and well, he still really didnt...), and only getting 50 bucks a cut, when the prostitute gets $200.00 (well, $150.00 after his cut, yeah...).i still laugh at the lot lizard comment ivey made (thems lot lizards, theyll screw anything with 20 bucks, and some are men dressed as woman... or something equally as weird), meaning, were better then them, as we may still be prostitutes, but we get paid better.other then that, its just a story of a young woman whom wanted something more from life then a dead end job while living at home (shes 18, remember?) and embarrassed by her mother basically doing the same thing (dead end job). at least she had a roof over her head and a job. she turned five tricks on the road... i wonder if the $750.00 she made was worth it? id guess not.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "the worst movie ive seen in years (and ive seen a lot of movies). acting is terrible, there is no plot whatsoever, there is no point whatsoever, i felt robbed after i rented this movie. they recommended it to me mind you! a disgrace for terrible movies! stay away from this terrible piece of c**p. save your money !\n",
      "Sentiment: [[0.5 0.5]]\n",
      "five medical students (kevin bacon, david labraccio; william baldwin, dr. joe hurley; oliver platt, randy steckle; julia roberts, dr. rachel mannus; kiefer sutherland, nelson) experiment with clandestine near death & afterlife experiences, (re)searching for medical & personal enlightenment. one by one, each medical students heart is stopped, then revived.under temporary death spells each experiences bizarre visions, including forgotten childhood memories. their flashbacks are like childrens nightmares. the revived students are disturbed by remembering regretful acts they had committed or had done against them. as they experience afterlife, they bring real life experiences back into the present. as they continue to experiment, their remembrances dramatically intensify; so much so, some are physically overcome. thus, they probe & transcend deeper into the death-afterlife experiences attempting to find a cure.even though the dvd was released in 2007, this motion picture was released in 1990. therefore, kevin bacon, william baldwin, julia roberts & kiefer sutherland were in the early stages of their adult acting careers. besides the plot being extremely intriguing, the suspense building to a dramatic climax & the script being tight & convincing, all of the young actors make flatliners, what is now an all-star cult semi-sci-fi suspense. who knew 17 years ago that the film careers of this young group of actors would skyrocket? i suspect that director joel schumacher did.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "the mill on the floss was one of the lesser novels by mary ann evans, who wrote under the male pseudonym george eliot. i tried to read this dull and very turgid novel years ago, but was unable to finish it. ill review this film version solely on its own merits, as i dont know how faithfully it follows the original novel.the films opening credits are printed in an old english typeface that suggests the mediaeval period, and so its a very poor choice for a film with a 19th-century setting. (on the other hand, about halfway into the film, we see a close-up shot of a handbill advertising an estate auction. this handbill is set in authentic victorian type fonts, and looks *very* convincing.) most of this film is extremely convincing in its depiction of the architecture and clothing of early 19th-century england. the precise location of this films story is never disclosed, but - judging by the actors accents - id place it as somewhere in the cotswolds, perhaps warwickshire.the plot, what there is of it, involves a mill that changes hands a couple of times (over a couple of decades) between two rival families, one wealthy and one working-class. i disagree with another imdb reviewer who claims that james mason has only a small role in this film. mason has the largest and most central role in this drama, as the scion of the wealthier family. as the spoilt and petulant tom tulliver, mason is darkly brooding and impetuous. his performance here belongs in a better film: it made me want to see wuthering heights recast with mason as heathcliff.as this is a multi-generational saga (something which george eliot did much better in middlemarch), several of the main roles in this film are split among two actors apiece: child actors in the prologue, adults in the main narrative. the prologue of this film features a very well-written scene, establishing tom tulliver as wilful and bully-ragging from an early age, and young philip wakeham as decent and thoughtful. through hard labour, philip has earned a halfpenny: tom tries to bully it away from him, but is unwilling to take the coin by brute force: he wants philip to *give* it to him. all the child actors in this movie, male and female, are talented and attractive. unfortunately, all of the children speak their dialogue in posh plummy-voiced accents that are utterly unlike the accents of the actors and actresses who play those same roles as adults. this discrepancy calls attention to the staginess of the material. regrettably, none of the later scenes are as good as this prologue.the climax features a crowd of labourers in a rainstorm, much better paced and photographed than the earlier scenes. but modern viewers (in britain, at least) can no longer take this sort of material seriously. by now, practically every british comedian has done a trouble at t mill, squire comedy routine, parodying precisely this subject matter, so i had difficulty watching this movie with a straight face.the character actress martita hunt is good in a small role, but the opening credits (in that old english typeface) misspell her forename as marita. ill rate this dull movie 3 points out of 10: one point apiece for james masons performance, the early scene with the children, and the authentic victorian typesetting in that auctioneers handbill.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "i just saw this film at the phoenix film festival today and loved it. the synopsis was listed in our program as an old shakespearean actor invites his three children to his suicide party. i wasnt sure if i was going to see it because when i read about it i liked the idea of a suicide party it sounded very interesting to me, but old shakespearean actor had me worried that the film would be kind of dry and boring. but i decided to give it a try. i am glad that i did. it was not dry and boring in the least, that dialogue was great, funny in a clever way, but not pretentious and difficult to understand. peter falk was terrific in this role, he stole the show. i also was pleasantly surprised by laura san giacomos performance, usually she bugs me, but i enjoyed watching her in this film very much. i think judge reinholds part could have been done better by another actor, at times he seemed kind of cheesy and it looked like acting, not like you were just watching this character. but the movie was so good i was able to forgive one actors awkwardness. i would recommend this film to anyone and have already told a few people to see it as soon as it is available to the general public. who knew suicide could be so hilarious?\n",
      "Sentiment: [[0.5 0.5]]\n",
      "the love letter is one of those movies that could have been really clever, but they wasted it. focusing on a letter wreaking havoc in a small town, the movie has an all-star cast with nothing to do. tom selleck and alice drummond had so recently co-starred in the super-hilarious in & out (also about an upset in a small town), in which they were both great, but here they look as though theyre getting drug all over the place. i cant tell what the people behind the camera are trying to do here (if anything), but they sure didnt accomplish anything. how tragic, that a potential laugh riot got so sorrowfully wasted.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "another fantastic offering from the monkey island team and though it was a long time coming and had to survive the departure of ron gilbert its another worthy installment. my only gripe is that it was a little short seeming in comparison to the previous two, though that might be because of a glorious lack of disk-swapping. roll on mi4.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this was included on the disk shorts: volume 2--a rather dull collection of short films. shorts are among my favorite style of films but somehow the people assembling this second collection had a hard time finding quality content--and it wasnt nearly as good as the first volume or other shorts collections. this short film feels like its woefully incomplete. there is a story, but so much in unanswered that the viewer, like me, feels a bit left out and unfulfilled.the film begins with a woman, her boyfriend and her westie (thats a dog, by the way) going to a lonely beach. the lady speaks with an accent that, at times, is a bit difficult to follow. given that i am hard of hearing, i sure would have loved if it had been closed captioned. anyway, the boyfriend goes for a swim while she naps. when she awakens, her dog is gone. she panics and makes the guy follow her all about looking for the dog. they spend most of the time arguing and being disagreeable. then, out of the blue, they stop to have sex. later, they find the dog--end of story.as far as the characters go, both seemed rather dysfunctional and unlikable. she was a fussy and demanding lady and he seemed to have contempt for her. when you wondered why they were together, their little sex break showed what bond kept them together.some might like the characterizations--i kept finding the people irritating and unreal--more like caricatures than people you might meet or know. also, the payoff for all this just isnt worth the wait (unless you want to see the guy naked).\n",
      "Sentiment: [[0.5 0.5]]\n",
      "im not really much of an abbott & costello fan (although i do enjoy whos on first) and, to be honest, there wasnt much in this movie that would inspire me to watch any more of their work. it wasnt really bad. it had some mildly amusing scenes, and actually a very convincing giant played by buddy baer, but somehow, given the fame of the duo and the esteem in which theyre generally held, i have to say i was expecting more. as the story goes, the pair stumble into a babysitting job, and during the reading of jack & the beanstalk as a bedtime story (with the kid reading it to costello), costellos jack falls asleep and dreams himself into the story. theres a wizard of oz kind of feel to the story, in that the characters in the dream are all the equivalents of real-life acquaintances of jack, and the movie opens in black & white and shifts to colour during the dream sequence. the fight scenes between jack and the giant and the dance scene between jack and polly (dorothy ford) are among the amusing parts of the movie. polly, of course, also leads to one of the questions of the movie - what happened to her? jack and gang apparently left her behind in the giants castle! i know - it was just a dream, so who cares. still - i wondered. there were also a couple of cute song and dance routines. my 4 year old giggled a bit during this, so she was able to appreciate some of the humour. i found it to be an acceptable timewaster, but certainly not anything that would convince you of abbott and costello as comic geniuses. 4/10\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this movie was dreadful. biblically very inaccurate. moses was 80 years old when he led the people out of egypt, the movie has him about forty. moses was about forty when he fled egypt, was gone for forty years, and was with them wandering for forty years. moses was 120 years old when he died, and was denied the privilege of crossing over to the promised land. i realize movies use a lot of poetic license as the biblical account isnt that long, but, if making a biblical movie they still need to reflect the facts known, and keep the general flavor of the main biblical character, this movie fails in this aspect, and in many others.even though the 1956 version has its problems as well, theatrically it was much better.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "i dont think ive ever gave something a 1/10 rating, but this one easily gets the denomination. i find it hard just to sit through one of his jokes. its not just that the jokes are so bad, but combine that with the fact that carson daily has zero charisma, cant set up or finish a punchline, and youve got a late night comedy recipe that will really turn your stomach.i have watched the show, never in its entirety, but many times still. it just creeps up on me after conan. i usually watch a minute or two just to see if carson daily is still the worst talk show host ever.actually if you ever do see him interviewing a guest, its just that, an interview. i feel so sorry every time he has a guest on and their confused smiles try to mask their body language thats screaming, get me the hell away from this freak! i do recommend watching the show, not for a laugh, but to ponder, how he got on the air and what hes still doing there. watch as much as you can, i think you will find its complete awkwardness...interesting.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "excellent story-telling and cinematography. poignant, biting social commentary.superb effects. well-filmed and acted.however, the parallel action between the present and the travel adventures (though very well done) at times drags on a little too much (about 3 hrs), and over-interrupts the flow of the story.i first read the book as a child, and enjoyed the parts about the giants and the tiny people -- but the book lost me when it got to the floating island and the land of the yahoos! well, although the adventure plot may sound like a childrens story, its in fact a very adult story, full of symbolism about the moral decay in england at the time of jonathan swift, the author of the novel that the film is based upon.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "i completely forgot that id seen this within a couple of days, which is pretty revealing in itself. the umpteenth version of gaston phantom of the opera lerouxs locked-door country-house mystery, i had heard that it was an engaging and witty update. so it appeared from the likable title sequence and a few neat touches in the opening scene, but the film very quickly ground to a halt and became vaguely tedious and wholly unsatisfying.as a mystery the major problem is that it is fundamentally unsolvable by the audience: like the worst agatha christies, it depends on a character appearing in the final act with a wealth of background information that we have not been privy to. as a film, be it comedy or thriller, the crucial problem is that characterisation is almost non-existent. with the exception of the killer, everyone is a face-value version of the typical suspects in the typical country-house murder story - reporter, endangered heiress, suspicious fiancé, scatterbrained scientist father (a surprisingly poor michel lonsdale), etc. theres no depth and little of interest, and the frequently over-ripe or misjudged performances dont help. you frankly dont care about anyone in it, so theres no jeopardy or suspense. only claude rich and, in the last reel, pierre arditi get anything to work with, and only in the last reel does the film get close to a sense of resonance that is too fleeting to be really effective.for the rest, we get endless exposition and a couple of ineffective would-be comic set pieces (a promising one with a photographer trapped inside a grandfather clock is just too poorly thought through to pay off), with dennis podalydes reduced to irving the explainer for the last third of the picture. im not fond of country-house movies or agatha christie style whodunits, so those who are might cit it a lot more slack, but i found it a poor show. as rich says when the mystery is revealed, its all rather something of a disappointment.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "i like action movies. i have a softspot for b flicks with bad dialogue and wooden acting. so, ive been wracking my brain to come up with one of my guilty pleasures that was worse than this blockbuster. i cant. youd be hard pressed to put together a bigger piece of cr*p than this bruce willis vehicle.armageddon is the story (and i use that term loosely) of a team of super-drillers flying off to destroy an asteroid before it destroys the earth. realistic? not really. but who cares? its an action flick. im not blasting the premise.minor spoilers:the movie begins with a couple of scenes designed to introduce the threat and the characters. bruce willis is the tough-as-nails leader of the team, and spends his first bit of screen time chasing around ben affleck with a gun for the unforgiveable act of sleeping with his daughter. for some reason, that didnt make me laugh. it was forced, like everything in this movie.the team is called in because theyre the only people in the whole wide world who can drill the asteroid. okay, im prepared to accept that premise if it gets us to the action - the supposed meat of the movie. more attempts at humor, with each character going out to do some crazy, nutty thing before blast off. again, lame. finally, they take off. heres where the movie really pi**ed me off. they arrive on the rock, and set to work. would you believe it, nothing works right and everything has a suspenseful countdown!!! whoah! ten, nine, eight... one - oh, surprise surprise we saved the day again!!and dont even get me started on the jerky camerawork. when i saw it in the theater i thought i was going to be sick. i can only assume they were trying to cover up the gargantuan holes left by the insipid performances by cutting away to a different shot every few seconds (and this from someone raised on mtv - mr short attention span himself).just when i thought it couldnt get any worse... wait - theres a manufactured tearjerker ending that was so tacked on it made the rest of the film a virtual citizen kane.summary: the witticisms werent witty. the plot - well, i said id let that one go. the acting was bad. really bad. even billy bob couldnt rise above the script, which was worse. camerawork - again, bad. (i didnt even mention the dumbest love scene in the history of motion pictures - think animal crackers).rating: 1 out of 10. (im giving a half point for steve buscemi, who makes me smile against my will and another half point for the times i was able to look at the lovely liv tyler and attempt to ignore her acting performance) this is far and away the worst movie ive gone to see in the theater... ever.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this is one of the worst sandra bullock movie since speed 2 but not quite that bad. i really lost it with those out of the blue not so special effect. guys, if youre an insomniac go with your girl to see this movie. i give it three sleepies!\n",
      "Sentiment: [[0.5 0.5]]\n",
      "watched this flick on saturday afternoon cable. man, did it drag. i got the metaphors, symbolism, and all that stuff. no, i didnt care one way or another about the sexuality of the characters. but, the pacing of the story and the scripting almost put me to sleep.that is..... until ruth marshall got naked. if youre a breast-man who is not homo-phobic, you may want to rent it. ruth has a lesbian sex scene thats pretty hot, and then a hetero sex scene that is a notch higher than most standard movie fare. her jiggly d-cups made the film worth the watch.--the mighty avatar\n",
      "Sentiment: [[0.5 0.5]]\n",
      "i went to see tkia with high expectations, which might have influence on my opinion on it. i have seen all of the dogme films, and this tkia, is by far the worst. the story intertwines with themes from shakespeares play: king lear, but never succeeds in capturing the audience and making them care. the directing of the actors is very loose, even for dogme style movies, and results in poor undefinable acting. the story lacks any dynamics whatsoever, and i lost interest very shortly. there are some scenes in the film which are there to shock the viewer, but i dont think they enhanced the story at all. mifunes sidste sang and festen are both dogmefilms that proved to be well directed, and had good storylines, so i shall look forward to better dogmefilms in the future. perhaps aake sandgrens an invisible man-dogme 6 will prove to lift the quality again. for he is, like vinterberg and s.k. jacobsen a skilled and educated director.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "all credit to writer/director gilles mimouni who fashioned this winding, twisting tale of deceit and betrayal. while keeping the utmost control, he maintains the audience at arms length, never allowing them to become completely aware of the goings on. even his clever denouement has you guessing.the three central performances are also top class, with vincent cassel, romane bohringer and monica bellucci doing their utmost to add to the mystery. jean-phillippe ecoffey supplements strength in his supporting role. to give away plot details or character specifics would not be fair.thierry arbogast uses the camera effectively to sweep us through this enigma, and cardine biggerstaffs editing keeps the story a step ahead of us. the theme from peter chase is sublime in its marriage to the ideal of the script.many may say gilles mimouni is trying to confront several deeper issues on the them of love. for me this is simply a haunting, elusive riddle that weaves a fascinating web. only the french are capable of such tantalisation. hollywood would have ruined this with a happy ending.monday, march 2, 1998 - hoyts croydonno-one does thriller quite like the french. when they get it right, they really get it right.vincent cassell is intriguing as the deceptive max, romane bohringer obsessive as the new lisa, and monica bellucci is mysterious as the first lisa. the plot from gilles mimouni is a whirlwind of deliberate deception and fatally crossed wires.all credit must go to his manipulation of the clever plot, and the performances from the three leads. as lucien, jean-phillippe ecoffey is strong and emotional.friday, january 15, 1999 - video\n",
      "Sentiment: [[0.5 0.5]]\n",
      "as a writing teacher, there are two ending i never allow my students to use: then i woke up and then i got run over by a truck. i am now going to add, then i got a bump on the head. i feel its utterly unfair to use these tricks to cover up a lack of imagination. the whole issue of transmigration could have been handled with some intelligence and craft, yet, in this film, they either couldnt or wouldnt do that. im not saying its totally worthless, but it is so predictable in its progress, except for the stupid ending. there are even gangsters who go to the police to get help from this guy. they should have done him in immediately. its just a forgettable, borderline horror/sci fi film, with nothing new to offer.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "i dont know why this has gotten any decent reviews as it could be the weakest horror comedy ive ever seen. englund is just in it for a cameo and his performance is as unnecessary as most of the lame attempts at jokes (and scares). the direction is terrible and the acting is worse. it seems like every year producers are trying to make another evil dead but these weak unoriginal attempts are just stepping on the memory of a true horror classic.whether its filmmakers saying,this isnt a remake but its an 80s throwback (which is just as unoriginal in my opinion - hatchet) or people trying to plug this with other horror classics, its still misleading and wont make up for the lack of scares, horror, comedy, or even a decent movie for that matter.avoid at all costs!!!\n",
      "Sentiment: [[0.5 0.5]]\n",
      "this film was released in the uk under the name blood rites. it was banned outright and never submitted again for release.as the ghastly ones, it was supposedly a hit with the horror hungry denizens of new york citys famed 42nd street grindhouse circuit. if you are looking for some bloody horror, then you will find it in this film.unfortunately to see the developmentally disabled colin (hal borske) chomp down on a live rabbit, you have to put up with shaky 16mm camera work that makes ed wood look positively marvelous.three sisters are to spend three days in the family homestead with their husbands before the old mans money is disbursed. naturally, in such a situation, people start dropping dead. family secrets are exposed and lots of blood is spilled, especially during a gruesome dismemberment.maybe it was the bunny bit that the brits objected to, i know i did.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "uncle fred olen ray once again gives us a little of his retromedia goodness in the form of this soft-core cinemax non-classic.a numb-nut pair are out looking at rocks when they come across a swirling vortex a black hole as the intelligent dolts put it. pretty soon an attractive cave girl from one million years ago happens into out time-line and she beds her way into the future. pretty soon her studly other half makes his way into the future as well and blazes a path through the beds of the future.ray again delivers a passable (but barely) smut-fest that has horrible acting but some decent skin. yeah its barely titillating but heh! if worse comes to worse it will cure you insomnia.s10 reviews: 1/5 or 3/10\n",
      "Sentiment: [[0.5 0.5]]\n",
      "ok, its watchable if you are sick in bed or have nothing else to do. the suspension of disbelief required to get through this movie is significant though. first, in todays modern society do you believe college coeds get that committed to someone in that short of a period of time even if you are a virtuous habitat volunteer who likes autistic kids? and the 2 week romance blossoms into a letter exchange that leads to johns conflict of whether to re-enlist right after 9/11/01...really? he asks her what to do? every guy we know was not gonna be sitting on the sidelines after those towers came down(my husband was one of them and i love him and am proud of him for going) johns character is so flat. hes nearly expressionless the entire movie. hes good looking but not spec ops...he seems unsure not confident, quiet instead of a hell-raiser, no tattoos, gets into a fight with the preppy boys that is nothing more than a pushing match really...walks around without a cover on his head nearly the whole movie...and there are military technical flaws everywhere (epaulets upside down?). the war scenes are dumb...john and another guy heading off on their own...huh?, then other guy gets shot and john drags him 10 feet and starts giving buddy aid before securing the area or back-up arrival or even having their backs against cover...its a gunfight for gods sakes, you dont stop fighting until its over...heck i wanted to shoot john in the back. back home, when the truth is revealed and she spills the wine...we hated her for removing her shirt in his presence...wth? break his heart and then tease him into adultery? shes a head case trollop. best part of the movie is when he drives away from her...at least he had some self respect and honor there. overall unbelievable story and we generally did not care about these characters or their love. dismal!\n",
      "Sentiment: [[0.5 0.5]]\n",
      "if i was only allowed to watch one program in my entire life, i would definitely have to pick the chasers war on everything. of all the satirical shows that have been on australian television, i found chaser to be the funniest of all. it is just so amazing, the boys arent afraid to do anything.whether its dress up as hitler to get into a polish club, or push a massive ball of string around melbourne to try out the tourism ads or rock up to the coke factory naked in a bath with $2.40 to buy some water. the chaser boys will go there.in agreement with the comments above (and/or below) the chasers war on everything is more popular than their previous program cnnnn. but cnnnn was just as funny. some unforgettable moments from that show... clean up cambodia!!! classic.so anyway to stop me from ranting further, i strongly advise you to at least give chaser a chance, youll more than likely find it hilarious!!!!\n",
      "Sentiment: [[0.5 0.5]]\n",
      "a truly terrific, touching film. female melodrama at its finest, with a lot of comedy: great dialogue, characters and writing. any woman can relate to the story because its a classic: youre in love with mr. right but he has no interest in you until some guy who seems completely wrong comes along and you fall head-over-heels in love. but of course, its not that simplistic. the characters are real and all of the performances are perfect. the movie is hilarious as well, every scene skewers society. id recommend this film to anyone who loves a well-written screenplay of humor and melodrama. you can relate to every character and the plot moves in unexpected directions. a great, underrated movie.\n",
      "Sentiment: [[0.5 0.5]]\n",
      "pepe le pew can either really creep you out or totally sweep you off your feet. either way, you cant help feeling a little awe on beholding this classic wb character. this commentater personally believes that pepe was the inspiration behind other would be animated casanovas today from cartoon networks johnny bravo to disneys lumiere from beauty and the beast. his unique brand of love making is to be wondered at in todays world where his antics would normally be slapped with a sexual harassment warrant and at least a 50m distance from all his victims. in this particular cartoon, a world weary cat decides to do an ultimate makeover and earn some respect for a change for pretending to be a skunk. all goes well, until pepe arrives and promptly pursues the unfortunate feline with his overwhelmingly enthusiastic love-making.the groundwork for pepes many trademarks are laid in this cartoon. from his adorable frenchified love calls to that aggravatingly calm hop-chase of his. this cartoon only goes to show that as far as the world of cartoon fantasy is concerned, the most ardent wooer can go the distance...and have his beloved pig-eon leaving dust trails behind them.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-d145f3a22fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sentiment: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, rev in enumerate(test_texts):\n",
    "    print(rev)\n",
    "    test_input = test_input_data[i].copy()\n",
    "    test_input = np.reshape(test_input, (1,test_input.shape[0], test_input.shape[1], test_input.shape[2]))\n",
    "    sentiment = np.argmax(model.predict(test_input))\n",
    "    print('Sentiment: ' + str(sentiment))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
