{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_terms_with_noise(json_file, num_samples, noise_threshold):\n",
    "    with open(json_file) as f:\n",
    "        med_terms_dict = json.load(f)\n",
    "    med_terms = list(med_terms_dict.keys())\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    cnt = 0\n",
    "    while cnt < num_samples:\n",
    "        for term in med_terms:\n",
    "            if cnt < num_samples :\n",
    "                input_text = noise_maker(term, noise_threshold)\n",
    "                input_text = input_text[:-1]   \n",
    "\n",
    "                target_text = '\\t' + term + '\\n'\n",
    "\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])        \n",
    "                cnt += 1\n",
    "    return input_texts, target_texts, gt_texts, med_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspecial_chars = [\\'\\\\\\', \\'/\\', \\'-\\', \\'—\\' , \\':\\', \\'[\\', \\']\\', \\',\\', \\'.\\', \\'\"\\', \\';\\', \\'%\\', \\'~\\', \\'(\\', \\')\\', \\'{\\', \\'}\\', \\'$\\', \\'#\\']\\nw = \\'-Hel2*l(3o\\'\\nw = w.lower()\\nprint(w)\\nw_ = re.sub(r\\'[\\\\\\\\/\\\\-\\\\—\\\\:\\\\[\\\\]\\\\,\\\\.\"\\\\;\\\\%\\\\~\\\\(\\\\)\\\\{\\\\}\\\\$\\\\#\\\\?\\\\●\\\\@\\\\+\\\\-\\\\*\\\\d]\\', r\\'\\', w)\\n#w_ = re.sub(r\\'[#?]\\', r\\'\\', w)\\nprint(w_)\\nprint(re.findall(r\\'[\\\\\\\\/\\\\-\\\\—\\\\:\\\\[\\\\]\\\\,\\\\.\"\\\\;\\\\%\\\\~\\\\(\\\\)\\\\{\\\\}\\\\$\\\\#\\\\?\\\\●\\\\@\\\\+\\\\-\\\\*\\\\d]\\', w))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#']\n",
    "w = '-Hel2*l(3o'\n",
    "w = w.lower()\n",
    "print(w)\n",
    "w_ = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w)\n",
    "#w_ = re.sub(r'[#?]', r'', w)\n",
    "print(w_)\n",
    "print(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', w))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        processed_word = word.lower()\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, word in enumerate(word_tokenize(input_text)):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[process_word(word)]\n",
    "        for t, word in enumerate(word_tokenize(target_text)):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[process_word(word)]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start word.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[process_word(word)]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        '''\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            #weights=[np.eye(num_encoder_tokens)],# Load from GloVe\n",
    "                            mask_zero=True, trainable=True)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    print(decoder_outputs)\n",
    "    print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    print(encoder_inputs)\n",
    "    print(encoder_outputs)\n",
    "    print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, word in enumerate(word_tokenize(text)):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[word]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 1000000\n",
    "min_sent_len = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of lenghts\n",
    "lengths = []\n",
    "for text in input_texts:\n",
    "    lengths.append(len(text))\n",
    "    lengths.append(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fba74a74cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = plt.hist(lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1180.,   940.,  1384.,  1322.,  1174.,   722.,   592.,   536.,\n",
       "         332.,   286.,   242.,   186.,   180.,   214.,   112.,   154.,\n",
       "          68.,    82.,    58.,    88.,    70.,    64.,    32.,    30.,\n",
       "          20.,    26.,    32.,    24.,    58.,    14.,    64.,     6.,\n",
       "          28.,    16.,    24.,    28.,     8.,    18.,    14.,    18.,\n",
       "          12.,    24.,    14.,    28.,    14.,     4.,    12.,    44.,\n",
       "           4.,     6.,     2.,     4.,     6.,     0.,    36.,     0.,\n",
       "           4.,     4.,     8.,     6.,    14.,     8.,     8.,     8.,\n",
       "           2.,     0.,     6.,     2.,     2.,     4.,    12.,    14.,\n",
       "           8.,    12.,     6.,     0.,     4.,     4.,     2.,     0.,\n",
       "           2.,     0.,     4.,     6.,     0.,     4.,    14.,    26.,\n",
       "           4.,     0.,     2.,     4.,     4.,     2.,     4.,     2.,\n",
       "           6.,     0.,     2.,     2.,     2.,     6.,     4.,     2.,\n",
       "          40.,     4.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,    20.,     8.,    18.,     4.,     0.,\n",
       "           0.,     0.,     0.,     6.,    24.,     2.,     0.,     0.,\n",
       "           0.,     2.,     2.,     0.,     0.,     2.,     2.,     4.,\n",
       "           2.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     4.,     4.,\n",
       "          16.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     4.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     2.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   ,     4.878,     9.756,    14.634,    19.512,    24.39 ,\n",
       "          29.268,    34.146,    39.024,    43.902,    48.78 ,    53.658,\n",
       "          58.536,    63.414,    68.292,    73.17 ,    78.048,    82.926,\n",
       "          87.804,    92.682,    97.56 ,   102.438,   107.316,   112.194,\n",
       "         117.072,   121.95 ,   126.828,   131.706,   136.584,   141.462,\n",
       "         146.34 ,   151.218,   156.096,   160.974,   165.852,   170.73 ,\n",
       "         175.608,   180.486,   185.364,   190.242,   195.12 ,   199.998,\n",
       "         204.876,   209.754,   214.632,   219.51 ,   224.388,   229.266,\n",
       "         234.144,   239.022,   243.9  ,   248.778,   253.656,   258.534,\n",
       "         263.412,   268.29 ,   273.168,   278.046,   282.924,   287.802,\n",
       "         292.68 ,   297.558,   302.436,   307.314,   312.192,   317.07 ,\n",
       "         321.948,   326.826,   331.704,   336.582,   341.46 ,   346.338,\n",
       "         351.216,   356.094,   360.972,   365.85 ,   370.728,   375.606,\n",
       "         380.484,   385.362,   390.24 ,   395.118,   399.996,   404.874,\n",
       "         409.752,   414.63 ,   419.508,   424.386,   429.264,   434.142,\n",
       "         439.02 ,   443.898,   448.776,   453.654,   458.532,   463.41 ,\n",
       "         468.288,   473.166,   478.044,   482.922,   487.8  ,   492.678,\n",
       "         497.556,   502.434,   507.312,   512.19 ,   517.068,   521.946,\n",
       "         526.824,   531.702,   536.58 ,   541.458,   546.336,   551.214,\n",
       "         556.092,   560.97 ,   565.848,   570.726,   575.604,   580.482,\n",
       "         585.36 ,   590.238,   595.116,   599.994,   604.872,   609.75 ,\n",
       "         614.628,   619.506,   624.384,   629.262,   634.14 ,   639.018,\n",
       "         643.896,   648.774,   653.652,   658.53 ,   663.408,   668.286,\n",
       "         673.164,   678.042,   682.92 ,   687.798,   692.676,   697.554,\n",
       "         702.432,   707.31 ,   712.188,   717.066,   721.944,   726.822,\n",
       "         731.7  ,   736.578,   741.456,   746.334,   751.212,   756.09 ,\n",
       "         760.968,   765.846,   770.724,   775.602,   780.48 ,   785.358,\n",
       "         790.236,   795.114,   799.992,   804.87 ,   809.748,   814.626,\n",
       "         819.504,   824.382,   829.26 ,   834.138,   839.016,   843.894,\n",
       "         848.772,   853.65 ,   858.528,   863.406,   868.284,   873.162,\n",
       "         878.04 ,   882.918,   887.796,   892.674,   897.552,   902.43 ,\n",
       "         907.308,   912.186,   917.064,   921.942,   926.82 ,   931.698,\n",
       "         936.576,   941.454,   946.332,   951.21 ,   956.088,   960.966,\n",
       "         965.844,   970.722,   975.6  ,   980.478,   985.356,   990.234,\n",
       "         995.112,   999.99 ,  1004.868,  1009.746,  1014.624,  1019.502,\n",
       "        1024.38 ,  1029.258,  1034.136,  1039.014,  1043.892,  1048.77 ,\n",
       "        1053.648,  1058.526,  1063.404,  1068.282,  1073.16 ,  1078.038,\n",
       "        1082.916,  1087.794,  1092.672,  1097.55 ,  1102.428,  1107.306,\n",
       "        1112.184,  1117.062,  1121.94 ,  1126.818,  1131.696,  1136.574,\n",
       "        1141.452,  1146.33 ,  1151.208,  1156.086,  1160.964,  1165.842,\n",
       "        1170.72 ,  1175.598,  1180.476,  1185.354,  1190.232,  1195.11 ,\n",
       "        1199.988,  1204.866,  1209.744,  1214.622,  1219.5  ,  1224.378,\n",
       "        1229.256,  1234.134,  1239.012,  1243.89 ,  1248.768,  1253.646,\n",
       "        1258.524,  1263.402,  1268.28 ,  1273.158,  1278.036,  1282.914,\n",
       "        1287.792,  1292.67 ,  1297.548,  1302.426,  1307.304,  1312.182,\n",
       "        1317.06 ,  1321.938,  1326.816,  1331.694,  1336.572,  1341.45 ,\n",
       "        1346.328,  1351.206,  1356.084,  1360.962,  1365.84 ,  1370.718,\n",
       "        1375.596,  1380.474,  1385.352,  1390.23 ,  1395.108,  1399.986,\n",
       "        1404.864,  1409.742,  1414.62 ,  1419.498,  1424.376,  1429.254,\n",
       "        1434.132,  1439.01 ,  1443.888,  1448.766,  1453.644,  1458.522,\n",
       "        1463.4  ,  1468.278,  1473.156,  1478.034,  1482.912,  1487.79 ,\n",
       "        1492.668,  1497.546,  1502.424,  1507.302,  1512.18 ,  1517.058,\n",
       "        1521.936,  1526.814,  1531.692,  1536.57 ,  1541.448,  1546.326,\n",
       "        1551.204,  1556.082,  1560.96 ,  1565.838,  1570.716,  1575.594,\n",
       "        1580.472,  1585.35 ,  1590.228,  1595.106,  1599.984,  1604.862,\n",
       "        1609.74 ,  1614.618,  1619.496,  1624.374,  1629.252,  1634.13 ,\n",
       "        1639.008,  1643.886,  1648.764,  1653.642,  1658.52 ,  1663.398,\n",
       "        1668.276,  1673.154,  1678.032,  1682.91 ,  1687.788,  1692.666,\n",
       "        1697.544,  1702.422,  1707.3  ,  1712.178,  1717.056,  1721.934,\n",
       "        1726.812,  1731.69 ,  1736.568,  1741.446,  1746.324,  1751.202,\n",
       "        1756.08 ,  1760.958,  1765.836,  1770.714,  1775.592,  1780.47 ,\n",
       "        1785.348,  1790.226,  1795.104,  1799.982,  1804.86 ,  1809.738,\n",
       "        1814.616,  1819.494,  1824.372,  1829.25 ,  1834.128,  1839.006,\n",
       "        1843.884,  1848.762,  1853.64 ,  1858.518,  1863.396,  1868.274,\n",
       "        1873.152,  1878.03 ,  1882.908,  1887.786,  1892.664,  1897.542,\n",
       "        1902.42 ,  1907.298,  1912.176,  1917.054,  1921.932,  1926.81 ,\n",
       "        1931.688,  1936.566,  1941.444,  1946.322,  1951.2  ,  1956.078,\n",
       "        1960.956,  1965.834,  1970.712,  1975.59 ,  1980.468,  1985.346,\n",
       "        1990.224,  1995.102,  1999.98 ,  2004.858,  2009.736,  2014.614,\n",
       "        2019.492,  2024.37 ,  2029.248,  2034.126,  2039.004,  2043.882,\n",
       "        2048.76 ,  2053.638,  2058.516,  2063.394,  2068.272,  2073.15 ,\n",
       "        2078.028,  2082.906,  2087.784,  2092.662,  2097.54 ,  2102.418,\n",
       "        2107.296,  2112.174,  2117.052,  2121.93 ,  2126.808,  2131.686,\n",
       "        2136.564,  2141.442,  2146.32 ,  2151.198,  2156.076,  2160.954,\n",
       "        2165.832,  2170.71 ,  2175.588,  2180.466,  2185.344,  2190.222,\n",
       "        2195.1  ,  2199.978,  2204.856,  2209.734,  2214.612,  2219.49 ,\n",
       "        2224.368,  2229.246,  2234.124,  2239.002,  2243.88 ,  2248.758,\n",
       "        2253.636,  2258.514,  2263.392,  2268.27 ,  2273.148,  2278.026,\n",
       "        2282.904,  2287.782,  2292.66 ,  2297.538,  2302.416,  2307.294,\n",
       "        2312.172,  2317.05 ,  2321.928,  2326.806,  2331.684,  2336.562,\n",
       "        2341.44 ,  2346.318,  2351.196,  2356.074,  2360.952,  2365.83 ,\n",
       "        2370.708,  2375.586,  2380.464,  2385.342,  2390.22 ,  2395.098,\n",
       "        2399.976,  2404.854,  2409.732,  2414.61 ,  2419.488,  2424.366,\n",
       "        2429.244,  2434.122,  2439.   ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  9.756\n",
      "Count of most probable lenght =  1384.0\n",
      "Min length =  4.878\n"
     ]
    }
   ],
   "source": [
    "max_sent_len =  h[1][np.argmax(h[0])]\n",
    "min_sent_len = h[1][1]\n",
    "print('Most probable length = ', max_sent_len)\n",
    "print('Count of most probable lenght = ', np.max(h[0]))\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len =  50#int(np.ceil(max_sent_len))\n",
    "min_sent_len = 4#int(np.floor(min_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  50\n",
      "Min length =  4\n"
     ]
    }
   ],
   "source": [
    "print('Most probable length = ', max_sent_len)\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR_tess, target_texts_tess, gt_tess = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "num_samples = 0\n",
    "OCR_data = os.path.join(data_path, 'output_handwritten.txt')\n",
    "input_texts_OCR_hand, target_texts_OCR_hand, gt_texts_OCR_hand = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texts_OCR\n",
    "#target_texts = target_texts_OCR\n",
    "input_texts_OCR = input_texts_OCR_tess + input_texts_OCR_hand\n",
    "target_texts_OCR = target_texts_tess + target_texts_OCR_hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "threshold = 0.9\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = target_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Medical Terms dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = os.path.join(data_path, 'abbrevs.json')\n",
    "threshold = 0.9\n",
    "num_samples = 0\n",
    "input_texts_MedTerms, target_texts_MedTerms, _, med_terms_dict = load_medical_terms_with_noise(json_file, num_samples, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'det.': 'let it be given', 'p.v.': 'through the vagina', 'PT': 'prothrombin time', 'Ta': 'tantalum', 'lat': 'lateral', 'BCG': 'bacille Calmette-Guérin', 'MM': 'mucous membrane', 'AK': 'above the knee', 'a.c., ac': 'before a meal', 'KI': 'potassium iodine', 'cg': 'centigram', 'mEq': 'milliequivalent', 'sol': 'solution, dissolved', 'PEFR': 'peak expiratory flow rate', 'MI': 'myocardial infarction', 'BMS': 'bone marrow suppression', 'TPI': '', 'EPS': 'extrapyramidal symptoms', 'O.S.': 'left eye', 'LUE': 'left upper extremity', 'ECG': 'electrocardiogram, electrocardiograph', 'S.E.': 'standard error', 'q.v.': 'as much as you please', 'ECMO': 'extracorporeal membrane oxygenation', 'ACLS': 'advanced cardiac life support', 'PND': 'paroxysmal nocturnal dyspnea', 'ASC-US': 'atypical squamous cells of undetermined significance', 'UV': 'ultraviolet', 'MDI': 'metered-dose inhaler', 'b.i.n.': 'twice a night', 'LLL': 'left lower lobe', 'ad sat.': 'to saturation', 'EIA': 'enzyme immunosorbent assay', 'dieb. tert.': 'every third day', 'BBB': 'blood-brain barrier', 'GnRH': 'gonadotropin-releasing hormone', 'TB': 'tuberculin', 'RAI': 'radioactive iodine', 'PNS': 'peripheral nervous system', 'D and C': 'dilatation and curettage', 'FHT': 'fetal heart tone', 'EP': 'extrapyramidal', 'VOE': 'VistA-Office Electronic Health Record', 'ASA': 'acetylsalicylic acid', 'dim.': 'halved', 'elix.': 'elixir', 'PPD': 'purified protein derivative (TB test)', 'q.2h.': 'every 2 hours', 'TG': 'thyroglobulin', 'GABA': 'gamma-aminobutyric acid', 'CNS': 'central nervous system', 'THR': 'total hip replacement', 'WF/BF': 'white female/black female', 'TUMA': 'transurethral microwave antenna', 'CEA': 'carcinoembryonic antigen', 'TNT': 'trinitrotoluene', 'FBS': 'fasting blood sugar', 'ARC': 'AIDS-related complex', 'at. wt.': 'atomic weight', 'ant.': 'anterior', 'RLL': 'right lower lobe', 'VORB': 'verbal order read back', 'LE': 'lower extremity', 'GDM': 'gestational diabetes mellitus', 'CHF': 'congestive heart failure', 't.d.s.': 'to be taken three times daily', 'cath': 'catheter', 'hor. decub.': 'bedtime', 'Pu': 'plutonium', 'TAH': 'total abdominal hysterectomy', 'ap': 'before dinner', 'Pt': 'platinum', 'ASCA': 'anti-', 'IOP': 'intraocular pressure', 'mist.': 'a mixture', 'OT': 'occupational therapy', 'REM': 'rapid eye movement', 'Tx': 'treatment', 'CLL': 'chronic lymphocytic leukemia', 'COX-2': 'cyclooxygenase 2 inhibitors', 'PAD': 'peripheral arterial disease', 'AIDS': 'acquired immunodeficiency syndrome', 'TURP': 'transurethral resection of the prostate', 'dr.': 'dram', 'ICS': 'intercostal space', 'DBP': 'diastolic blood pressure', 'DPat': 'diphtheria-acellular pertussis tetanus (vaccine)', 'ERCP': 'endoscopic retrograde cholangiopancreatography', 'Em': 'emmetropia', 'US': 'ultrasonic, ultrasound', 'ARDS': 'acute respiratory distress syndrome', 'T6': 'thoracic nerve pair 6', 'MAT': 'Miller Analogies Test', 'post.': 'posterior', 'Sed rate': 'sedimentation rate', 'dB': 'decibel', 'NSAID': 'nonsteroidal anti-inflammatory drug', 'TLC, tlc': 'tender loving care', 'Bi': 'bismuth', 'ASC': 'atypical squamous cells', 'SGA': 'small for gestational age', 'h/o': 'history of', 'tr, tinct.': 'tincture', 'AC': 'adrenal cortex', 'Hz': 'hertz (cycles per second)', 'EST': 'electroshock therapy', 'yo': 'years old', 'RLQ': 'right lower quadrant', 'GDS': 'Geriatric Depression Scale', 'INR': 'international normalized ratio', 'RDA': 'recommended daily/dietary allowance', 'IgG': 'immunoglobulin G', 'N&V, N/V': 'nausea and vomiting', 'FTT': 'failure to thrive', 'DNR': 'do not resuscitate', 'HAV': 'hepatitis A virus', 'Fe': 'iron', 'VZIG': 'varicella zoster immune globulin', 'AML': 'acute myelogenous (myeloblastic) leukemia', 'CPR': 'cardiopulmonary resuscitation', 'Al': 'aluminum', 'inf.': 'inferior', 'ATCC': 'American Type Culture Collection', 'ss': 'a half', 'EBV': 'Epstein-Barr virus', 't.i.n.': 'three times a night', 'ICP': 'intracranial pressure', 'GYN': 'gynecology', 'ROS': 'review of systems', 'USP': 'United States Pharmacopeia', 'RT': 'radiation therapy', 'lmp': 'last menstrual period', 'FUO': 'fever of unknown origin', 'part. vic': 'in divided doses', 'INF': 'interferon', 'GI': 'gastrointestinal', 'CR': 'conditioned reflex', 'IgE': 'immunoglobulin E', 'bpm': 'beats per minute', 'BE': 'barium enema', 'MCV': 'mean corpuscular volume', 'lig': 'ligament', 'NGT': 'nasogastric tube', 'S.D.': 'standard deviation', 'ult. praes.': 'the last ordered', 'IV': 'intravenous', 'SDAT': 'senile dementia of the Alzheimer type', 'NRC': 'normal retinal correspondence', 'TNF-I': 'tumor necrosis factor inhibitor', 'CIS': 'carcinoma in situ', 'ICD': 'implantable cardioverter defibrillator', 'A/CA': 'accommodative/convergence accommodation ratio', 'Co': 'cobalt', 'PFP, P4P': 'pay for performance', 'OCD': 'obsessive-compulsive disorder', 'PACU': 'postanesthesia care unit', 'PUBS': 'percutaneous umbilical blood sampling', 'PE': 'physical examination', 'CHD': 'congenital heart disease', 'NKA': 'no known allergies', 'WM/BM': 'white male/black male', 'NMS': 'neuroleptic malignant syndrome', 'Hy': 'hyperopia', 'BRM': 'biologic response modifier', 'O.D.': 'right eye', 'p.r.n.': 'as needed', 'ICU': 'intensive care unit', 'GVHD': 'graft-versus-host disease', 'RD': 'Raynaud disease', 'DFV': 'Doppler flow velocimetry', 'TPR': 'temperature, pulse, and respiration', 'ung.': 'ointment', 'AICD': 'automatic implantable cardiac defibrillator', 'ESWL': 'extracorporeal shock wave lithotripsy', 'dL': 'deciliter', 'ROM': 'range of motion', 'ABO': 'three basic blood groups', 'Mg': 'magnesium', 'LVAD': 'left ventricular assist device', 'RF': 'rheumatoid factor', 'IED': 'improvised explosive device', 'SNS': 'sympathetic nervous system', 'P, p': 'melting point', 'Cu': 'copper', 't.i.d.': 'three times a day', 'LEEP': 'loop electrosurgical excision procedure', 'Mn': 'manganese', 'PDA': 'patent ductus arteriosus', 'CBC': 'complete blood count', 'HGSIL': 'high-grade squamous intraepithelial lesion', 'LTD': 'lowest tolerated dose', 'TNF-α': 'tumor necrosis factor alpha', 'EMG': 'electromyogram, electromyography', 'ICSH': 'interstitial cell-stimulating hormone', 's.q.': 'subcutaneous(ly)', 'F and E': 'fluid and electrolyte', 'EOM': 'extraocular muscles', 'SSRI': 'selective serotonin reuptake inhibitor', 'Vf': 'field of vision', 'MLF': 'medial longitudinal fasciculus', 'SNRI': 'serotonin and norepinephrine reuptake inhibitor', 'AVM': 'arteriovenous malformation', 'DVT': 'deep vein thrombosis', 'HOB': 'head of bed', 'G, g, gm': 'gram', 'OSHA': 'Occupational Safety and Health Administration', 'RUL': 'right upper lobe', 'OOB': 'out of bed', 'FA': 'fatty acid', 'WAP': 'written action plan', 'RPM': 'revolutions per minute', 'CVRB': 'critical value read back', 'A/G': 'albumin/globulin ratio', 'HAART': 'highly active antiretroviral therapy', 'Cal': 'large calorie', 'PCP': '', 'DTs': 'delirium tremens', 'c.n.': 'tomorrow night', 'MELD': 'Model for End-Stage Liver Disease', 'aa': 'of each', 'RE': 'right eye', 'DOE': 'dyspnea on exertion', 'SLP': 'speech-language pathology', 'TAT': 'thematic apperception test', 'UHF': 'ultrahigh frequency', 'S.': 'sacral', 'ME ratio': 'myeloid/erythroid ratio', 'PMH': 'past medical history', 'VA': 'visual acuity', 'SOB': 'shortness of breath', 'TA': 'toxin-antitoxin', 'IUFD': 'intrauterine fetal death', 'PCWP': 'pulmonary capillary wedge pressure', 'PKU': 'phenylketonuria', 'mm': 'millimeter', 'SCI': 'spinal cord injury', 'bol.': 'pill', 'RBC': 'red blood cell', 'CAH': 'chronic active hepatitis', 'EENT': 'eye, ear, nose, and throat', 'IDDM': 'insulin-dependent diabetes mellitus', 'TENS': 'transcutaneous electrical nerve stimulation', 'TN': 'trigeminal nerve', 'PDR': '', 'HSIL': 'high-grade squamous intraepithelial lesion', 'IC': 'inspiratory capacity', 'MAO-B': 'monoamine oxidase-B', 'H&H': 'hematocrit and hemoglobin', 'LGA': 'large for gestational age', 'BMI': 'body mass index', 'LLQ': 'left lower quadrant', 'AD': 'advance directive', 'q.s.': 'as much as needed', 'EGD': 'esophagogastroduodenoscopy', 'CF': 'cystic fibrosis', 'BROW': 'barley, rye, oats, and wheat', 'ABI': 'ankle-brachial index', 'FFP': 'fresh frozen plasma', 'A&P': 'auscultation and percussion', 'wt.': 'weight', 'URI': 'upper respiratory infection', 'GH': 'growth hormone', 'CO': 'carbon monoxide', 'TPN': 'total parenteral nutrition', 'BCP': 'birth control pills', 'cm': 'centimeter', 'ml': 'milliliter', 'PABA': 'para-aminobenzoic acid (vitamin B10)', 'DKA': 'diabetic ketoacidosis', 'HPI': 'history of present illness', 'PID': 'pelvic inflammatory disease', 'a.m.a.': 'against medical advice', 'COLD': 'chronic obstructive lung disease', 'BMR': 'basal metabolic rate', 'LH ': 'luteinizing hormone', 'RUQ': 'right upper quadrant', 'C&S': 'culture and sensitivity', 'asc.': 'ascending', 'B.P.': 'British Pharmacopeia', 'AMI': 'acute myocardial infarction', 'AM': 'morning', 'ACE': 'angiotensin-converting enzyme', 'ALP': 'alkaline phosphatase', 'LUQ': 'left upper quadrant', 'VT': 'ventricular tachycardia', 'TNM': 'tumor-node-metastasis', 'LUL': 'left upper lobe', 'Hib': '', 'BSE': 'breast self-examination', 'AChR': 'acetylcholine receptor', 'PVC': 'premature ventricular contraction', 'Cl': 'chlorine', 'liq.': 'liquid', 'CDC': 'Centers for Disease Control and Prevention', 'TORB': 'telephone order read back', 'DOA': 'dead on arrival', 'EDD': 'estimated date of delivery (formerly EDC: estimated date of confinement)', 'PVR': 'peripheral vascular resistance', 'DNA': 'deoxyribonucleic acid', 'CBI': 'continuous bladder irrigation', 'MRI': 'magnetic resonance imaging', 'Tl': 'thallium', 'A-P': 'anterior-posterior', 'pH': 'hydrogen ion concentration', 'MG': 'myasthenia gravis', 'MS': 'mitral stenosis', 'PCA': 'patient-controlled analgesia', 'DOB': 'date of birth', 'FAP': 'familial adenomatous polyposis', 'NS': 'normal saline', 'IL-8': 'interleukin 8', 'TRAP criteria': 'tremor, rigidity, akinesia or postural instablity bradykinesia, and postural instability', 'Alb': 'albumin', 'PCOS': 'polycystic ovarian syndrome', 'VLDL': 'very low density lipoprotein', 'BBT': 'basal body temperature', 'PSV': 'prostate-specific antigen', 'aq. dest.': 'distilled water', 'Rh': 'rhesus factor', 'CD4': 'T-helper cells', 'tTG': 'antitransglutaminase', 'TKR': 'total knee replacement', 'Zn': 'zinc', 'pil.': 'pill', 'noct.': 'in the night', 'dc': 'discontinue', 'IPPB': 'intermittent positive pressure breathing', 'I&O': 'intake and output', 'CC': 'chief complaint', 'top.': 'topically', 'MMSE': 'Mini-Mental Status Examination', 'SC, sc, s.c.': 'subcutaneous(ly)', 'cap.': 'capsule', 'GERD': 'gastroesophageal reflux disease', 'BW': 'birth weight', 'IUCD': 'intrauterine contraceptive device', 'Dx': 'diagnosis', 'AVP': 'arginine vasopressin', 'AF': 'atrial fibrillation', 'gr': 'grain', 'PMI': 'point of maximal impulse', 'UC': 'ulcerative colitis', 'ESRD': 'end-stage renal disease', 'PIPDA (scan)': '99mTc-para-isopropylacetanilido-iminodiaacetic acid (cholescintigraphy)', 'Se': 'selenium', 'stat.': 'immediately', 'mc': 'millicurie', 'amp': 'ampule', 'CVS': 'chorionic villi sampling', 'CVA': 'cardiovascular accident', 'omn. hor.': 'every hour', 'anti-CCP': 'anticyclic citrullinated peptide', 'NIDDM': 'noninsulin-dependent diabetes mellitus', 'NIH': 'National Institutes of Health', 'mol wt': 'molecular weight', 'SSS': 'sick sinus syndrome', 'mV': 'millivolt', 'ad lib.': 'freely', 'PO': 'orally', 'dil.': 'dilute', 'semih.': 'half an hour', 'Pb': 'lead', 'VD': 'venereal disease', 'CD8': 'cytotoxic cells', 'AST': 'aspartate aminotransferase', 'MAP': 'mean arterial pressure', 'RHD': 'rheumatic heart disease', 'NSR': 'normal sinus rhythm', 'PI': 'present illness', 'LFT': 'liver function test', 'My': 'myopia', 'LLE': 'left lower extremity', 's.o.s.': 'if necessary', 'trit.': 'triturate, grind', 'N/A': 'not applicable', 'CVC': 'central venous catheter', 'PSA': 'prostate-specific antigen', 'TMJ': 'temporomandibular joint', 'CT': 'computed/computerized tomography', 'BM': 'bowel movement', 'TNTM': 'too numerous to mention', 'RNA': 'ribonucleic acid', 'CIN': 'cervical intraepithelial neoplasia', 'VC': 'vital capacity', 'ol.': 'oil', 'STD': 'sexually transmitted disease', 'CV': 'cardiovascular', 'R/O': 'rule out', 'NAA': 'nucleic acid amplification', 'TM': 'tympanic membrane', 'OTC': 'over-the-counter', 'DTR': 'deep tendon reflex(es)', 'Re': 'rhenium', 'WAIS': 'Wechsler Adult Intelligence Scale', 'EF': 'ejection fraction', 'D5/½ /NS': '5% dextrose and half-normal saline solution (0.45% NaCl)', 'kv': 'kilovolt', 'DMARD': 'disease-modulating antirheumatic drug', 'p.r.': 'through the rectum', 'HDV': 'hepatitis D', 'NK': 'natural killer', 'bib.': 'drink', 'CP': 'cerebral palsy', 'ad': 'up to', 'TEN': 'toxic epidermal necrolysis', 'c.m.s.': 'to be taken tomorrow morning', 'QFT-G': 'QuantiFERON-TB Gold', 'RDS': 'respiratory distress syndrome', 'MED': 'minimum effective dose', 'Tb': 'terbium', 'CI': 'cardiac index', 'WN': 'well-nourished', 'per': 'through or by', 'D5W': 'dextrose 5% in water', 'Sn': 'tin', 'MLD': 'minimum lethal dose', 'MD': 'muscular dystrophy', 'KVO': 'keep vein open', 'FISH': 'fluorescence in situ hybridization', 'MA': 'mental age', 'SPECT': 'single-photon emission computed tomography', 'aPTT': 'activated partial thromboplastin', 'ADHD': 'attention deficit-hyperactivity disorder', 'HEPA': 'high-efficiency particulate air', 'Sr': 'strontium', 'GTT': 'glucose tolerance test', 'IL-1': 'interleukin 1', 'MPN': 'most probable number', 'mcg': 'microgram', 'GRAS': 'generally recognized as safe', 'FDA': '(U.S.) Food and Drug Administration', 'alt. hor.': 'every other hour', 'NDC': 'National Drug Code', 'Ci': 'curie', 'MBD': 'minimal brain dysfunction', 'ER': 'Emergency Room, extended-release', 'ALL': 'acute lymphocytic leukemia', 'mm Hg': 'millimeters of mercury', 'hx, Hx': 'history', 'admov.': 'apply', 'CPM': 'continuous passive motion', 'HDL': 'high-density lipoprotein', 'PMN': 'polymorphonuclear neutrophil leukocytes', 'DHT': 'dihydrotestosterone', 'A-V': 'arteriovenous', 'Te': 'tellurium', 'DNH': 'do not hospitalize', 'RLE': 'right lower extremity', 'µg': 'microgram', 'Gtt, gtt': 'drops', 'CS': 'cardiogenic shock', 'HELLP': 'hemolysis, elevated liver enzymes, low platelets', 'DWI': 'driving while intoxicated', 'PET': 'positron emission tomography', 'ASD': 'atrial septal defect', 'mr ': 'milliroentgen', 'HCT, Hct': 'hematocrit', 'AED': 'antiepileptic drug', 'R/T': 'related to', 'av.': 'avoirdupois', 'FSH': 'follicle-stimulating hormone', 'PFT': 'pulmonary function test', 'HTLV-III': 'human T lymphotropic virus type III', 'PMS': 'premenstrual syndrome', 'Ast': 'astigmatism', 'WNL': 'within normal limits', 'DRG': 'diagnosis-related group', 'CABG': 'coronary artery bypass graft', 'ECF': 'extended care facility', 'Rn': 'radon', 'Treg': 'regulatory T cell', 'OPD': 'outpatient department', 'EMA-IgA': 'immunoglobulin A antiendomysial', 'SNF': 'skilled nursing facility', 'q.h.': 'every hour', 'q.4h.': 'every 4 hours', 'HCP': 'health care professional', 'MMR': 'measles-mumps-rubella (vaccine)', 'rad': 'radiation absorbed dose', 'VF': 'ventricular fibrillation', 'med': 'medial', 'Au': 'gold', 'CPD': 'cephalopelvic disproportion', 'HEENT': 'head, eye, ear, nose, and throat', 'Endo': 'endocrine', 'S-A': 'sinoatrial', 'CBRNE': 'chemical, biological, radiological, nuclear, and explosive agents', 'FD': 'fatal dose', 'mor. sol.': 'as accustomed', 'ut. dict.': 'as directed', 'mg': 'milligram', 'om. mane vel noc.': 'every morning or night', 'ASCVD': 'atherosclerotic cardiovascular disease', 'KUB': 'kidney, ureter, and bladder', 'WBC': 'white blood cell', 'fl.': 'flexor', 'MCH': 'mean corpuscular hemoglobin', 'HIV': 'human immunodeficiency virus', 'Li': 'lithium', 'BMT': 'bone marrow transplantation', 'SERM': 'selective estrogen receptor modulator', 'CAP': 'let (the patient) take', 'mEq/L': 'milliequivalent per liter', 'BSA': 'body surface area', 'HR': 'heart rate', 'pro time/PT': 'prothrombin time', 'alt. noc.': 'every other night', 'syr.': 'syrup', 'sph': 'spherical', 'inj.': 'injection', 'CSF': 'cerebrospinal fluid', 'qns': 'quantity not sufficient', 'HBV': 'hepatitis B virus', 'SLE': 'systemic lupus erythematosus', 'int.': 'internal', 'S/P': 'no change after', 'ACTH': 'adrenocorticotropic hormone', 'GGT': 'gamma-glutamyl transferase', 'SJS': 'Stevens-Johnson syndrome', 'IRV': 'inspiratory reserve volume', 'CRS-R': 'Conners Rating Scales-Revised', 'LV': 'left ventricle', 'NPO': 'nothing by mouth', 'L&D': 'labor and delivery', 'Fld': 'fluid', 'jt.': 'joint', 'garg': 'gargle', 'SV': 'stroke volume', 'TUR': 'transurethral resection', 'NMJ': 'neuromuscular junction', 'supf.': 'superficial', 'CMT': 'certified medication technician', 'MRgFUS': 'MR-guided focused ultrasound surgery', 'APAP': 'acetaminophen', 'PRBCs': 'packed red blood cells', 'bipap': 'bilevel positive airway pressure', 'NICU': 'neonatal intensive care unit', 'ChE': 'cholinesterase', 'ppm': 'parts per million', 'acc.': 'accommodation', 'HCG': 'human chorionic gonadotropin', 'HPV': 'human papillomavirus', 'UE': 'upper extremity', 'BAC': 'blood alcohol concentration', 'h, hr': 'hour', 'in d.': 'daily', 'JRA': 'juvenile rheumatoid arthritis', 'Na': 'sodium', 'omn. noct.': 'every night', 'ABG': 'arterial blood gas', 'TPO': 'thyroid peroxidase', 'Ao.': 'aorta', 'D5/0.9 NaCl': '5% dextrose and normal saline solution (0.9% NaCl)', 'UTI': 'urinary tract infection', 'UA': 'urinalysis', 'TIA': 'transient ischemic attack', 'Derm': 'dermatology', 'Th': 'thorium', 'vol.': 'volume', 'NPN': 'nonprotein nitrogen', 'LDL': 'low-density lipoprotein', 'LBW': 'low birth weight', 'lab': 'laboratory', 'FP': 'family practice', 'NMDA': '', 'BP': 'blood pressure', 'DC': 'direct current', 'RAIU': 'radioactive iodine uptake', 'BLS': 'basic life support', 'AHF': 'antihemophilic factor', 'HF': 'heart failure', 'q.l.': 'as much as wanted', 'ET-1': 'endothelin-1', 'dur. dolor': 'while pain lasts', 'TSD': 'time since death', 'VSD': 'ventricular septal defect', 'CPC': 'clinicopathologic conference', 'OR': 'operating room', 'guttat.': 'drop by drop', 'MRA': 'magnetic resonance angiography', 'HLA': 'human leukocyteantigen', 'KOH': 'potassium hydroxide', 'c.n.s.': 'to be taken tomorrow night', 'WDWN': 'well-developed, well-nourished', 'ECT': 'electroconvulsive therapy', 'c/o': 'complains of', 'H1N1': 'hemagglutinin type 1 and neuraminidase type 1', 'b.': 'bone', 'HEV': 'hepatitis E', 'ELISA': 'enzyme-linked immunosorbent assay', 'COPD': 'chronic obstructive pulmonary disease', 'spt.': 'spirit', 'RR': 'recovery room', 'sup.': 'superior', 'ADL, ADLs': 'activities of daily living', 'Strep': '', 'KS': 'Kaposi sarcoma', 'T.A.T.': 'toxin-antitoxin', 'BNP': 'brain natriuretic peptide', 'CXR': 'chest x-ray', 'OB': 'obstetrics', 'CPK': 'creatine phosphokinase', 'PIH': 'pregnancy-induced hypertension', 'SAD': 'seasonal affective disorder', 'qt': 'quart', 'AFB': 'acid-fast bacillus', 'SARS': 'severe acute respiratory syndrome', 'hgb': 'hemoglobin', 'ah': 'hypermetropic astigmatism', 'T&A': 'tonsillectomy and adenoidectomy', 'GC': 'gonococcus or gonorrheal', 'LDH': 'lactate dehydrogenase', 'nCi': 'nanocurie', 'RA': 'rheumatoid arthritis', 'OmPC': 'outer membrane porin C', 'AAA': 'abdominal aortic aneurysm', 'VDRL': 'Venereal Disease Research Laboratories', 'BPH': 'benign prostatic hyperplasia', 'AChE': 'acetylcholinesterase', 'IQ': 'intelligence quotient', 'sp gr': 'specific gravity', 'POLST': 'physician orders for life-sustaining therapy', 'man. prim.': 'first thing in the morning', 'STU': 'skin test unit', 'IM': 'intramuscular', 'Staph': '', 'SPF': 'skin protection factor', 'P-A': 'placenta abruption', 'Si': 'silicon', 'COMT': 'catechol-O-methyltransferase', 'Sig.': 'write on label', 'HRT': 'hormone replacement therapy', 'PEEP': 'positive end expiratory pressure', 'PBI': 'protein-bound iodine', 'TNF': 'tumor necrosis factor', 'BCLS': 'basic cardiac life support', 'grad': 'by degrees', 'TEE': 'transesophageal echocardiogram', 'EMS': 'emergency medical service', 'EKG': 'electrocardiogram', 'PEG': 'percutaneous endoscopic gastrostomy', 'GP': 'general practitioner', 'AsH': 'hypermetropic astigmatism', 'Be': 'beryllium', 'LOC': 'level/loss of consciousness', 'pd': 'prism diopter', 'FEV': 'forced expiratory volume', 'AMLS': 'Advanced Medical Life Support', 'ENT': 'ear, nose, and throat', 'MVA': 'motor vehicle accident', 'ALS': 'amyotrophic lateral sclerosis', 'IVP': 'intravenous pyelogram', 'IDM': 'infants of diabetic mothers', 'oz': 'ounce', 'PNH': 'paroxysmal nocturnal hemoglobinuria', 'IBW': 'ideal body weight', 'IUD': 'intrauterine device', 'CSH': 'combat support hospital', 'aq. frig.': 'cold water', 'st.': 'let it/them stand', 'PD': 'interpupillary distance', 'RFT': 'renal function test', 'Am': 'mixed astigmatism', 'BHS': 'beta-hemolytic streptococci', 'CCU': 'coronary care unit', 'DJD': 'degenerative joint disease', 'p.c.': 'after meals', 'DEXA': 'dual-energy x-ray absorptiometry', 'CK-MB': 'serum creatine kinase, myocardial-bound', 'STS': 'serologic test for syphilis', 'Bx': 'biopsy', 'CA': 'coronary artery', 'Pap, Pap test': 'Papanicolaou smear', 'MW': 'molecular weight', 'BK': 'below the knee', 'PUVA': 'psoralen ultraviolet A', 'HTN': 'hypertension', 'OC': 'oral contraceptive', 'WH': 'well-hydrated', 'SIADH': 'syndrome of inappropriate diuretic hormone', 'w/v.': 'weight in volume', 'NG, ng': 'nasogastric', '/d': 'per day', 'CVP': 'central venous pressure', 'NAD': 'no acute distress', 'vol %': 'volume percent', 'DIC': 'disseminated intravascular coagulation', 'dieb. alt.': 'every other day', 'CBT': 'cognitive behavioral therapy', 'Ba': 'barium', 'tinct., tr': 'tincture', 'SI': 'international system of units', 'PCR': 'polymerase chain reaction', 'OU': 'each eye', 'CREST': 'calcinosis, Raynaud phenomenon, esophageal dysfunction, sclerodactyly, telangiectasia (cluster of features of systemic sclerosis scleroderma)', 'PIP': 'proximal interphalangeal', 'kg': 'kilogram', 'µEq': 'microequivalent', 'VMA': 'vanillylmandelic acid', 'ESR': 'erythrocyte sedimentation rate', 'Sb': 'antimony', 'Sx': 'symptoms', 'Hg': 'mercury', 'CMV': 'cytomegalovirus', 'Pharm': 'pharmacy', 'AsM': 'myopic astigmatism', 'ARMD': 'age-related macular degeneration', 'HER2': 'human EGF (epidermal growth factor) receptor 2', 'Id.': 'the same', 'mMol': 'millimole', 'DI': 'diabetes insipidus', 'G6PD': 'glucose-6-phosphate dehydrogenase', 'Ra': 'radium', 'hor. som, h.s.': 'bedtime', 'P-ANCA': 'perinuclear antineutrophil cytoplasmic antibody', 'LP': 'lumbar puncture', 'EEG': 'electroencephalogram', 'VS': 'volumetric solution', 'vv': 'veins', 'ANP': 'atrial natriuretic peptide', 'Ig': 'immunoglobulin', 'PTT': 'partial thromboplastin time', 'AFP': 'alpha-fetoprotein', 'Umb': 'umbilicus', 'abd': 'abdominal/abdomen', 'PALS': 'pediatric advanced life support', 'ORIF': 'open reduction with/and internal fixation', 'AI': 'aortic incompetence', 'MCHC': 'mean corpuscular hemoglobin concentration', 'HD': 'hearing distance', 'q.p.': 'as much as desired', 'PM': 'afternoon/evening', 'n.b.': 'note well', 'MPC': 'maximum permitted concentration', 'ADH': 'antidiuretic hormone', 'MV': 'mitral valve', 'noct. maneq.': 'night and morning', 'RQ': 'respiratory quotient', 'CFTR': 'cystic fibrosis transmembrane regulator', 'lb': 'pound', 'LSIL': 'low-grade squamous epithelial lesion', 'ED': 'emergency department', 'pt': 'pint', 'anat': 'anatomy or anatomic', 'V/Q': 'ventilation/perfusion', 'ETOH, EtOH': 'ethyl alcohol', 'DM': 'diabetes mellitus', 'Ni': 'nickel', 'ext.': 'extensor', 'q.3h.': 'every 3 hours', 'CPHSS': 'Cincinnati Prehospital Stroke Scale', 'TIBC': 'total iron-binding capacity', 'HCV': 'hepatitis C virus', 'GFR': 'glomerular filtration rate', 'HIDA': 'hepatobiliary iminodiacetic acid (cholescintigraphy)', 'CPAP': 'continuous positive airway pressure', 'ECHO': 'echocardiography', 'alt. dieb.': 'every other day', 'SIDS': 'sudden infant death syndrome', 'RUE': 'right upper extremity', 'BD': 'Buerger disease', 'GB': 'gallbladder', 'CAD': 'coronary artery disease', 'CK': 'creatine kinase', 'yr': 'year', 'GSW': 'gunshot wound', 'HSV': 'herpes simplex virus', 'Ag': 'silver', 'DRE': 'digital rectal examination', 'ACh': 'acetylcholine', 'ANNA': 'anti-neuronal nuclear antibody', 'RML': 'right middle lobe of lung', 'AGC': 'atypical glandular cells', 'SB': 'small bowel', 'GU': 'genitourinary', 'LVH': 'left ventricular hypertrophy', 'BUN': 'blood urea nitrogen', 'b.i.d., bid': 'twice a day', 'PICC': 'peripherally inserted central catheter', 'VLBW': 'very low birth weight', 'non rep': 'do not repeat', 'PERRLA': 'pupils equal, regular, react to light and accommodation', 'LR': 'lactated Ringer (solution)', 'MID': 'minimum infective dose', 'AQ, aq': 'water', 'nn': 'nerves', 'mor. dict.': 'as directed', 'DISIDA (scan)': 'diisopropyl iminodiacetic acid (cholescintigraphy)', 'instill.': 'instillation', 'RSV': 'respiratory syncytial virus', 'AS': 'ankylosing spondylitis', 'ANA': 'antinuclear antibody', 'SVC': 'superior vena cava', 'ALT': 'alanine aminotransferase', 'comp.': 'compound', 'DPT': 'diphtheria-pertussis-tetanus (vaccine)', 'CRP': 'c. reactive protein', 'TSE': 'testicular self-examination', 'SBP': 'systolic blood pressure', 'As.': 'astigmatism', 'cc': 'cubic centimeter', 'TSH': 'thyroid-stimulating hormone', 'PP': 'placenta previa', 'USAN': 'United States Adopted Name'}\n"
     ]
    }
   ],
   "source": [
    "print(med_terms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts += input_texts_MedTerms\n",
    "target_texts += target_texts_MedTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts_MedTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13579\n",
      "Claim Tye:v VB Acciden -Accidental Inujry \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "\n",
      "PuolicyholderOwner Iformation \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "\n",
      "First Name: \n",
      " \tFirst Name:\n",
      "\n",
      "\n",
      "Middle Name/Initail: \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "\n",
      "Last Name: \n",
      " \tLast Name:\n",
      "\n",
      "\n",
      "Social Securty Numqber: \n",
      " \tSocial Security Number:\n",
      "\n",
      "\n",
      "Birth Date: \n",
      " \tBirth Date:\n",
      "\n",
      "\n",
      "Genqder: \n",
      " \tGender:\n",
      "\n",
      "\n",
      "Language Preferensce: \n",
      " \tLanguage Preference:\n",
      "\n",
      "\n",
      "Addressa Line 1: \n",
      " \tAddress Line 1:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word level vocab grows large when we add input_texts in case of large noisy data augmentation. To avoid this, we can try to correct spelling first.\n",
    "When we plug char level with word level models, vocab is better built on the target_texts, which is the output. The input will be at char level anyway. This will reduce vocab.\n",
    "'''\n",
    "#all_texts = target_texts + input_texts \n",
    "all_texts = target_texts\n",
    "\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)\n",
    "np.savez('vocab', vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 13579\n",
      "Number of unique input tokens: 1563\n",
      "Number of unique output tokens: 1563\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'claim': 4,\n",
       " 'type': 5,\n",
       " 'vb': 6,\n",
       " 'accident': 7,\n",
       " 'accidental': 8,\n",
       " 'injury': 9,\n",
       " 'information': 10,\n",
       " 'first': 11,\n",
       " 'name': 12,\n",
       " 'middle': 13,\n",
       " 'last': 14,\n",
       " 'social': 15,\n",
       " 'security': 16,\n",
       " 'number': 17,\n",
       " 'birth': 18,\n",
       " 'date': 19,\n",
       " 'gender': 20,\n",
       " 'language': 21,\n",
       " 'preference': 22,\n",
       " 'address': 23,\n",
       " 'line': 24,\n",
       " 'city': 25,\n",
       " 'postal': 26,\n",
       " 'code': 27,\n",
       " 'country': 28,\n",
       " 'email': 29,\n",
       " 'page': 30,\n",
       " 'radiology': 31,\n",
       " 'report': 32,\n",
       " 'patient': 33,\n",
       " 'mrn': 34,\n",
       " 'accession': 35,\n",
       " 'ref': 36,\n",
       " 'physician': 37,\n",
       " 'unknown': 38,\n",
       " 'study': 39,\n",
       " 'hospital': 40,\n",
       " 'technique': 41,\n",
       " 'views': 42,\n",
       " 'left': 43,\n",
       " 'wrist': 44,\n",
       " 'cormarison': 45,\n",
       " 'none': 46,\n",
       " 'availabie': 47,\n",
       " 'comparison': 48,\n",
       " 'available': 49,\n",
       " 'findings': 50,\n",
       " 'impression': 51,\n",
       " 'acute': 52,\n",
       " 'osseous': 53,\n",
       " 'abnormality': 54,\n",
       " 'identified': 55,\n",
       " 'daytime': 56,\n",
       " 'phone': 57,\n",
       " 'event': 58,\n",
       " 'stopped': 59,\n",
       " 'working': 60,\n",
       " 'yes': 61,\n",
       " 'physically': 62,\n",
       " 'work': 63,\n",
       " 'hours': 64,\n",
       " 'worked': 65,\n",
       " 'day': 66,\n",
       " 'scheduled': 67,\n",
       " 'missed': 68,\n",
       " 'returned': 69,\n",
       " 'related': 70,\n",
       " 'time': 71,\n",
       " 'surgery': 72,\n",
       " 'required': 73,\n",
       " 'indicator': 74,\n",
       " 'outpatient': 75,\n",
       " 'medical': 76,\n",
       " 'provider': 77,\n",
       " 'roles': 78,\n",
       " 'treating': 79,\n",
       " 'patrick': 80,\n",
       " 'emerson': 81,\n",
       " 'business': 82,\n",
       " 'telephone': 83,\n",
       " 'fax': 84,\n",
       " 'visit': 85,\n",
       " 'next': 86,\n",
       " 'hospitalization': 87,\n",
       " 'discharge': 88,\n",
       " 'procedure': 89,\n",
       " 'arthiscopic': 90,\n",
       " 'employment': 91,\n",
       " 'employer': 92,\n",
       " 'policy': 93,\n",
       " 'electronic': 94,\n",
       " 'submission': 95,\n",
       " 'identifier': 96,\n",
       " 'electronically': 97,\n",
       " 'signed': 98,\n",
       " 'fraud': 99,\n",
       " 'statements': 100,\n",
       " 'reviewed': 101,\n",
       " 'benefits': 102,\n",
       " 'center': 103,\n",
       " 'fmla': 104,\n",
       " 'requests': 105,\n",
       " 'insured': 106,\n",
       " '’': 107,\n",
       " 'signature': 108,\n",
       " 'printed': 109,\n",
       " 'confirmation': 110,\n",
       " 'coverage': 111,\n",
       " 'group': 112,\n",
       " 'customer': 113,\n",
       " 'ee': 114,\n",
       " 'effective': 115,\n",
       " 'employee': 116,\n",
       " 'acc': 117,\n",
       " 'january': 118,\n",
       " 'wellness': 119,\n",
       " 'benefit': 120,\n",
       " 'total': 121,\n",
       " 'monthly': 122,\n",
       " 'premium': 123,\n",
       " 'montly': 124,\n",
       " 'payroll': 125,\n",
       " 'deduction': 126,\n",
       " 'account': 127,\n",
       " 'dob': 128,\n",
       " 'f': 129,\n",
       " 'exam': 130,\n",
       " 'referring': 131,\n",
       " 'phys': 132,\n",
       " 'stephen': 133,\n",
       " 'gelovich': 134,\n",
       " 'tax': 135,\n",
       " 'mri': 136,\n",
       " 'without': 137,\n",
       " 'contrast': 138,\n",
       " 'results': 139,\n",
       " 'faxed': 140,\n",
       " 'bravo': 141,\n",
       " 'dependent': 142,\n",
       " 'detail': 143,\n",
       " 'zachary': 144,\n",
       " 'jager': 145,\n",
       " 'billed': 146,\n",
       " 'amounts': 147,\n",
       " 'contract': 148,\n",
       " 'adjustment': 149,\n",
       " 'allowed': 150,\n",
       " 'amount': 151,\n",
       " 'covered': 152,\n",
       " 'reason': 153,\n",
       " 'deductible': 154,\n",
       " 'carrier': 155,\n",
       " 'paid': 156,\n",
       " 'responsibility': 157,\n",
       " 'totals': 158,\n",
       " 'appeals': 159,\n",
       " 'rights': 160,\n",
       " 'important': 161,\n",
       " 'appeal': 162,\n",
       " 'languages': 163,\n",
       " 'contact': 164,\n",
       " 'know': 165,\n",
       " 'specialty': 166,\n",
       " 'orthopedic': 167,\n",
       " 'surgeon': 168,\n",
       " 'kari': 169,\n",
       " 'lund': 170,\n",
       " 'orthopedist': 171,\n",
       " 'dan': 172,\n",
       " 'palmer': 173,\n",
       " '&': 174,\n",
       " 'may': 175,\n",
       " 'spouse': 176,\n",
       " 'child': 177,\n",
       " 'black': 178,\n",
       " 'hills': 179,\n",
       " 'pc': 180,\n",
       " 'pmt': 181,\n",
       " 'due': 182,\n",
       " 'statement': 183,\n",
       " 'ins': 184,\n",
       " 'description': 185,\n",
       " 'e': 186,\n",
       " 'm': 187,\n",
       " 'new': 188,\n",
       " 'moderat': 189,\n",
       " 'clo': 190,\n",
       " 'tx': 191,\n",
       " 'phalangealfx': 192,\n",
       " 'finger': 193,\n",
       " 'splint': 194,\n",
       " 'offic': 195,\n",
       " 'cons': 196,\n",
       " 'moderate': 197,\n",
       " 'sever': 198,\n",
       " 'rad': 199,\n",
       " 'mini': 200,\n",
       " 'applic': 201,\n",
       " 'hand': 202,\n",
       " 'lower': 203,\n",
       " 'forearm': 204,\n",
       " 'fiberglass': 205,\n",
       " 'gauntlet': 206,\n",
       " 'cast': 207,\n",
       " 'yrs': 208,\n",
       " 'pat': 209,\n",
       " 'adjust': 210,\n",
       " 'current': 211,\n",
       " 'days': 212,\n",
       " 'balance': 213,\n",
       " 'pending': 214,\n",
       " 'message': 215,\n",
       " 'make': 216,\n",
       " 'checks': 217,\n",
       " 'payable': 218,\n",
       " 'billing': 219,\n",
       " 'questions': 220,\n",
       " 'choice': 221,\n",
       " 'health': 222,\n",
       " 'administrators': 223,\n",
       " 'forwarding': 224,\n",
       " 'service': 225,\n",
       " 'requested': 226,\n",
       " 'regional': 227,\n",
       " 'inc': 228,\n",
       " 'participant': 229,\n",
       " 'id': 230,\n",
       " 'original': 231,\n",
       " 'print': 232,\n",
       " 'website': 233,\n",
       " 'individual': 234,\n",
       " 'summary': 235,\n",
       " 'plan': 236,\n",
       " 'status': 237,\n",
       " 'period': 238,\n",
       " 'pocket': 239,\n",
       " 'explanation': 240,\n",
       " 'retain': 241,\n",
       " 'purposes': 242,\n",
       " 'family': 243,\n",
       " 'network': 244,\n",
       " 'karl': 245,\n",
       " 'services': 246,\n",
       " 'modifiers': 247,\n",
       " 'tc': 248,\n",
       " 'rt': 249,\n",
       " 'qualified': 250,\n",
       " 'sign': 251,\n",
       " 'interpreters': 252,\n",
       " 'written': 253,\n",
       " 'jacquelin': 254,\n",
       " 'brainard': 255,\n",
       " 'compliance': 256,\n",
       " 'officer': 257,\n",
       " 'mail': 258,\n",
       " 'department': 259,\n",
       " 'human': 260,\n",
       " 'complaint': 261,\n",
       " 'forms': 262,\n",
       " 'dakota': 263,\n",
       " 'ez': 264,\n",
       " 'ways': 265,\n",
       " 'pay': 266,\n",
       " 'automated': 267,\n",
       " 'attendant': 268,\n",
       " 'payments': 269,\n",
       " 'please': 270,\n",
       " 'call': 271,\n",
       " 'upon': 272,\n",
       " 'receipt': 273,\n",
       " 'improved': 274,\n",
       " 'online': 275,\n",
       " 'experience': 276,\n",
       " '|': 277,\n",
       " 'update': 278,\n",
       " 'info': 279,\n",
       " 'see': 280,\n",
       " 'details': 281,\n",
       " 'back': 282,\n",
       " 'show': 283,\n",
       " 'proc': 284,\n",
       " 'units': 285,\n",
       " 'charges': 286,\n",
       " 'insur': 287,\n",
       " 'thorac': 288,\n",
       " 'spine': 289,\n",
       " 'commercial': 290,\n",
       " 'non': 291,\n",
       " 'ct': 292,\n",
       " 'abd': 293,\n",
       " 'pelv': 294,\n",
       " 'payment': 295,\n",
       " 'chest': 296,\n",
       " 'charge': 297,\n",
       " 'harges': 298,\n",
       " 'digit': 299,\n",
       " 'today': 300,\n",
       " \"'s\": 301,\n",
       " 'ethnicity': 302,\n",
       " 'hispanic': 303,\n",
       " 'latino': 304,\n",
       " 'preferred': 305,\n",
       " 'english': 306,\n",
       " 'suzanne': 307,\n",
       " 'newsom': 308,\n",
       " 'cnp': 309,\n",
       " '•': 310,\n",
       " 'lethargy': 311,\n",
       " 'cough': 312,\n",
       " 'vitals': 313,\n",
       " 'lbs': 314,\n",
       " 'kg': 315,\n",
       " 'wt': 316,\n",
       " 'temp': 317,\n",
       " 'hr': 318,\n",
       " 'oxygen': 319,\n",
       " 'sat': 320,\n",
       " 'allergies': 321,\n",
       " 'amoxicillin': 322,\n",
       " 'rash': 323,\n",
       " 'possible': 324,\n",
       " 'hives': 325,\n",
       " 'active': 326,\n",
       " 'diagnoses': 327,\n",
       " 'include': 328,\n",
       " 'frontal': 329,\n",
       " 'sinusitis': 330,\n",
       " 'unspecified': 331,\n",
       " 'dizziness': 332,\n",
       " 'giddiness': 333,\n",
       " 'medication': 334,\n",
       " 'list': 335,\n",
       " 'medications': 336,\n",
       " 'taking': 337,\n",
       " 'zyrtec': 338,\n",
       " 'childrens': 339,\n",
       " 'allergy': 340,\n",
       " 'notes': 341,\n",
       " 'tests': 342,\n",
       " 'illumigene': 343,\n",
       " 'myco': 344,\n",
       " 'http': 345,\n",
       " 'basic': 346,\n",
       " 'metabolic': 347,\n",
       " 'sodium': 348,\n",
       " 'range': 349,\n",
       " 'potassium': 350,\n",
       " 'chloride': 351,\n",
       " 'glucose': 352,\n",
       " 'bun': 353,\n",
       " 'creatinine': 354,\n",
       " 'calcium': 355,\n",
       " 'crea': 356,\n",
       " 'ratio': 357,\n",
       " 'anion': 358,\n",
       " 'gap': 359,\n",
       " 'calc': 360,\n",
       " 'cbc': 361,\n",
       " 'diff': 362,\n",
       " 'wbc': 363,\n",
       " 'rbc': 364,\n",
       " 'hgb': 365,\n",
       " 'hct': 366,\n",
       " 'mcv': 367,\n",
       " 'fl': 368,\n",
       " 'mch': 369,\n",
       " 'pg': 370,\n",
       " 'mchc': 371,\n",
       " 'mpv': 372,\n",
       " 'platelets': 373,\n",
       " 'neutrophils': 374,\n",
       " 'lymphocytes': 375,\n",
       " 'monocytes': 376,\n",
       " 'conditions': 377,\n",
       " 'problem': 378,\n",
       " 'idiopathic': 379,\n",
       " 'urticaria': 380,\n",
       " 'document': 381,\n",
       " 'wish': 382,\n",
       " 'keep': 383,\n",
       " 'policyholder': 384,\n",
       " 'owner': 385,\n",
       " 'eastside': 386,\n",
       " 'acct': 387,\n",
       " 'jasminder': 388,\n",
       " 'singh': 389,\n",
       " 'dev': 390,\n",
       " 'pa': 391,\n",
       " 'excuse': 392,\n",
       " 'east': 393,\n",
       " 'side': 394,\n",
       " 'unum': 395,\n",
       " 'april': 396,\n",
       " 'weekly': 397,\n",
       " 'ph': 398,\n",
       " 'primary': 399,\n",
       " 'thoracic': 400,\n",
       " 'strain': 401,\n",
       " 'strained': 402,\n",
       " 'following': 403,\n",
       " 'occurs': 404,\n",
       " 'feel': 405,\n",
       " 'weakness': 406,\n",
       " 'arms': 407,\n",
       " 'legs': 408,\n",
       " 'severe': 409,\n",
       " 'increase': 410,\n",
       " 'pain': 411,\n",
       " 'lumbosacral': 412,\n",
       " 'weak': 413,\n",
       " 'becomes': 414,\n",
       " 'follow': 415,\n",
       " 'take': 416,\n",
       " 'directed': 417,\n",
       " 'additional': 418,\n",
       " 'prescriptions': 419,\n",
       " 'prescriber': 420,\n",
       " 'paper': 421,\n",
       " 'prescription': 422,\n",
       " 'given': 423,\n",
       " 'preventative': 424,\n",
       " 'instructions': 425,\n",
       " 'diagnosis': 426,\n",
       " 'knee': 427,\n",
       " 'david': 428,\n",
       " 'bruce': 429,\n",
       " 'identiﬁer': 430,\n",
       " 'june': 431,\n",
       " 'concussion': 432,\n",
       " 'devin': 433,\n",
       " 'conrad': 434,\n",
       " 'september': 435,\n",
       " 'form': 436,\n",
       " 'attending': 437,\n",
       " 'part': 438,\n",
       " 'completed': 439,\n",
       " 'icd': 440,\n",
       " 'unable': 441,\n",
       " 'expected': 442,\n",
       " 'delivery': 443,\n",
       " 'actual': 444,\n",
       " 'vaginal': 445,\n",
       " 'per': 446,\n",
       " 'continued': 447,\n",
       " 'facility': 448,\n",
       " 'state': 449,\n",
       " 'zip': 450,\n",
       " 'performed': 451,\n",
       " 'surgical': 452,\n",
       " 'cpt': 453,\n",
       " 'degree': 454,\n",
       " 'check': 455,\n",
       " 'filing': 456,\n",
       " 'b': 457,\n",
       " 'suffix': 458,\n",
       " 'mi': 459,\n",
       " 'spanish': 460,\n",
       " 'short': 461,\n",
       " 'term': 462,\n",
       " 'disability': 463,\n",
       " 'long': 464,\n",
       " 'life': 465,\n",
       " 'insurance': 466,\n",
       " 'voluntary': 467,\n",
       " 'motor': 468,\n",
       " 'vehicle': 469,\n",
       " 'physicians': 470,\n",
       " 'hospitals': 471,\n",
       " 'considerations': 472,\n",
       " 'male': 473,\n",
       " 'female': 474,\n",
       " 'member': 475,\n",
       " 'relationship': 476,\n",
       " 'person': 477,\n",
       " 'marital': 478,\n",
       " 'single': 479,\n",
       " 'occ': 480,\n",
       " 'title': 481,\n",
       " 'resinmixer': 482,\n",
       " 'hire': 483,\n",
       " 'termination': 484,\n",
       " 'atw': 485,\n",
       " 'limitations': 486,\n",
       " 'permitted': 487,\n",
       " 'months': 488,\n",
       " 'office': 489,\n",
       " 'crane': 490,\n",
       " 'composites': 491,\n",
       " 'florence': 492,\n",
       " 'earn': 493,\n",
       " 'change': 494,\n",
       " 'leave': 495,\n",
       " 'absence': 496,\n",
       " 'record': 497,\n",
       " 'loaded': 498,\n",
       " 'residence': 499,\n",
       " 'physical': 500,\n",
       " 'access': 501,\n",
       " 'home': 502,\n",
       " 'supervisor': 503,\n",
       " 'coverages': 504,\n",
       " 'product': 505,\n",
       " 'flex': 506,\n",
       " 'funding': 507,\n",
       " 'fully': 508,\n",
       " 'division': 509,\n",
       " 'eff': 510,\n",
       " 'earnings': 511,\n",
       " 'hourly': 512,\n",
       " 'mode': 513,\n",
       " 'aso': 514,\n",
       " 'self': 515,\n",
       " 'probable': 516,\n",
       " 'duration': 517,\n",
       " 'condition': 518,\n",
       " 'dates': 519,\n",
       " 'admission': 520,\n",
       " 'treatment': 521,\n",
       " 'seen': 522,\n",
       " 'past': 523,\n",
       " 'anticipated': 524,\n",
       " 'nature': 525,\n",
       " 'estimated': 526,\n",
       " 'treatments': 527,\n",
       " 'job': 528,\n",
       " 'attached': 529,\n",
       " 'checked': 530,\n",
       " 'episodic': 531,\n",
       " 'flare': 532,\n",
       " 'ups': 533,\n",
       " 'practice': 534,\n",
       " 'care': 535,\n",
       " 'including': 536,\n",
       " 'confinement': 537,\n",
       " 'provide': 538,\n",
       " 'advice': 539,\n",
       " 'stop': 540,\n",
       " 'mgmt': 541,\n",
       " 'svc': 542,\n",
       " 'applicable': 543,\n",
       " 'deductions': 544,\n",
       " 'schedule': 545,\n",
       " 'week': 546,\n",
       " 'sick': 547,\n",
       " 'variable': 548,\n",
       " 'sunday': 549,\n",
       " 'monday': 550,\n",
       " 'tuesday': 551,\n",
       " 'wednesday': 552,\n",
       " 'thursday': 553,\n",
       " 'friday': 554,\n",
       " 'saturday': 555,\n",
       " 'hospitalized': 556,\n",
       " 'explain': 557,\n",
       " 'height': 558,\n",
       " 'weight': 559,\n",
       " 'secondary': 560,\n",
       " 'functional': 561,\n",
       " 'capacity': 562,\n",
       " 'restrictions': 563,\n",
       " 'elizabeth': 564,\n",
       " 'edgewood': 565,\n",
       " 'facesheet': 566,\n",
       " 'sex': 567,\n",
       " 'adm': 568,\n",
       " 'demographics': 569,\n",
       " 'ssn': 570,\n",
       " 'reg': 571,\n",
       " 'verified': 572,\n",
       " 'renew': 573,\n",
       " 'admitting': 574,\n",
       " 'larkin': 575,\n",
       " 'john': 576,\n",
       " 'md': 577,\n",
       " 'elective': 578,\n",
       " 'incomplete': 579,\n",
       " 'area': 580,\n",
       " 'edg': 581,\n",
       " 'sc': 582,\n",
       " 'crestview': 583,\n",
       " 'discharged': 584,\n",
       " 'confirmed': 585,\n",
       " 'hose': 586,\n",
       " 'ital': 587,\n",
       " 'class': 588,\n",
       " 'guarantor': 589,\n",
       " 'relation': 590,\n",
       " 'pt': 591,\n",
       " 'seh': 592,\n",
       " 'precert': 593,\n",
       " 'subscriber': 594,\n",
       " 'operative': 595,\n",
       " 'brief': 596,\n",
       " 'op': 597,\n",
       " 'note': 598,\n",
       " 'author': 599,\n",
       " 'j': 600,\n",
       " 'filed': 601,\n",
       " 'editor': 602,\n",
       " 'healthcare': 603,\n",
       " 'body': 604,\n",
       " 'mass': 605,\n",
       " 'index': 606,\n",
       " 'role': 607,\n",
       " 'anesthesia': 608,\n",
       " 'general': 609,\n",
       " 'specimens': 610,\n",
       " 'log': 611,\n",
       " 'blood': 612,\n",
       " 'loss': 613,\n",
       " 'course': 614,\n",
       " 'pacu': 615,\n",
       " 'operation': 616,\n",
       " 'best': 617,\n",
       " 'reached': 618,\n",
       " 'broken': 619,\n",
       " 'big': 620,\n",
       " 'toe': 621,\n",
       " 'foot': 622,\n",
       " 'todd': 623,\n",
       " 'francis': 624,\n",
       " 'podiatrist': 625,\n",
       " 'ryan': 626,\n",
       " 'kish': 627,\n",
       " 'toledo': 628,\n",
       " 'er': 629,\n",
       " 'xray': 630,\n",
       " 'july': 631,\n",
       " 'paramount': 632,\n",
       " 'promedica': 633,\n",
       " 'authorization': 634,\n",
       " 'modifier': 635,\n",
       " 'indicates': 636,\n",
       " 'c': 637,\n",
       " 'dpm': 638,\n",
       " 'ems': 639,\n",
       " 'christine': 640,\n",
       " 'nolen': 641,\n",
       " 'waukesha': 642,\n",
       " 'memorial': 643,\n",
       " 'cleaning': 644,\n",
       " 'bandage': 645,\n",
       " 'montano': 646,\n",
       " 'ci': 647,\n",
       " 'web': 648,\n",
       " 'user': 649,\n",
       " 'prohealth': 650,\n",
       " 'umr': 651,\n",
       " 'adjustments': 652,\n",
       " 'patients': 653,\n",
       " 'invoice': 654,\n",
       " 'previous': 655,\n",
       " 'return': 656,\n",
       " 'portion': 657,\n",
       " 'mastercard': 658,\n",
       " 'discover': 659,\n",
       " 'american': 660,\n",
       " 'express': 661,\n",
       " 'card': 662,\n",
       " 'enclosed': 663,\n",
       " 'emergency': 664,\n",
       " 'associates': 665,\n",
       " 'using': 666,\n",
       " 'addressee': 667,\n",
       " 'industrial': 668,\n",
       " 'loop': 669,\n",
       " 'dr': 670,\n",
       " 'dept': 671,\n",
       " 'ppo': 672,\n",
       " 'adj': 673,\n",
       " 'applied': 674,\n",
       " 'rendered': 675,\n",
       " 'fiserv': 676,\n",
       " 'wi': 677,\n",
       " 'stmt': 678,\n",
       " 'doctor': 679,\n",
       " 'legend': 680,\n",
       " 'd': 681,\n",
       " 'comments': 682,\n",
       " 'souha': 683,\n",
       " 'hakim': 684,\n",
       " 'medexpress': 685,\n",
       " 'codes': 686,\n",
       " 'urgent': 687,\n",
       " 'clairn': 688,\n",
       " 'vijay': 689,\n",
       " 'patel': 690,\n",
       " 'holder': 691,\n",
       " 'qty': 692,\n",
       " 'clinical': 693,\n",
       " 'chief': 694,\n",
       " 'penicillins': 695,\n",
       " 'taken': 696,\n",
       " 'bp': 697,\n",
       " 'mmhg': 698,\n",
       " 'pulse': 699,\n",
       " 'bpm': 700,\n",
       " 'resp': 701,\n",
       " 'lb': 702,\n",
       " 'ft': 703,\n",
       " 'bmi': 704,\n",
       " 'meds': 705,\n",
       " 'albuterol': 706,\n",
       " 'sulfate': 707,\n",
       " 'encounter': 708,\n",
       " 'progress': 709,\n",
       " 'subjective': 710,\n",
       " 'history': 711,\n",
       " 'provided': 712,\n",
       " 'dad': 713,\n",
       " 'interpreter': 714,\n",
       " 'used': 715,\n",
       " 'presents': 716,\n",
       " 'review': 717,\n",
       " 'systems': 718,\n",
       " 'cardiovascular': 719,\n",
       " 'negative': 720,\n",
       " 'skin': 721,\n",
       " 'neurological': 722,\n",
       " 'headaches': 723,\n",
       " 'objective': 724,\n",
       " 'hent': 725,\n",
       " 'right': 726,\n",
       " 'ear': 727,\n",
       " 'tympanic': 728,\n",
       " 'membrane': 729,\n",
       " 'normal': 730,\n",
       " 'nose': 731,\n",
       " 'oropharynx': 732,\n",
       " 'clear': 733,\n",
       " 'eyes': 734,\n",
       " 'conjunctivae': 735,\n",
       " 'eom': 736,\n",
       " 'neck': 737,\n",
       " 'supple': 738,\n",
       " 'rigidity': 739,\n",
       " 'murmur': 740,\n",
       " 'heard': 741,\n",
       " 'lymphadenopathy': 742,\n",
       " 'cervical': 743,\n",
       " 'adenopathy': 744,\n",
       " 'alert': 745,\n",
       " 'warm': 746,\n",
       " 'noted': 747,\n",
       " 'assessment': 748,\n",
       " 'advise': 749,\n",
       " 'ss': 750,\n",
       " 'penobscot': 751,\n",
       " 'community': 752,\n",
       " 'suite': 753,\n",
       " 'medicine': 754,\n",
       " 'mental': 755,\n",
       " 'transmission': 756,\n",
       " 'sheet': 757,\n",
       " 'ext': 758,\n",
       " 'pages': 759,\n",
       " 'cover': 760,\n",
       " 'thank': 761,\n",
       " 'revised': 762,\n",
       " 'imaging': 763,\n",
       " 'mainecare': 764,\n",
       " 'fqhc': 765,\n",
       " 'low': 766,\n",
       " 'jt': 767,\n",
       " 'erin': 768,\n",
       " 'barker': 769,\n",
       " 'joseph': 770,\n",
       " 'ordering': 771,\n",
       " 'intercondylar': 772,\n",
       " 'space': 773,\n",
       " 'acl': 774,\n",
       " 'pcl': 775,\n",
       " 'intact': 776,\n",
       " 'unremarkable': 777,\n",
       " 'marrow': 778,\n",
       " 'signal': 779,\n",
       " 'dictation': 780,\n",
       " 'location': 781,\n",
       " 'mpsynernet': 782,\n",
       " 'reading': 783,\n",
       " 'kasper': 784,\n",
       " 'jared': 785,\n",
       " 'orthopedics': 786,\n",
       " 'sports': 787,\n",
       " 'np': 788,\n",
       " 'thompson': 789,\n",
       " 'mcguire': 790,\n",
       " 'initial': 791,\n",
       " 'evaluation': 792,\n",
       " 'responsible': 793,\n",
       " 'samara': 794,\n",
       " 'shiromani': 795,\n",
       " 'cc': 796,\n",
       " 'present': 797,\n",
       " 'illness': 798,\n",
       " 'persistent': 799,\n",
       " 'patella': 800,\n",
       " 'alta': 801,\n",
       " 'tendonitis': 802,\n",
       " 'asthma': 803,\n",
       " 'fractures': 804,\n",
       " 'changes': 805,\n",
       " 'father': 806,\n",
       " 'htn': 807,\n",
       " 'sister': 808,\n",
       " 'factor': 809,\n",
       " 'v': 810,\n",
       " 'seizures': 811,\n",
       " 'hs': 812,\n",
       " 'risk': 813,\n",
       " 'factors': 814,\n",
       " 'tobacco': 815,\n",
       " 'use': 816,\n",
       " 'never': 817,\n",
       " 'smoker': 818,\n",
       " 'passive': 819,\n",
       " 'smoke': 820,\n",
       " 'exposure': 821,\n",
       " 'alcohol': 822,\n",
       " 'drug': 823,\n",
       " 'caffeine': 824,\n",
       " 'drinks': 825,\n",
       " 'cellular': 826,\n",
       " 'medsupport': 827,\n",
       " 'exercise': 828,\n",
       " 'times': 829,\n",
       " 'field': 830,\n",
       " 'hockey': 831,\n",
       " 'cardio': 832,\n",
       " 'seatbelt': 833,\n",
       " 'sun': 834,\n",
       " 'occasionally': 835,\n",
       " 'fall': 836,\n",
       " 'problems': 837,\n",
       " 'dx': 838,\n",
       " 'critical': 839,\n",
       " 'chemicals': 840,\n",
       " 'zithromax': 841,\n",
       " 'azithromycin': 842,\n",
       " 'adhesive': 843,\n",
       " 'tape': 844,\n",
       " 'exertion': 845,\n",
       " 'complains': 846,\n",
       " 'denies': 847,\n",
       " 'varicose': 848,\n",
       " 'veins': 849,\n",
       " 'bone': 850,\n",
       " 'deformity': 851,\n",
       " 'neuro': 852,\n",
       " 'psych': 853,\n",
       " 'post': 854,\n",
       " 'traumatic': 855,\n",
       " 'stress': 856,\n",
       " 'disorder': 857,\n",
       " 'endo': 858,\n",
       " 'vital': 859,\n",
       " 'signs': 860,\n",
       " 'profile': 861,\n",
       " 'inches': 862,\n",
       " 'pounds': 863,\n",
       " 'rate': 864,\n",
       " 'minute': 865,\n",
       " 'sitting': 866,\n",
       " 'arm': 867,\n",
       " 'r': 868,\n",
       " 'intensity': 869,\n",
       " 'aching': 870,\n",
       " 'sharp': 871,\n",
       " 'lungs': 872,\n",
       " 'bilaterally': 873,\n",
       " 'p': 874,\n",
       " 'heart': 875,\n",
       " 'recommendations': 876,\n",
       " 'james': 877,\n",
       " 'greene': 878,\n",
       " 'orders': 879,\n",
       " 'assistant': 880,\n",
       " 'room': 881,\n",
       " 'diagnostic': 882,\n",
       " 'arthroscopy': 883,\n",
       " 'danielle': 884,\n",
       " 'st': 885,\n",
       " 'onge': 886,\n",
       " 'femoral': 887,\n",
       " 'block': 888,\n",
       " 'pac': 889,\n",
       " 'chondromalacia': 890,\n",
       " 'burning': 891,\n",
       " 'entered': 892,\n",
       " 'amy': 893,\n",
       " 'cyr': 894,\n",
       " 'progressing': 895,\n",
       " 'well': 896,\n",
       " 'postoperatively': 897,\n",
       " 'mariah': 898,\n",
       " 'larsen': 899,\n",
       " 'ma': 900,\n",
       " 'screening': 901,\n",
       " 'constitutional': 902,\n",
       " 'developed': 903,\n",
       " 'dtrs': 904,\n",
       " 'omega': 905,\n",
       " 'mg': 906,\n",
       " 'capsule': 907,\n",
       " 'one': 908,\n",
       " 'n': 909,\n",
       " 'counseling': 910,\n",
       " 'educational': 911,\n",
       " 'finley': 912,\n",
       " 'kevin': 913,\n",
       " 'bal': 914,\n",
       " 'item': 915,\n",
       " 'immunization': 916,\n",
       " 'admin': 917,\n",
       " 'ovr': 918,\n",
       " 'reach': 919,\n",
       " 'centers': 920,\n",
       " 'bethel': 921,\n",
       " 'year': 922,\n",
       " 'old': 923,\n",
       " 'started': 924,\n",
       " 'ago': 925,\n",
       " 'slipping': 926,\n",
       " 'ice': 927,\n",
       " 'tools': 928,\n",
       " 'screenings': 929,\n",
       " 'instrument': 930,\n",
       " 'score': 931,\n",
       " 'mdd': 932,\n",
       " 'classification': 933,\n",
       " 'questionnaire': 934,\n",
       " 'testing': 935,\n",
       " 'coronary': 936,\n",
       " 'disease': 937,\n",
       " 'order': 938,\n",
       " 'interpretation': 939,\n",
       " 'result': 940,\n",
       " 'region': 941,\n",
       " 'updated': 942,\n",
       " 'retired': 943,\n",
       " 'river': 944,\n",
       " 'sales': 945,\n",
       " 'manager': 946,\n",
       " 'support': 947,\n",
       " 'currently': 948,\n",
       " 'married': 949,\n",
       " 'smoking': 950,\n",
       " 'usage': 951,\n",
       " 'years': 952,\n",
       " 'pack': 953,\n",
       " 'prior': 954,\n",
       " 'sig': 955,\n",
       " 'desc': 956,\n",
       " 'start': 957,\n",
       " 'refilled': 958,\n",
       " 'elsewhere': 959,\n",
       " 'reconciliation': 960,\n",
       " 'reconciled': 961,\n",
       " 'ingredient': 962,\n",
       " 'reaction': 963,\n",
       " 'comment': 964,\n",
       " 'known': 965,\n",
       " 'system': 966,\n",
       " 'respiratory': 967,\n",
       " 'dyspnea': 968,\n",
       " 'integumentary': 969,\n",
       " 'ms': 970,\n",
       " 'positive': 971,\n",
       " 'standing': 972,\n",
       " 'dressed': 973,\n",
       " 'shoes': 974,\n",
       " 'pressure': 975,\n",
       " 'oral': 976,\n",
       " 'regular': 977,\n",
       " 'somatic': 978,\n",
       " 'rib': 979,\n",
       " 'dysfunction': 980,\n",
       " 'michael': 981,\n",
       " 'gould': 982,\n",
       " 'county': 983,\n",
       " 'potomac': 984,\n",
       " 'valley': 985,\n",
       " '”': 986,\n",
       " 'sate': 987,\n",
       " 'wo': 988,\n",
       " 'cm': 989,\n",
       " 'x': 990,\n",
       " 'baker': 991,\n",
       " 'cyst': 992,\n",
       " 'approved': 993,\n",
       " 'croft': 994,\n",
       " 'stone': 995,\n",
       " 'narrative': 996,\n",
       " 'indication': 997,\n",
       " 'posterior': 998,\n",
       " 'lateral': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int # Some special chars need to be removed TODO: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'claim',\n",
       " 5: 'type',\n",
       " 6: 'vb',\n",
       " 7: 'accident',\n",
       " 8: 'accidental',\n",
       " 9: 'injury',\n",
       " 10: 'information',\n",
       " 11: 'first',\n",
       " 12: 'name',\n",
       " 13: 'middle',\n",
       " 14: 'last',\n",
       " 15: 'social',\n",
       " 16: 'security',\n",
       " 17: 'number',\n",
       " 18: 'birth',\n",
       " 19: 'date',\n",
       " 20: 'gender',\n",
       " 21: 'language',\n",
       " 22: 'preference',\n",
       " 23: 'address',\n",
       " 24: 'line',\n",
       " 25: 'city',\n",
       " 26: 'postal',\n",
       " 27: 'code',\n",
       " 28: 'country',\n",
       " 29: 'email',\n",
       " 30: 'page',\n",
       " 31: 'radiology',\n",
       " 32: 'report',\n",
       " 33: 'patient',\n",
       " 34: 'mrn',\n",
       " 35: 'accession',\n",
       " 36: 'ref',\n",
       " 37: 'physician',\n",
       " 38: 'unknown',\n",
       " 39: 'study',\n",
       " 40: 'hospital',\n",
       " 41: 'technique',\n",
       " 42: 'views',\n",
       " 43: 'left',\n",
       " 44: 'wrist',\n",
       " 45: 'cormarison',\n",
       " 46: 'none',\n",
       " 47: 'availabie',\n",
       " 48: 'comparison',\n",
       " 49: 'available',\n",
       " 50: 'findings',\n",
       " 51: 'impression',\n",
       " 52: 'acute',\n",
       " 53: 'osseous',\n",
       " 54: 'abnormality',\n",
       " 55: 'identified',\n",
       " 56: 'daytime',\n",
       " 57: 'phone',\n",
       " 58: 'event',\n",
       " 59: 'stopped',\n",
       " 60: 'working',\n",
       " 61: 'yes',\n",
       " 62: 'physically',\n",
       " 63: 'work',\n",
       " 64: 'hours',\n",
       " 65: 'worked',\n",
       " 66: 'day',\n",
       " 67: 'scheduled',\n",
       " 68: 'missed',\n",
       " 69: 'returned',\n",
       " 70: 'related',\n",
       " 71: 'time',\n",
       " 72: 'surgery',\n",
       " 73: 'required',\n",
       " 74: 'indicator',\n",
       " 75: 'outpatient',\n",
       " 76: 'medical',\n",
       " 77: 'provider',\n",
       " 78: 'roles',\n",
       " 79: 'treating',\n",
       " 80: 'patrick',\n",
       " 81: 'emerson',\n",
       " 82: 'business',\n",
       " 83: 'telephone',\n",
       " 84: 'fax',\n",
       " 85: 'visit',\n",
       " 86: 'next',\n",
       " 87: 'hospitalization',\n",
       " 88: 'discharge',\n",
       " 89: 'procedure',\n",
       " 90: 'arthiscopic',\n",
       " 91: 'employment',\n",
       " 92: 'employer',\n",
       " 93: 'policy',\n",
       " 94: 'electronic',\n",
       " 95: 'submission',\n",
       " 96: 'identifier',\n",
       " 97: 'electronically',\n",
       " 98: 'signed',\n",
       " 99: 'fraud',\n",
       " 100: 'statements',\n",
       " 101: 'reviewed',\n",
       " 102: 'benefits',\n",
       " 103: 'center',\n",
       " 104: 'fmla',\n",
       " 105: 'requests',\n",
       " 106: 'insured',\n",
       " 107: '’',\n",
       " 108: 'signature',\n",
       " 109: 'printed',\n",
       " 110: 'confirmation',\n",
       " 111: 'coverage',\n",
       " 112: 'group',\n",
       " 113: 'customer',\n",
       " 114: 'ee',\n",
       " 115: 'effective',\n",
       " 116: 'employee',\n",
       " 117: 'acc',\n",
       " 118: 'january',\n",
       " 119: 'wellness',\n",
       " 120: 'benefit',\n",
       " 121: 'total',\n",
       " 122: 'monthly',\n",
       " 123: 'premium',\n",
       " 124: 'montly',\n",
       " 125: 'payroll',\n",
       " 126: 'deduction',\n",
       " 127: 'account',\n",
       " 128: 'dob',\n",
       " 129: 'f',\n",
       " 130: 'exam',\n",
       " 131: 'referring',\n",
       " 132: 'phys',\n",
       " 133: 'stephen',\n",
       " 134: 'gelovich',\n",
       " 135: 'tax',\n",
       " 136: 'mri',\n",
       " 137: 'without',\n",
       " 138: 'contrast',\n",
       " 139: 'results',\n",
       " 140: 'faxed',\n",
       " 141: 'bravo',\n",
       " 142: 'dependent',\n",
       " 143: 'detail',\n",
       " 144: 'zachary',\n",
       " 145: 'jager',\n",
       " 146: 'billed',\n",
       " 147: 'amounts',\n",
       " 148: 'contract',\n",
       " 149: 'adjustment',\n",
       " 150: 'allowed',\n",
       " 151: 'amount',\n",
       " 152: 'covered',\n",
       " 153: 'reason',\n",
       " 154: 'deductible',\n",
       " 155: 'carrier',\n",
       " 156: 'paid',\n",
       " 157: 'responsibility',\n",
       " 158: 'totals',\n",
       " 159: 'appeals',\n",
       " 160: 'rights',\n",
       " 161: 'important',\n",
       " 162: 'appeal',\n",
       " 163: 'languages',\n",
       " 164: 'contact',\n",
       " 165: 'know',\n",
       " 166: 'specialty',\n",
       " 167: 'orthopedic',\n",
       " 168: 'surgeon',\n",
       " 169: 'kari',\n",
       " 170: 'lund',\n",
       " 171: 'orthopedist',\n",
       " 172: 'dan',\n",
       " 173: 'palmer',\n",
       " 174: '&',\n",
       " 175: 'may',\n",
       " 176: 'spouse',\n",
       " 177: 'child',\n",
       " 178: 'black',\n",
       " 179: 'hills',\n",
       " 180: 'pc',\n",
       " 181: 'pmt',\n",
       " 182: 'due',\n",
       " 183: 'statement',\n",
       " 184: 'ins',\n",
       " 185: 'description',\n",
       " 186: 'e',\n",
       " 187: 'm',\n",
       " 188: 'new',\n",
       " 189: 'moderat',\n",
       " 190: 'clo',\n",
       " 191: 'tx',\n",
       " 192: 'phalangealfx',\n",
       " 193: 'finger',\n",
       " 194: 'splint',\n",
       " 195: 'offic',\n",
       " 196: 'cons',\n",
       " 197: 'moderate',\n",
       " 198: 'sever',\n",
       " 199: 'rad',\n",
       " 200: 'mini',\n",
       " 201: 'applic',\n",
       " 202: 'hand',\n",
       " 203: 'lower',\n",
       " 204: 'forearm',\n",
       " 205: 'fiberglass',\n",
       " 206: 'gauntlet',\n",
       " 207: 'cast',\n",
       " 208: 'yrs',\n",
       " 209: 'pat',\n",
       " 210: 'adjust',\n",
       " 211: 'current',\n",
       " 212: 'days',\n",
       " 213: 'balance',\n",
       " 214: 'pending',\n",
       " 215: 'message',\n",
       " 216: 'make',\n",
       " 217: 'checks',\n",
       " 218: 'payable',\n",
       " 219: 'billing',\n",
       " 220: 'questions',\n",
       " 221: 'choice',\n",
       " 222: 'health',\n",
       " 223: 'administrators',\n",
       " 224: 'forwarding',\n",
       " 225: 'service',\n",
       " 226: 'requested',\n",
       " 227: 'regional',\n",
       " 228: 'inc',\n",
       " 229: 'participant',\n",
       " 230: 'id',\n",
       " 231: 'original',\n",
       " 232: 'print',\n",
       " 233: 'website',\n",
       " 234: 'individual',\n",
       " 235: 'summary',\n",
       " 236: 'plan',\n",
       " 237: 'status',\n",
       " 238: 'period',\n",
       " 239: 'pocket',\n",
       " 240: 'explanation',\n",
       " 241: 'retain',\n",
       " 242: 'purposes',\n",
       " 243: 'family',\n",
       " 244: 'network',\n",
       " 245: 'karl',\n",
       " 246: 'services',\n",
       " 247: 'modifiers',\n",
       " 248: 'tc',\n",
       " 249: 'rt',\n",
       " 250: 'qualified',\n",
       " 251: 'sign',\n",
       " 252: 'interpreters',\n",
       " 253: 'written',\n",
       " 254: 'jacquelin',\n",
       " 255: 'brainard',\n",
       " 256: 'compliance',\n",
       " 257: 'officer',\n",
       " 258: 'mail',\n",
       " 259: 'department',\n",
       " 260: 'human',\n",
       " 261: 'complaint',\n",
       " 262: 'forms',\n",
       " 263: 'dakota',\n",
       " 264: 'ez',\n",
       " 265: 'ways',\n",
       " 266: 'pay',\n",
       " 267: 'automated',\n",
       " 268: 'attendant',\n",
       " 269: 'payments',\n",
       " 270: 'please',\n",
       " 271: 'call',\n",
       " 272: 'upon',\n",
       " 273: 'receipt',\n",
       " 274: 'improved',\n",
       " 275: 'online',\n",
       " 276: 'experience',\n",
       " 277: '|',\n",
       " 278: 'update',\n",
       " 279: 'info',\n",
       " 280: 'see',\n",
       " 281: 'details',\n",
       " 282: 'back',\n",
       " 283: 'show',\n",
       " 284: 'proc',\n",
       " 285: 'units',\n",
       " 286: 'charges',\n",
       " 287: 'insur',\n",
       " 288: 'thorac',\n",
       " 289: 'spine',\n",
       " 290: 'commercial',\n",
       " 291: 'non',\n",
       " 292: 'ct',\n",
       " 293: 'abd',\n",
       " 294: 'pelv',\n",
       " 295: 'payment',\n",
       " 296: 'chest',\n",
       " 297: 'charge',\n",
       " 298: 'harges',\n",
       " 299: 'digit',\n",
       " 300: 'today',\n",
       " 301: \"'s\",\n",
       " 302: 'ethnicity',\n",
       " 303: 'hispanic',\n",
       " 304: 'latino',\n",
       " 305: 'preferred',\n",
       " 306: 'english',\n",
       " 307: 'suzanne',\n",
       " 308: 'newsom',\n",
       " 309: 'cnp',\n",
       " 310: '•',\n",
       " 311: 'lethargy',\n",
       " 312: 'cough',\n",
       " 313: 'vitals',\n",
       " 314: 'lbs',\n",
       " 315: 'kg',\n",
       " 316: 'wt',\n",
       " 317: 'temp',\n",
       " 318: 'hr',\n",
       " 319: 'oxygen',\n",
       " 320: 'sat',\n",
       " 321: 'allergies',\n",
       " 322: 'amoxicillin',\n",
       " 323: 'rash',\n",
       " 324: 'possible',\n",
       " 325: 'hives',\n",
       " 326: 'active',\n",
       " 327: 'diagnoses',\n",
       " 328: 'include',\n",
       " 329: 'frontal',\n",
       " 330: 'sinusitis',\n",
       " 331: 'unspecified',\n",
       " 332: 'dizziness',\n",
       " 333: 'giddiness',\n",
       " 334: 'medication',\n",
       " 335: 'list',\n",
       " 336: 'medications',\n",
       " 337: 'taking',\n",
       " 338: 'zyrtec',\n",
       " 339: 'childrens',\n",
       " 340: 'allergy',\n",
       " 341: 'notes',\n",
       " 342: 'tests',\n",
       " 343: 'illumigene',\n",
       " 344: 'myco',\n",
       " 345: 'http',\n",
       " 346: 'basic',\n",
       " 347: 'metabolic',\n",
       " 348: 'sodium',\n",
       " 349: 'range',\n",
       " 350: 'potassium',\n",
       " 351: 'chloride',\n",
       " 352: 'glucose',\n",
       " 353: 'bun',\n",
       " 354: 'creatinine',\n",
       " 355: 'calcium',\n",
       " 356: 'crea',\n",
       " 357: 'ratio',\n",
       " 358: 'anion',\n",
       " 359: 'gap',\n",
       " 360: 'calc',\n",
       " 361: 'cbc',\n",
       " 362: 'diff',\n",
       " 363: 'wbc',\n",
       " 364: 'rbc',\n",
       " 365: 'hgb',\n",
       " 366: 'hct',\n",
       " 367: 'mcv',\n",
       " 368: 'fl',\n",
       " 369: 'mch',\n",
       " 370: 'pg',\n",
       " 371: 'mchc',\n",
       " 372: 'mpv',\n",
       " 373: 'platelets',\n",
       " 374: 'neutrophils',\n",
       " 375: 'lymphocytes',\n",
       " 376: 'monocytes',\n",
       " 377: 'conditions',\n",
       " 378: 'problem',\n",
       " 379: 'idiopathic',\n",
       " 380: 'urticaria',\n",
       " 381: 'document',\n",
       " 382: 'wish',\n",
       " 383: 'keep',\n",
       " 384: 'policyholder',\n",
       " 385: 'owner',\n",
       " 386: 'eastside',\n",
       " 387: 'acct',\n",
       " 388: 'jasminder',\n",
       " 389: 'singh',\n",
       " 390: 'dev',\n",
       " 391: 'pa',\n",
       " 392: 'excuse',\n",
       " 393: 'east',\n",
       " 394: 'side',\n",
       " 395: 'unum',\n",
       " 396: 'april',\n",
       " 397: 'weekly',\n",
       " 398: 'ph',\n",
       " 399: 'primary',\n",
       " 400: 'thoracic',\n",
       " 401: 'strain',\n",
       " 402: 'strained',\n",
       " 403: 'following',\n",
       " 404: 'occurs',\n",
       " 405: 'feel',\n",
       " 406: 'weakness',\n",
       " 407: 'arms',\n",
       " 408: 'legs',\n",
       " 409: 'severe',\n",
       " 410: 'increase',\n",
       " 411: 'pain',\n",
       " 412: 'lumbosacral',\n",
       " 413: 'weak',\n",
       " 414: 'becomes',\n",
       " 415: 'follow',\n",
       " 416: 'take',\n",
       " 417: 'directed',\n",
       " 418: 'additional',\n",
       " 419: 'prescriptions',\n",
       " 420: 'prescriber',\n",
       " 421: 'paper',\n",
       " 422: 'prescription',\n",
       " 423: 'given',\n",
       " 424: 'preventative',\n",
       " 425: 'instructions',\n",
       " 426: 'diagnosis',\n",
       " 427: 'knee',\n",
       " 428: 'david',\n",
       " 429: 'bruce',\n",
       " 430: 'identiﬁer',\n",
       " 431: 'june',\n",
       " 432: 'concussion',\n",
       " 433: 'devin',\n",
       " 434: 'conrad',\n",
       " 435: 'september',\n",
       " 436: 'form',\n",
       " 437: 'attending',\n",
       " 438: 'part',\n",
       " 439: 'completed',\n",
       " 440: 'icd',\n",
       " 441: 'unable',\n",
       " 442: 'expected',\n",
       " 443: 'delivery',\n",
       " 444: 'actual',\n",
       " 445: 'vaginal',\n",
       " 446: 'per',\n",
       " 447: 'continued',\n",
       " 448: 'facility',\n",
       " 449: 'state',\n",
       " 450: 'zip',\n",
       " 451: 'performed',\n",
       " 452: 'surgical',\n",
       " 453: 'cpt',\n",
       " 454: 'degree',\n",
       " 455: 'check',\n",
       " 456: 'filing',\n",
       " 457: 'b',\n",
       " 458: 'suffix',\n",
       " 459: 'mi',\n",
       " 460: 'spanish',\n",
       " 461: 'short',\n",
       " 462: 'term',\n",
       " 463: 'disability',\n",
       " 464: 'long',\n",
       " 465: 'life',\n",
       " 466: 'insurance',\n",
       " 467: 'voluntary',\n",
       " 468: 'motor',\n",
       " 469: 'vehicle',\n",
       " 470: 'physicians',\n",
       " 471: 'hospitals',\n",
       " 472: 'considerations',\n",
       " 473: 'male',\n",
       " 474: 'female',\n",
       " 475: 'member',\n",
       " 476: 'relationship',\n",
       " 477: 'person',\n",
       " 478: 'marital',\n",
       " 479: 'single',\n",
       " 480: 'occ',\n",
       " 481: 'title',\n",
       " 482: 'resinmixer',\n",
       " 483: 'hire',\n",
       " 484: 'termination',\n",
       " 485: 'atw',\n",
       " 486: 'limitations',\n",
       " 487: 'permitted',\n",
       " 488: 'months',\n",
       " 489: 'office',\n",
       " 490: 'crane',\n",
       " 491: 'composites',\n",
       " 492: 'florence',\n",
       " 493: 'earn',\n",
       " 494: 'change',\n",
       " 495: 'leave',\n",
       " 496: 'absence',\n",
       " 497: 'record',\n",
       " 498: 'loaded',\n",
       " 499: 'residence',\n",
       " 500: 'physical',\n",
       " 501: 'access',\n",
       " 502: 'home',\n",
       " 503: 'supervisor',\n",
       " 504: 'coverages',\n",
       " 505: 'product',\n",
       " 506: 'flex',\n",
       " 507: 'funding',\n",
       " 508: 'fully',\n",
       " 509: 'division',\n",
       " 510: 'eff',\n",
       " 511: 'earnings',\n",
       " 512: 'hourly',\n",
       " 513: 'mode',\n",
       " 514: 'aso',\n",
       " 515: 'self',\n",
       " 516: 'probable',\n",
       " 517: 'duration',\n",
       " 518: 'condition',\n",
       " 519: 'dates',\n",
       " 520: 'admission',\n",
       " 521: 'treatment',\n",
       " 522: 'seen',\n",
       " 523: 'past',\n",
       " 524: 'anticipated',\n",
       " 525: 'nature',\n",
       " 526: 'estimated',\n",
       " 527: 'treatments',\n",
       " 528: 'job',\n",
       " 529: 'attached',\n",
       " 530: 'checked',\n",
       " 531: 'episodic',\n",
       " 532: 'flare',\n",
       " 533: 'ups',\n",
       " 534: 'practice',\n",
       " 535: 'care',\n",
       " 536: 'including',\n",
       " 537: 'confinement',\n",
       " 538: 'provide',\n",
       " 539: 'advice',\n",
       " 540: 'stop',\n",
       " 541: 'mgmt',\n",
       " 542: 'svc',\n",
       " 543: 'applicable',\n",
       " 544: 'deductions',\n",
       " 545: 'schedule',\n",
       " 546: 'week',\n",
       " 547: 'sick',\n",
       " 548: 'variable',\n",
       " 549: 'sunday',\n",
       " 550: 'monday',\n",
       " 551: 'tuesday',\n",
       " 552: 'wednesday',\n",
       " 553: 'thursday',\n",
       " 554: 'friday',\n",
       " 555: 'saturday',\n",
       " 556: 'hospitalized',\n",
       " 557: 'explain',\n",
       " 558: 'height',\n",
       " 559: 'weight',\n",
       " 560: 'secondary',\n",
       " 561: 'functional',\n",
       " 562: 'capacity',\n",
       " 563: 'restrictions',\n",
       " 564: 'elizabeth',\n",
       " 565: 'edgewood',\n",
       " 566: 'facesheet',\n",
       " 567: 'sex',\n",
       " 568: 'adm',\n",
       " 569: 'demographics',\n",
       " 570: 'ssn',\n",
       " 571: 'reg',\n",
       " 572: 'verified',\n",
       " 573: 'renew',\n",
       " 574: 'admitting',\n",
       " 575: 'larkin',\n",
       " 576: 'john',\n",
       " 577: 'md',\n",
       " 578: 'elective',\n",
       " 579: 'incomplete',\n",
       " 580: 'area',\n",
       " 581: 'edg',\n",
       " 582: 'sc',\n",
       " 583: 'crestview',\n",
       " 584: 'discharged',\n",
       " 585: 'confirmed',\n",
       " 586: 'hose',\n",
       " 587: 'ital',\n",
       " 588: 'class',\n",
       " 589: 'guarantor',\n",
       " 590: 'relation',\n",
       " 591: 'pt',\n",
       " 592: 'seh',\n",
       " 593: 'precert',\n",
       " 594: 'subscriber',\n",
       " 595: 'operative',\n",
       " 596: 'brief',\n",
       " 597: 'op',\n",
       " 598: 'note',\n",
       " 599: 'author',\n",
       " 600: 'j',\n",
       " 601: 'filed',\n",
       " 602: 'editor',\n",
       " 603: 'healthcare',\n",
       " 604: 'body',\n",
       " 605: 'mass',\n",
       " 606: 'index',\n",
       " 607: 'role',\n",
       " 608: 'anesthesia',\n",
       " 609: 'general',\n",
       " 610: 'specimens',\n",
       " 611: 'log',\n",
       " 612: 'blood',\n",
       " 613: 'loss',\n",
       " 614: 'course',\n",
       " 615: 'pacu',\n",
       " 616: 'operation',\n",
       " 617: 'best',\n",
       " 618: 'reached',\n",
       " 619: 'broken',\n",
       " 620: 'big',\n",
       " 621: 'toe',\n",
       " 622: 'foot',\n",
       " 623: 'todd',\n",
       " 624: 'francis',\n",
       " 625: 'podiatrist',\n",
       " 626: 'ryan',\n",
       " 627: 'kish',\n",
       " 628: 'toledo',\n",
       " 629: 'er',\n",
       " 630: 'xray',\n",
       " 631: 'july',\n",
       " 632: 'paramount',\n",
       " 633: 'promedica',\n",
       " 634: 'authorization',\n",
       " 635: 'modifier',\n",
       " 636: 'indicates',\n",
       " 637: 'c',\n",
       " 638: 'dpm',\n",
       " 639: 'ems',\n",
       " 640: 'christine',\n",
       " 641: 'nolen',\n",
       " 642: 'waukesha',\n",
       " 643: 'memorial',\n",
       " 644: 'cleaning',\n",
       " 645: 'bandage',\n",
       " 646: 'montano',\n",
       " 647: 'ci',\n",
       " 648: 'web',\n",
       " 649: 'user',\n",
       " 650: 'prohealth',\n",
       " 651: 'umr',\n",
       " 652: 'adjustments',\n",
       " 653: 'patients',\n",
       " 654: 'invoice',\n",
       " 655: 'previous',\n",
       " 656: 'return',\n",
       " 657: 'portion',\n",
       " 658: 'mastercard',\n",
       " 659: 'discover',\n",
       " 660: 'american',\n",
       " 661: 'express',\n",
       " 662: 'card',\n",
       " 663: 'enclosed',\n",
       " 664: 'emergency',\n",
       " 665: 'associates',\n",
       " 666: 'using',\n",
       " 667: 'addressee',\n",
       " 668: 'industrial',\n",
       " 669: 'loop',\n",
       " 670: 'dr',\n",
       " 671: 'dept',\n",
       " 672: 'ppo',\n",
       " 673: 'adj',\n",
       " 674: 'applied',\n",
       " 675: 'rendered',\n",
       " 676: 'fiserv',\n",
       " 677: 'wi',\n",
       " 678: 'stmt',\n",
       " 679: 'doctor',\n",
       " 680: 'legend',\n",
       " 681: 'd',\n",
       " 682: 'comments',\n",
       " 683: 'souha',\n",
       " 684: 'hakim',\n",
       " 685: 'medexpress',\n",
       " 686: 'codes',\n",
       " 687: 'urgent',\n",
       " 688: 'clairn',\n",
       " 689: 'vijay',\n",
       " 690: 'patel',\n",
       " 691: 'holder',\n",
       " 692: 'qty',\n",
       " 693: 'clinical',\n",
       " 694: 'chief',\n",
       " 695: 'penicillins',\n",
       " 696: 'taken',\n",
       " 697: 'bp',\n",
       " 698: 'mmhg',\n",
       " 699: 'pulse',\n",
       " 700: 'bpm',\n",
       " 701: 'resp',\n",
       " 702: 'lb',\n",
       " 703: 'ft',\n",
       " 704: 'bmi',\n",
       " 705: 'meds',\n",
       " 706: 'albuterol',\n",
       " 707: 'sulfate',\n",
       " 708: 'encounter',\n",
       " 709: 'progress',\n",
       " 710: 'subjective',\n",
       " 711: 'history',\n",
       " 712: 'provided',\n",
       " 713: 'dad',\n",
       " 714: 'interpreter',\n",
       " 715: 'used',\n",
       " 716: 'presents',\n",
       " 717: 'review',\n",
       " 718: 'systems',\n",
       " 719: 'cardiovascular',\n",
       " 720: 'negative',\n",
       " 721: 'skin',\n",
       " 722: 'neurological',\n",
       " 723: 'headaches',\n",
       " 724: 'objective',\n",
       " 725: 'hent',\n",
       " 726: 'right',\n",
       " 727: 'ear',\n",
       " 728: 'tympanic',\n",
       " 729: 'membrane',\n",
       " 730: 'normal',\n",
       " 731: 'nose',\n",
       " 732: 'oropharynx',\n",
       " 733: 'clear',\n",
       " 734: 'eyes',\n",
       " 735: 'conjunctivae',\n",
       " 736: 'eom',\n",
       " 737: 'neck',\n",
       " 738: 'supple',\n",
       " 739: 'rigidity',\n",
       " 740: 'murmur',\n",
       " 741: 'heard',\n",
       " 742: 'lymphadenopathy',\n",
       " 743: 'cervical',\n",
       " 744: 'adenopathy',\n",
       " 745: 'alert',\n",
       " 746: 'warm',\n",
       " 747: 'noted',\n",
       " 748: 'assessment',\n",
       " 749: 'advise',\n",
       " 750: 'ss',\n",
       " 751: 'penobscot',\n",
       " 752: 'community',\n",
       " 753: 'suite',\n",
       " 754: 'medicine',\n",
       " 755: 'mental',\n",
       " 756: 'transmission',\n",
       " 757: 'sheet',\n",
       " 758: 'ext',\n",
       " 759: 'pages',\n",
       " 760: 'cover',\n",
       " 761: 'thank',\n",
       " 762: 'revised',\n",
       " 763: 'imaging',\n",
       " 764: 'mainecare',\n",
       " 765: 'fqhc',\n",
       " 766: 'low',\n",
       " 767: 'jt',\n",
       " 768: 'erin',\n",
       " 769: 'barker',\n",
       " 770: 'joseph',\n",
       " 771: 'ordering',\n",
       " 772: 'intercondylar',\n",
       " 773: 'space',\n",
       " 774: 'acl',\n",
       " 775: 'pcl',\n",
       " 776: 'intact',\n",
       " 777: 'unremarkable',\n",
       " 778: 'marrow',\n",
       " 779: 'signal',\n",
       " 780: 'dictation',\n",
       " 781: 'location',\n",
       " 782: 'mpsynernet',\n",
       " 783: 'reading',\n",
       " 784: 'kasper',\n",
       " 785: 'jared',\n",
       " 786: 'orthopedics',\n",
       " 787: 'sports',\n",
       " 788: 'np',\n",
       " 789: 'thompson',\n",
       " 790: 'mcguire',\n",
       " 791: 'initial',\n",
       " 792: 'evaluation',\n",
       " 793: 'responsible',\n",
       " 794: 'samara',\n",
       " 795: 'shiromani',\n",
       " 796: 'cc',\n",
       " 797: 'present',\n",
       " 798: 'illness',\n",
       " 799: 'persistent',\n",
       " 800: 'patella',\n",
       " 801: 'alta',\n",
       " 802: 'tendonitis',\n",
       " 803: 'asthma',\n",
       " 804: 'fractures',\n",
       " 805: 'changes',\n",
       " 806: 'father',\n",
       " 807: 'htn',\n",
       " 808: 'sister',\n",
       " 809: 'factor',\n",
       " 810: 'v',\n",
       " 811: 'seizures',\n",
       " 812: 'hs',\n",
       " 813: 'risk',\n",
       " 814: 'factors',\n",
       " 815: 'tobacco',\n",
       " 816: 'use',\n",
       " 817: 'never',\n",
       " 818: 'smoker',\n",
       " 819: 'passive',\n",
       " 820: 'smoke',\n",
       " 821: 'exposure',\n",
       " 822: 'alcohol',\n",
       " 823: 'drug',\n",
       " 824: 'caffeine',\n",
       " 825: 'drinks',\n",
       " 826: 'cellular',\n",
       " 827: 'medsupport',\n",
       " 828: 'exercise',\n",
       " 829: 'times',\n",
       " 830: 'field',\n",
       " 831: 'hockey',\n",
       " 832: 'cardio',\n",
       " 833: 'seatbelt',\n",
       " 834: 'sun',\n",
       " 835: 'occasionally',\n",
       " 836: 'fall',\n",
       " 837: 'problems',\n",
       " 838: 'dx',\n",
       " 839: 'critical',\n",
       " 840: 'chemicals',\n",
       " 841: 'zithromax',\n",
       " 842: 'azithromycin',\n",
       " 843: 'adhesive',\n",
       " 844: 'tape',\n",
       " 845: 'exertion',\n",
       " 846: 'complains',\n",
       " 847: 'denies',\n",
       " 848: 'varicose',\n",
       " 849: 'veins',\n",
       " 850: 'bone',\n",
       " 851: 'deformity',\n",
       " 852: 'neuro',\n",
       " 853: 'psych',\n",
       " 854: 'post',\n",
       " 855: 'traumatic',\n",
       " 856: 'stress',\n",
       " 857: 'disorder',\n",
       " 858: 'endo',\n",
       " 859: 'vital',\n",
       " 860: 'signs',\n",
       " 861: 'profile',\n",
       " 862: 'inches',\n",
       " 863: 'pounds',\n",
       " 864: 'rate',\n",
       " 865: 'minute',\n",
       " 866: 'sitting',\n",
       " 867: 'arm',\n",
       " 868: 'r',\n",
       " 869: 'intensity',\n",
       " 870: 'aching',\n",
       " 871: 'sharp',\n",
       " 872: 'lungs',\n",
       " 873: 'bilaterally',\n",
       " 874: 'p',\n",
       " 875: 'heart',\n",
       " 876: 'recommendations',\n",
       " 877: 'james',\n",
       " 878: 'greene',\n",
       " 879: 'orders',\n",
       " 880: 'assistant',\n",
       " 881: 'room',\n",
       " 882: 'diagnostic',\n",
       " 883: 'arthroscopy',\n",
       " 884: 'danielle',\n",
       " 885: 'st',\n",
       " 886: 'onge',\n",
       " 887: 'femoral',\n",
       " 888: 'block',\n",
       " 889: 'pac',\n",
       " 890: 'chondromalacia',\n",
       " 891: 'burning',\n",
       " 892: 'entered',\n",
       " 893: 'amy',\n",
       " 894: 'cyr',\n",
       " 895: 'progressing',\n",
       " 896: 'well',\n",
       " 897: 'postoperatively',\n",
       " 898: 'mariah',\n",
       " 899: 'larsen',\n",
       " 900: 'ma',\n",
       " 901: 'screening',\n",
       " 902: 'constitutional',\n",
       " 903: 'developed',\n",
       " 904: 'dtrs',\n",
       " 905: 'omega',\n",
       " 906: 'mg',\n",
       " 907: 'capsule',\n",
       " 908: 'one',\n",
       " 909: 'n',\n",
       " 910: 'counseling',\n",
       " 911: 'educational',\n",
       " 912: 'finley',\n",
       " 913: 'kevin',\n",
       " 914: 'bal',\n",
       " 915: 'item',\n",
       " 916: 'immunization',\n",
       " 917: 'admin',\n",
       " 918: 'ovr',\n",
       " 919: 'reach',\n",
       " 920: 'centers',\n",
       " 921: 'bethel',\n",
       " 922: 'year',\n",
       " 923: 'old',\n",
       " 924: 'started',\n",
       " 925: 'ago',\n",
       " 926: 'slipping',\n",
       " 927: 'ice',\n",
       " 928: 'tools',\n",
       " 929: 'screenings',\n",
       " 930: 'instrument',\n",
       " 931: 'score',\n",
       " 932: 'mdd',\n",
       " 933: 'classification',\n",
       " 934: 'questionnaire',\n",
       " 935: 'testing',\n",
       " 936: 'coronary',\n",
       " 937: 'disease',\n",
       " 938: 'order',\n",
       " 939: 'interpretation',\n",
       " 940: 'result',\n",
       " 941: 'region',\n",
       " 942: 'updated',\n",
       " 943: 'retired',\n",
       " 944: 'river',\n",
       " 945: 'sales',\n",
       " 946: 'manager',\n",
       " 947: 'support',\n",
       " 948: 'currently',\n",
       " 949: 'married',\n",
       " 950: 'smoking',\n",
       " 951: 'usage',\n",
       " 952: 'years',\n",
       " 953: 'pack',\n",
       " 954: 'prior',\n",
       " 955: 'sig',\n",
       " 956: 'desc',\n",
       " 957: 'start',\n",
       " 958: 'refilled',\n",
       " 959: 'elsewhere',\n",
       " 960: 'reconciliation',\n",
       " 961: 'reconciled',\n",
       " 962: 'ingredient',\n",
       " 963: 'reaction',\n",
       " 964: 'comment',\n",
       " 965: 'known',\n",
       " 966: 'system',\n",
       " 967: 'respiratory',\n",
       " 968: 'dyspnea',\n",
       " 969: 'integumentary',\n",
       " 970: 'ms',\n",
       " 971: 'positive',\n",
       " 972: 'standing',\n",
       " 973: 'dressed',\n",
       " 974: 'shoes',\n",
       " 975: 'pressure',\n",
       " 976: 'oral',\n",
       " 977: 'regular',\n",
       " 978: 'somatic',\n",
       " 979: 'rib',\n",
       " 980: 'dysfunction',\n",
       " 981: 'michael',\n",
       " 982: 'gould',\n",
       " 983: 'county',\n",
       " 984: 'potomac',\n",
       " 985: 'valley',\n",
       " 986: '”',\n",
       " 987: 'sate',\n",
       " 988: 'wo',\n",
       " 989: 'cm',\n",
       " 990: 'x',\n",
       " 991: 'baker',\n",
       " 992: 'cyst',\n",
       " 993: 'approved',\n",
       " 994: 'croft',\n",
       " 995: 'stone',\n",
       " 996: 'narrative',\n",
       " 997: 'indication',\n",
       " 998: 'posterior',\n",
       " 999: 'lateral',\n",
       " ...}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'amunt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a14dae08c28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                              \u001b[0mmax_encoder_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_encoder_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                              \u001b[0mnum_encoder_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_encoder_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                                              vocab_to_int=vocab_to_int)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-c212e58e66f4>\u001b[0m in \u001b[0;36mvectorize_data\u001b[0;34m(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# c0..cn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocess_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# c0'..cm'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'amunt'"
     ]
    }
   ],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  \n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('encoder_model.hdf5')\n",
    "decoder_model.save('decoder_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_gt_sequence(encoder_input_data[5:6], int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Medical Terms dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_MedTerms\n",
    "target_texts = target_texts_MedTerms\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    text = input_texts[seq_index]\n",
    "    decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "    print('-')\n",
    "    print('Input sentence:', text)\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for seq_index in range(100):\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    text = test_input_texts[seq_index]\n",
    "\n",
    "    decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "    print('-')\n",
    "    print('Input sentence:', text)\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on separate tesseract corrected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(100):\n",
    "\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    text = input_texts[seq_index]\n",
    "    decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "    print('-')\n",
    "    print('Input sentence:', text)\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain transfer from noisy spelling mistakes to OCR corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on noisy spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_transfer.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on OCR correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "# Keep test data from the corrected OCR, as this what we care about\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# Vectorize train data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "# Vectorize test data\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.001# Reduce the learning rate for fine tuning\n",
    "model.load_weights('best_model_transfer.hdf5')\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_transfer.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
