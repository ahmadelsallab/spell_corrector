{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    #for row in open(file_name, encoding='utf8'):\n",
    "    for row in open(file_name):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(\"\\t\")\n",
    "            input_text = sents[0]\n",
    "            \n",
    "            target_text = '\\t' + sents[1] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[1])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        #for row in open(file_name, encoding='utf8'):\n",
    "        for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0\n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "    # Add special tokens to vocab_to_int\n",
    "    codes = ['\\t','\\n']\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t, vocab_to_int[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, vocab_to_int[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, vocab_to_int['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sent_len = 1000000\n",
    "min_sent_len = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Histogram of lenghts\n",
    "lengths = []\n",
    "for text in input_texts:\n",
    "    lengths.append(len(text))\n",
    "    lengths.append(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEmpJREFUeJzt3W2MXNd93/Hvr2KkxM4DKWnlqiTd\npRPCrRKkNbGQ1bowjKjVkwNTBSxARhARDguiqJw6dYOYrl8oSBBA7kPUCkgFMBFjqjDkGI4DEZVS\nhZAdGAUqxStH1oMZhWtZEddkxA0oK0GNxFHy74s5W4+Xy32YWe6Se74fYDD3/u+Zuefs5cxv7z0z\ny1QVkqT+/J2N7oAkaWMYAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTywZAksNJziR5fpFtP5+k\nklzd1pPk/iQzSZ5Nsmeo7b4kJ9pt39oOQ5K0Wis5A/gkcMvCYpKdwL8AXhkq3wrsbrcDwAOt7ZXA\nPcA7geuBe5JsG6fjkqTxbFmuQVV9McnkIpvuA34BeGSothd4qAZfL34yydYk1wLvAY5V1VmAJMcY\nhMrDS+376quvrsnJxXYtSTqfp59++s+qamK5dssGwGKSvA/4RlV9Jcnwpu3AyaH12VY7X31Jk5OT\nTE9Pj9JFSepWkj9ZSbtVB0CSNwEfB25abPMitVqivtjzH2Bw+Yi3vvWtq+2eJGmFRvkU0A8Du4Cv\nJHkZ2AF8OcnfZfCb/c6htjuAU0vUz1FVh6pqqqqmJiaWPYORJI1o1QFQVc9V1TVVNVlVkwze3PdU\n1Z8CR4G72qeBbgBer6rTwOPATUm2tcnfm1pNkrRBVvIx0IeB/wO8Pclskv1LNH8MeAmYAX4d+DcA\nbfL3l4EvtdsvzU8IS5I2Ri7m/w9gamqqnASWpNVJ8nRVTS3Xzm8CS1KnDABJ6pQBIEmdMgAkqVMG\ngCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1KlNHwCTBx/d\n6C5I0kVp0weAJGlxBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqeWDYAkh5OcSfL8UO0/JfmjJM8m+Z0k\nW4e2fSzJTJIXk9w8VL+l1WaSHFz7oUiSVmMlZwCfBG5ZUDsG/FhV/Tjwx8DHAJJcB9wJ/Gh7zH9P\nclmSy4BfA24FrgM+0NpKkjbIsgFQVV8Ezi6o/V5VvdFWnwR2tOW9wKer6q+q6uvADHB9u81U1UtV\n9W3g062tJGmDrMUcwM8Av9uWtwMnh7bNttr56pKkDTJWACT5OPAG8Kn50iLNaon6Ys95IMl0kum5\nublxuidJWsLIAZBkH/CTwE9V1fyb+Sywc6jZDuDUEvVzVNWhqpqqqqmJiYlRu/dd/HtAknSukQIg\nyS3AR4H3VdW3hjYdBe5MckWSXcBu4A+ALwG7k+xKcjmDieKj43VdkjSOLcs1SPIw8B7g6iSzwD0M\nPvVzBXAsCcCTVfWvq+qFJJ8Bvsrg0tDdVfU37Xk+BDwOXAYcrqoXLsB4JEkrtGwAVNUHFik/uET7\nXwF+ZZH6Y8Bjq+qdJOmC8ZvAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLU\nKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0y\nACSpU8sGQJLDSc4keX6odmWSY0lOtPttrZ4k9yeZSfJskj1Dj9nX2p9Isu/CDEeStFIrOQP4JHDL\ngtpB4Imq2g080dYBbgV2t9sB4AEYBAZwD/BO4HrgnvnQkCRtjGUDoKq+CJxdUN4LHGnLR4Dbh+oP\n1cCTwNYk1wI3A8eq6mxVvQYc49xQkSSto1HnAN5SVacB2v01rb4dODnUbrbVzleXJG2QtZ4EziK1\nWqJ+7hMkB5JMJ5mem5tb085Jkr5j1AB4tV3aod2fafVZYOdQux3AqSXq56iqQ1U1VVVTExMTI3ZP\nkrScUQPgKDD/SZ59wCND9bvap4FuAF5vl4geB25Ksq1N/t7UapKkDbJluQZJHgbeA1ydZJbBp3nu\nBT6TZD/wCnBHa/4YcBswA3wL+CBAVZ1N8svAl1q7X6qqhRPLkqR1tGwAVNUHzrPpxkXaFnD3eZ7n\nMHB4Vb2TJF0wfhNYkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdWpTB8DkwUc3uguSdNHa1AEgSTo/\nA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIA\nJKlTBoAkdWqsAEjy75K8kOT5JA8n+d4ku5I8leREkt9Kcnlre0Vbn2nbJ9diAJKk0YwcAEm2A/8W\nmKqqHwMuA+4EPgHcV1W7gdeA/e0h+4HXqupHgPtaO0nSBhn3EtAW4PuSbAHeBJwGfgL4bNt+BLi9\nLe9t67TtNybJmPuXJI1o5ACoqm8A/xl4hcEb/+vA08A3q+qN1mwW2N6WtwMn22PfaO2vGnX/kqTx\njHMJaBuD3+p3AX8PeDNw6yJNa/4hS2wbft4DSaaTTM/NzY3aPUnSMsa5BPTPga9X1VxV/TXwOeCf\nAlvbJSGAHcCptjwL7ARo238IOLvwSavqUFVNVdXUxMTEGN2TJC1lnAB4BbghyZvatfwbga8CXwDe\n39rsAx5py0fbOm3756vqnDMASdL6GGcO4CkGk7lfBp5rz3UI+CjwkSQzDK7xP9ge8iBwVat/BDg4\nRr8lSWPasnyT86uqe4B7FpRfAq5fpO1fAneMsz9J0trxm8CS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0y\nACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNA\nkjplAEhSpwwASeqUASBJnTIAJKlTYwVAkq1JPpvkj5IcT/JPklyZ5FiSE+1+W2ubJPcnmUnybJI9\nazMESdIoxj0D+G/A/6qqfwD8I+A4cBB4oqp2A0+0dYBbgd3tdgB4YMx9S5LGMHIAJPlB4N3AgwBV\n9e2q+iawFzjSmh0Bbm/Le4GHauBJYGuSa0fuuSRpLOOcAbwNmAN+M8kfJvmNJG8G3lJVpwHa/TWt\n/Xbg5NDjZ1vtuyQ5kGQ6yfTc3NwY3ZMkLWWcANgC7AEeqKp3AP+X71zuWUwWqdU5hapDVTVVVVMT\nExNjdE+StJRxAmAWmK2qp9r6ZxkEwqvzl3ba/Zmh9juHHr8DODXG/iVJYxg5AKrqT4GTSd7eSjcC\nXwWOAvtabR/wSFs+CtzVPg10A/D6/KUiSdL62zLm438W+FSSy4GXgA8yCJXPJNkPvALc0do+BtwG\nzADfam0lSRtkrACoqmeAqUU23bhI2wLuHmd/45g8+Cgv3/vejdq9JF10/CawJHXKAJCkTnUVAJMH\nH93oLkjSRaOrAJAkfYcBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIA\nJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU2MHQJLLkvxhkv/Z1ncleSrJ\niSS/leTyVr+irc+07ZPj7luSNLq1OAP4MHB8aP0TwH1VtRt4Ddjf6vuB16rqR4D7WjtJ0gYZKwCS\n7ADeC/xGWw/wE8BnW5MjwO1teW9bp22/sbWXJG2Acc8A/ivwC8DftvWrgG9W1RttfRbY3pa3AycB\n2vbXW/vvkuRAkukk03Nzc2N2T5J0PiMHQJKfBM5U1dPD5UWa1gq2fadQdaiqpqpqamJiYtTuSZKW\nsWWMx74LeF+S24DvBX6QwRnB1iRb2m/5O4BTrf0ssBOYTbIF+CHg7Bj7lySNYeQzgKr6WFXtqKpJ\n4E7g81X1U8AXgPe3ZvuAR9ry0bZO2/75qjrnDECStD4uxPcAPgp8JMkMg2v8D7b6g8BVrf4R4OAF\n2LckaYXGuQT0/1XV7wO/35ZfAq5fpM1fAnesxf4kSePzm8CS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0y\nACSpUwaAJHWquwCYPPjoRndBki4K3QWAJGnAAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAk\nqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSp0YOgCQ7k3whyfEkLyT5cKtfmeRYkhPtflurJ8n9SWaS\nPJtkz1oNQpK0euOcAbwB/Puq+ofADcDdSa4DDgJPVNVu4Im2DnArsLvdDgAPjLFvSdKYRg6Aqjpd\nVV9uy38BHAe2A3uBI63ZEeD2trwXeKgGngS2Jrl25J5LksayJnMASSaBdwBPAW+pqtMwCAngmtZs\nO3By6GGzrSZJ2gBjB0CS7wd+G/i5qvrzpZouUqtFnu9Akukk03Nzc+N2T5J0HmMFQJLvYfDm/6mq\n+lwrvzp/aafdn2n1WWDn0MN3AKcWPmdVHaqqqaqampiYGKd75+V/CylJ430KKMCDwPGq+tWhTUeB\nfW15H/DIUP2u9mmgG4DX5y8VSZLW35YxHvsu4KeB55I802r/AbgX+EyS/cArwB1t22PAbcAM8C3g\ng2PsW5I0ppEDoKr+N4tf1we4cZH2Bdw96v4kSWur228COw8gqXfdBoAk9c4AkKROGQCS1CkDQJI6\nZQBIUqcMAEnqlAEgSZ3qOgD8LoCknnUdAJLUMwNAkjplAEhSp7oPAOcBJPWq+wCQpF4ZAAzOAjwT\nkNQbA0CSOmUASFKnDABJ6pQBsAznBiRtVgbAEN/sJfXEAFhgnBAwQCRdSrZsdAcuRgvfyCcPPsrL\n9753g3ojSRfGup8BJLklyYtJZpIcXO/9j2o+FC6F3/L9XoOklVjXAEhyGfBrwK3AdcAHkly3nn0Y\nx2JnBovVR33ezfamvdnGI202630GcD0wU1UvVdW3gU8De9e5D2Mb/g17JW9yax0UkrQWUlXrt7Pk\n/cAtVfWv2vpPA++sqg8t1n5qaqqmp6dH3t+l8Ib58r3vXbKfS21fuG2xtovNXcy3md+2cH2+Nvx8\no8yBLPXYlc6rLGy3mvmY1c7dbNRcz8U4x3Qx9qkn4/78kzxdVVPLtlvnALgDuHlBAFxfVT871OYA\ncKCtvh14cYxdXg382RiPv1Q57r447r6sZNx/v6omlnui9f4U0Cywc2h9B3BquEFVHQIOrcXOkkyv\nJAU3G8fdF8fdl7Uc93rPAXwJ2J1kV5LLgTuBo+vcB0kS63wGUFVvJPkQ8DhwGXC4ql5Yzz5IkgbW\n/YtgVfUY8Ng67W5NLiVdghx3Xxx3X9Zs3Os6CSxJunj4t4AkqVObMgAu1T83sVJJXk7yXJJnkky3\n2pVJjiU50e63tXqS3N9+Fs8m2bOxvV+5JIeTnEny/FBt1eNMsq+1P5Fk30aMZTXOM+5fTPKNdsyf\nSXLb0LaPtXG/mOTmofol9TpIsjPJF5IcT/JCkg+3+qY+5kuM+8If86raVDcGk8tfA94GXA58Bbhu\no/u1xmN8Gbh6Qe0/Agfb8kHgE235NuB3gQA3AE9tdP9XMc53A3uA50cdJ3Al8FK739aWt2302EYY\n9y8CP79I2+vav/ErgF3t3/5ll+LrALgW2NOWfwD44za+TX3Mlxj3BT/mm/EMYFP8uYkR7AWOtOUj\nwO1D9Ydq4Elga5JrN6KDq1VVXwTOLiivdpw3A8eq6mxVvQYcA2658L0f3XnGfT57gU9X1V9V1deB\nGQavgUvudVBVp6vqy235L4DjwHY2+TFfYtzns2bHfDMGwHbg5ND6LEv/MC9FBfxekqfbN6cB3lJV\np2HwDwq4ptU3289jtePcTOP/ULvUcXj+MgibdNxJJoF3AE/R0TFfMG64wMd8MwZAFqltto86vauq\n9jD4q6p3J3n3Em17+HnA+ce5Wcb/APDDwD8GTgP/pdU33biTfD/w28DPVdWfL9V0kdolO/ZFxn3B\nj/lmDIBl/9zEpa6qTrX7M8DvMDj1e3X+0k67P9Oab7afx2rHuSnGX1WvVtXfVNXfAr/O4JjDJht3\nku9h8Cb4qar6XCtv+mO+2LjX45hvxgDY1H9uIsmbk/zA/DJwE/A8gzHOf9phH/BIWz4K3NU+MXED\n8Pr86fQlarXjfBy4Kcm2dgp9U6tdUhbM2/xLBsccBuO+M8kVSXYBu4E/4BJ8HSQJ8CBwvKp+dWjT\npj7m5xv3uhzzjZ4Bv0Cz6rcxmEn/GvDxje7PGo/tbQxm978CvDA/PuAq4AngRLu/stXD4D/h+Rrw\nHDC10WNYxVgfZnDq+9cMfrvZP8o4gZ9hMFE2A3xwo8c14rj/RxvXs+1Ffe1Q+4+3cb8I3DpUv6Re\nB8A/Y3DJ4lngmXa7bbMf8yXGfcGPud8ElqRObcZLQJKkFTAAJKlTBoAkdcoAkKROGQCS1CkDQJI6\nZQBIUqcMAEnq1P8DnVnLpiihAA8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1ed468950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = plt.hist(lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1174.,   926.,  1344.,  1268.,  1192.,   768.,   568.,   548.,\n",
       "         404.,   318.,   172.,   194.,   166.,   228.,   116.,   148.,\n",
       "          88.,    76.,    74.,    78.,    74.,    42.,    38.,    32.,\n",
       "          32.,    26.,    30.,    30.,    50.,    50.,    30.,     8.,\n",
       "          24.,    26.,    20.,    22.,    12.,    14.,    22.,    12.,\n",
       "          18.,    22.,    10.,    28.,    16.,     4.,    14.,    40.,\n",
       "           2.,     6.,     6.,     0.,     6.,     2.,    34.,     4.,\n",
       "           0.,    12.,     6.,     6.,    10.,    14.,     6.,     2.,\n",
       "           4.,     2.,     6.,     2.,     0.,     2.,    16.,    12.,\n",
       "           8.,    16.,     2.,     2.,     4.,     4.,     2.,     0.,\n",
       "           2.,     0.,     2.,     6.,     2.,     4.,    10.,    28.,\n",
       "           4.,     2.,     4.,     4.,     2.,     0.,     4.,     4.,\n",
       "           6.,     0.,     4.,     0.,     4.,     4.,     2.,    22.,\n",
       "          22.,     2.,     2.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,    20.,     4.,    20.,     6.,     0.,\n",
       "           0.,     0.,     2.,    26.,     2.,     2.,     0.,     0.,\n",
       "           0.,     0.,     2.,     2.,     0.,     0.,     2.,     6.,\n",
       "           2.,     0.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     4.,    18.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     4.,     2.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     4.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     2.,\n",
       "           0.,     2.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   ,     4.906,     9.812,    14.718,    19.624,    24.53 ,\n",
       "          29.436,    34.342,    39.248,    44.154,    49.06 ,    53.966,\n",
       "          58.872,    63.778,    68.684,    73.59 ,    78.496,    83.402,\n",
       "          88.308,    93.214,    98.12 ,   103.026,   107.932,   112.838,\n",
       "         117.744,   122.65 ,   127.556,   132.462,   137.368,   142.274,\n",
       "         147.18 ,   152.086,   156.992,   161.898,   166.804,   171.71 ,\n",
       "         176.616,   181.522,   186.428,   191.334,   196.24 ,   201.146,\n",
       "         206.052,   210.958,   215.864,   220.77 ,   225.676,   230.582,\n",
       "         235.488,   240.394,   245.3  ,   250.206,   255.112,   260.018,\n",
       "         264.924,   269.83 ,   274.736,   279.642,   284.548,   289.454,\n",
       "         294.36 ,   299.266,   304.172,   309.078,   313.984,   318.89 ,\n",
       "         323.796,   328.702,   333.608,   338.514,   343.42 ,   348.326,\n",
       "         353.232,   358.138,   363.044,   367.95 ,   372.856,   377.762,\n",
       "         382.668,   387.574,   392.48 ,   397.386,   402.292,   407.198,\n",
       "         412.104,   417.01 ,   421.916,   426.822,   431.728,   436.634,\n",
       "         441.54 ,   446.446,   451.352,   456.258,   461.164,   466.07 ,\n",
       "         470.976,   475.882,   480.788,   485.694,   490.6  ,   495.506,\n",
       "         500.412,   505.318,   510.224,   515.13 ,   520.036,   524.942,\n",
       "         529.848,   534.754,   539.66 ,   544.566,   549.472,   554.378,\n",
       "         559.284,   564.19 ,   569.096,   574.002,   578.908,   583.814,\n",
       "         588.72 ,   593.626,   598.532,   603.438,   608.344,   613.25 ,\n",
       "         618.156,   623.062,   627.968,   632.874,   637.78 ,   642.686,\n",
       "         647.592,   652.498,   657.404,   662.31 ,   667.216,   672.122,\n",
       "         677.028,   681.934,   686.84 ,   691.746,   696.652,   701.558,\n",
       "         706.464,   711.37 ,   716.276,   721.182,   726.088,   730.994,\n",
       "         735.9  ,   740.806,   745.712,   750.618,   755.524,   760.43 ,\n",
       "         765.336,   770.242,   775.148,   780.054,   784.96 ,   789.866,\n",
       "         794.772,   799.678,   804.584,   809.49 ,   814.396,   819.302,\n",
       "         824.208,   829.114,   834.02 ,   838.926,   843.832,   848.738,\n",
       "         853.644,   858.55 ,   863.456,   868.362,   873.268,   878.174,\n",
       "         883.08 ,   887.986,   892.892,   897.798,   902.704,   907.61 ,\n",
       "         912.516,   917.422,   922.328,   927.234,   932.14 ,   937.046,\n",
       "         941.952,   946.858,   951.764,   956.67 ,   961.576,   966.482,\n",
       "         971.388,   976.294,   981.2  ,   986.106,   991.012,   995.918,\n",
       "        1000.824,  1005.73 ,  1010.636,  1015.542,  1020.448,  1025.354,\n",
       "        1030.26 ,  1035.166,  1040.072,  1044.978,  1049.884,  1054.79 ,\n",
       "        1059.696,  1064.602,  1069.508,  1074.414,  1079.32 ,  1084.226,\n",
       "        1089.132,  1094.038,  1098.944,  1103.85 ,  1108.756,  1113.662,\n",
       "        1118.568,  1123.474,  1128.38 ,  1133.286,  1138.192,  1143.098,\n",
       "        1148.004,  1152.91 ,  1157.816,  1162.722,  1167.628,  1172.534,\n",
       "        1177.44 ,  1182.346,  1187.252,  1192.158,  1197.064,  1201.97 ,\n",
       "        1206.876,  1211.782,  1216.688,  1221.594,  1226.5  ,  1231.406,\n",
       "        1236.312,  1241.218,  1246.124,  1251.03 ,  1255.936,  1260.842,\n",
       "        1265.748,  1270.654,  1275.56 ,  1280.466,  1285.372,  1290.278,\n",
       "        1295.184,  1300.09 ,  1304.996,  1309.902,  1314.808,  1319.714,\n",
       "        1324.62 ,  1329.526,  1334.432,  1339.338,  1344.244,  1349.15 ,\n",
       "        1354.056,  1358.962,  1363.868,  1368.774,  1373.68 ,  1378.586,\n",
       "        1383.492,  1388.398,  1393.304,  1398.21 ,  1403.116,  1408.022,\n",
       "        1412.928,  1417.834,  1422.74 ,  1427.646,  1432.552,  1437.458,\n",
       "        1442.364,  1447.27 ,  1452.176,  1457.082,  1461.988,  1466.894,\n",
       "        1471.8  ,  1476.706,  1481.612,  1486.518,  1491.424,  1496.33 ,\n",
       "        1501.236,  1506.142,  1511.048,  1515.954,  1520.86 ,  1525.766,\n",
       "        1530.672,  1535.578,  1540.484,  1545.39 ,  1550.296,  1555.202,\n",
       "        1560.108,  1565.014,  1569.92 ,  1574.826,  1579.732,  1584.638,\n",
       "        1589.544,  1594.45 ,  1599.356,  1604.262,  1609.168,  1614.074,\n",
       "        1618.98 ,  1623.886,  1628.792,  1633.698,  1638.604,  1643.51 ,\n",
       "        1648.416,  1653.322,  1658.228,  1663.134,  1668.04 ,  1672.946,\n",
       "        1677.852,  1682.758,  1687.664,  1692.57 ,  1697.476,  1702.382,\n",
       "        1707.288,  1712.194,  1717.1  ,  1722.006,  1726.912,  1731.818,\n",
       "        1736.724,  1741.63 ,  1746.536,  1751.442,  1756.348,  1761.254,\n",
       "        1766.16 ,  1771.066,  1775.972,  1780.878,  1785.784,  1790.69 ,\n",
       "        1795.596,  1800.502,  1805.408,  1810.314,  1815.22 ,  1820.126,\n",
       "        1825.032,  1829.938,  1834.844,  1839.75 ,  1844.656,  1849.562,\n",
       "        1854.468,  1859.374,  1864.28 ,  1869.186,  1874.092,  1878.998,\n",
       "        1883.904,  1888.81 ,  1893.716,  1898.622,  1903.528,  1908.434,\n",
       "        1913.34 ,  1918.246,  1923.152,  1928.058,  1932.964,  1937.87 ,\n",
       "        1942.776,  1947.682,  1952.588,  1957.494,  1962.4  ,  1967.306,\n",
       "        1972.212,  1977.118,  1982.024,  1986.93 ,  1991.836,  1996.742,\n",
       "        2001.648,  2006.554,  2011.46 ,  2016.366,  2021.272,  2026.178,\n",
       "        2031.084,  2035.99 ,  2040.896,  2045.802,  2050.708,  2055.614,\n",
       "        2060.52 ,  2065.426,  2070.332,  2075.238,  2080.144,  2085.05 ,\n",
       "        2089.956,  2094.862,  2099.768,  2104.674,  2109.58 ,  2114.486,\n",
       "        2119.392,  2124.298,  2129.204,  2134.11 ,  2139.016,  2143.922,\n",
       "        2148.828,  2153.734,  2158.64 ,  2163.546,  2168.452,  2173.358,\n",
       "        2178.264,  2183.17 ,  2188.076,  2192.982,  2197.888,  2202.794,\n",
       "        2207.7  ,  2212.606,  2217.512,  2222.418,  2227.324,  2232.23 ,\n",
       "        2237.136,  2242.042,  2246.948,  2251.854,  2256.76 ,  2261.666,\n",
       "        2266.572,  2271.478,  2276.384,  2281.29 ,  2286.196,  2291.102,\n",
       "        2296.008,  2300.914,  2305.82 ,  2310.726,  2315.632,  2320.538,\n",
       "        2325.444,  2330.35 ,  2335.256,  2340.162,  2345.068,  2349.974,\n",
       "        2354.88 ,  2359.786,  2364.692,  2369.598,  2374.504,  2379.41 ,\n",
       "        2384.316,  2389.222,  2394.128,  2399.034,  2403.94 ,  2408.846,\n",
       "        2413.752,  2418.658,  2423.564,  2428.47 ,  2433.376,  2438.282,\n",
       "        2443.188,  2448.094,  2453.   ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  9.812\n",
      "Count of most probable lenght =  1344.0\n",
      "Min length =  4.906\n"
     ]
    }
   ],
   "source": [
    "max_sent_len =  h[1][np.argmax(h[0])]\n",
    "min_sent_len = h[1][1]\n",
    "print('Most probable length = ', max_sent_len)\n",
    "print('Count of most probable lenght = ', np.max(h[0]))\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sent_len =  100#int(np.ceil(max_sent_len))\n",
    "min_sent_len = 4#int(np.floor(min_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  100\n",
      "Min length =  4\n"
     ]
    }
   ],
   "source": [
    "print('Most probable length = ', max_sent_len)\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4313"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "threshold = 0.9\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = target_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14313\n",
      "Clim Type: VB Accident - Accidentl Injury \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "\n",
      "Who vThe Reported Event Hapened o:EmployeeP/olicyhcolder \n",
      " \tWho The Reported Event Happened To: Employee/Policyholder\n",
      "\n",
      "\n",
      "Policyolder/Owner Information \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "\n",
      "Fisrt Name:r \n",
      " \tFirst Name:\n",
      "\n",
      "\n",
      "Middle NameI/nitial: \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "\n",
      "Last Naem: \n",
      " \tLast Name:\n",
      "\n",
      "\n",
      "Social Securit Nmber: \n",
      " \tSocial Security Number:\n",
      "\n",
      "\n",
      "Brirth Date: \n",
      " \tBirth Date:\n",
      "\n",
      "\n",
      "Gender:t \n",
      " \tGender:\n",
      "\n",
      "\n",
      "Lanugage Preferece: \n",
      " \tLanguage Preference:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)\n",
    "np.savez('vocab', vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 14313\n",
      "Number of unique input tokens: 121\n",
      "Number of unique output tokens: 121\n",
      "Max sequence length for inputs: 99\n",
      "Max sequence length for outputs: 99\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 200  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 121)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None, 121)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 256), (None, 387072      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, None, 256),  387072      input_8[0][0]                    \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 121)    31097       lstm_8[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 805,241\n",
      "Trainable params: 805,241\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# TODO: Add Embedding for chars\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "#encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "#encoder = Bidirectional(GRU(latent_dim, return_state=True))\n",
    "#encoder = GRU(latent_dim, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "#encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs)\n",
    "#encoder_outputs, state_f, state_b = encoder(encoder_inputs)\n",
    "\n",
    "#state = Concatenate()([state_f, state_b])\n",
    "#state_h = Concatenate()([state_f_h, state_b_h])\n",
    "#state_c = Concatenate()([state_f_c, state_b_c])\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "#encoder_states = [state]\n",
    "#encoder_states = [state_f_h, state_f_c, state_b_h, state_b_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "#decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "#decoder_lstm = GRU(latent_dim*2, return_sequences=True, return_state=True)\n",
    "'''\n",
    "decoder_outputs, _, _ = Bidirectional(decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states))\n",
    "'''\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "\n",
    "#decoder_outputs, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12166 samples, validate on 2147 samples\n",
      "Epoch 1/200\n",
      "12160/12166 [============================>.] - ETA: 0s - loss: 0.7765 - categorical_accuracy: 0.0986Epoch 00001: val_categorical_accuracy improved from -inf to 0.14773, saving model to best_model.hdf5\n",
      "12166/12166 [==============================] - 59s 5ms/step - loss: 0.7763 - categorical_accuracy: 0.0986 - val_loss: 0.5591 - val_categorical_accuracy: 0.1477\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:2344: UserWarning: Layer lstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_7/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_7/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3328/12166 [=======>......................] - ETA: 40s - loss: 0.5269 - categorical_accuracy: 0.1600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-0951ae1c1be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0;31m#validation_split=0.2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "#decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "#decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "#decoder_state_input = Input(shape=(latent_dim*2,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "#decoder_states_inputs = [decoder_state_input]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "#decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "#decoder_states = [state]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py:2344: UserWarning: Layer lstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_9:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_10:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('encoder_model.hdf5')\n",
    "decoder_model.save('decoder_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: DOcB\n",
      "\n",
      "GT sentence: DOB:\n",
      "\n",
      "Decoded sentence: PATE NT SERERE CORTENT PATIENT PATE NT STOREST PATE NT STOREST PATE NT STOREST PATE NT STOREST PATE \n",
      "-\n",
      "Input sentence: 2. Noawemamliy harmed\n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "\n",
      "Decoded sentence: Employee Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Physicin Name (as Name, First Nmae,MI, Sufifx)Please Prin\n",
      "GT sentence: Physician Name (Last Name, First Name, MI, Suffix) Please Print\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Postal Coe:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Total mployee Weekly Payroll Dedutcion:\n",
      "GT sentence: Total Employee Weekly Payroll Deduction:\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: ICD-10 DiagnosisCode:S62.616A DScPL FX PROX PHAL RxT LF\n",
      "GT sentence: ICD-10 Diagnosis Code: S62.616A DSPL FX PROX PHAL RT LF\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: ATETNnDIGN PHjYSICAN TSATEMENT (PLEASE PRINT)\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE PRINT)\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Forwarding Service Requested\n",
      "GT sentence: Forwarding Service Requested\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: 87891 ?\n",
      "GT sentence: Z87891 ?\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: sapnol: aPra dobtener aissencia en Espanol llae al 1-800-305-0849\n",
      "GT sentence: Espanol: Para obtener asistencia en Espanol, llame al 1-800-305-0849\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: Insuredn overag Type Coverage Effectiv eDatea\n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Not Covtered mAount\n",
      "GT sentence: Not Covered Amount\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Work Shcedule ShouldBe\n",
      "GT sentence: Work Schedule Should Be:\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Next Scheduled Appoitnmetnt Date? cShudled iWth?\n",
      "GT sentence: Next Scheduled Appointment Date? Scheduled With?\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Sdystm Ne/Pos eDtails\n",
      "GT sentence: System Neg/Pos Details\n",
      "\n",
      "Decoded sentence: Catrent Name:\n",
      "\n",
      "-\n",
      "Input sentence: SKIN:Warm adn dry; good uro;r npo lesions, rashes uonr ulcers.\n",
      "GT sentence: SKIN: Warm and dry; good turgor; no lesions, rashes or ulcers.\n",
      "\n",
      "Decoded sentence: PATE NT SERERE CORTENT PATIENT PATE NT STOREST PATE NT STOREST PATE NT STOREST PATE NT STOREST PATE \n",
      "-\n",
      "Input sentence: 'OTRgiilgfEigs\n",
      "GT sentence: TWIN CITIES ORTHOPEDICS\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: STTE:\n",
      "GT sentence: STATE:\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Caffeine use: 0 drinks per day\n",
      "GT sentence: Caffeine use: 0 drinks per day\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: vent ldates: unknown brtw\n",
      "GT sentence: Event dates: unknown rtw\n",
      "\n",
      "Decoded sentence: Country:\n",
      "\n",
      "-\n",
      "Input sentence: Objecive:\n",
      "GT sentence: Objective:\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Attending Porvider\n",
      "GT sentence: Attending Provider\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: jInpatiednt/Outptaient Indicatoqr: Outpatienpt\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Electroic Submissio\n",
      "\n",
      "GT sentence: Electronic Submission\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: No labsw ereo rdewred.\n",
      "GT sentence: No labs were ordered.\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Lymphatic -. No lymphadenopathy in affected area.\n",
      "GT sentence: Lymphatic -. No lymphadenopathy in affected area.\n",
      "\n",
      "Decoded sentence: Cate Dischork of Sectide the to the patient to the and belate stress and by please prone and to the \n",
      "-\n",
      "Input sentence: Hospital Name: Minsot aValle Surger yCenter\n",
      "GT sentence: Hospital Name: Minnesota Valley Surgery Center\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Lanaue Preoference\n",
      "\n",
      "GT sentence: Language Preference:\n",
      "\n",
      "Decoded sentence: Cate Discharge: No at Spouted to the patient to the and belate stress and by please prone and to the\n",
      "-\n",
      "Input sentence: Pratciipant ID:\n",
      "GT sentence: Participant ID:\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Hspital Name: Milnnestoa Vawlley Surgey Center\n",
      "GT sentence: Hospital Name: Minnesota Valley Surgery Center\n",
      "\n",
      "Decoded sentence: Current Name:\n",
      "\n",
      "-\n",
      "Input sentence: Occsiona 11-3t3\n",
      "\n",
      "GT sentence: Occasional 11-33%\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "\n",
      "Decoded sentence: Employee Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Gastornitestina:l no gastroitnesina sympotms.\n",
      "GT sentence: Gastrointestinal: no gastrointestinal symptoms.\n",
      "\n",
      "Decoded sentence: Cate Dischork of Sectide the to the patient to the and belate stress and by please prone and to the \n",
      "-\n",
      "Input sentence: Adress\n",
      "GT sentence: Address\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Country.\n",
      "GT sentence: Country:\n",
      "\n",
      "Decoded sentence: Confirmation - Health (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: epartment of ervice:\n",
      "GT sentence: department of service:\n",
      "\n",
      "Decoded sentence: Country:\n",
      "\n",
      "-\n",
      "Input sentence: Active: Reviewed Allergies; Penicillins - Rash\n",
      "GT sentence: Active: Reviewed Allergies; Penicillins - Rash\n",
      "\n",
      "Decoded sentence: Employee Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: I sSxurgery Reuirefd: No\n",
      "GT sentence: Is Surgery Required: No\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Customehr  Policy #:\n",
      "GT sentence: Customer  Policy #:\n",
      "\n",
      "Decoded sentence: Claim Date:\n",
      "\n",
      "-\n",
      "Input sentence: Phone\n",
      "GT sentence: Phone\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Exam Date:\n",
      "GT sentence: Exam Date:\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Offic ConsNew/estab Mtoderate Sver\n",
      "GT sentence: Offic Cons New/estab Moderate Sever\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: PERFORM INFORMATIO: Caudell, Susannah, RN\n",
      "GT sentence: PERFORM INFORMATION: Caudell, Susannah, RN\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Ifyee date of accident (mmlddiyy) EllI\n",
      "GT sentence: If yes, date of accident (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: r(Not for MF Reqsets)\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: EDGEonD\n",
      "GT sentence: EDGEWOOD\n",
      "\n",
      "Decoded sentence: Employee Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: FILED PRIeMARY TO FIRST CHOICE HALTH AfDMIN (FI005c)\n",
      "GT sentence: FILED PRIMARY TO FIRST CHOICE HEALTH ADMIN (FI005)\n",
      "\n",
      "Decoded sentence: Procedure Provider Information\n",
      "\n",
      "-\n",
      "Input sentence: TWIN CITIES\n",
      "GT sentence: TWIN CITIES\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: New tear of anterior cruciate ligament of right knee\n",
      "GT sentence: New tear of anterior cruciate ligament of right knee\n",
      "\n",
      "Decoded sentence: Procedure Provider Information\n",
      "\n",
      "-\n",
      "Input sentence: Medical Provider Specialty. Orthopedic Surgeon\n",
      "GT sentence: Medical Provider Specialty: Orthopedic Surgeon\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Office Visit\n",
      "GT sentence: Office Visit\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Created By: Hughes, Bittany\n",
      "GT sentence: Created By: Hughes, Brittany\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: In.s Pmt.\n",
      "GT sentence: Ins. Pmt.\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: TOTAL PATNT BALANCE:\n",
      "GT sentence: TOTAL PATIENT BALANCE:\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Datoe\n",
      "GT sentence: Date\n",
      "\n",
      "Decoded sentence: Cate Dischork of Sectide the to the patient to the and belate stress and by please prone and to the \n",
      "-\n",
      "Input sentence: Claim Event Information\n",
      "GT sentence: Claim Event Information\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: (mm/dd/yy) (mm/dd/yoy) (mm/dd/yy)\n",
      "GT sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Cumber Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Daio ofrst visiti'or this current conditionis)\n",
      "GT sentence: Date of first visit for this current condition(s) (mm/dd/yy):\n",
      "\n",
      "Decoded sentence: Cate Dischork of Sectide the to the patient to the and belate stress and by please prone and to the \n",
      "-\n",
      "Input sentence: TIR v2 Family MOOP Max\n",
      "GT sentence: TIER 2 Family MOOP Max\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: Socital iHstory\n",
      "GT sentence: Social History\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Ins: Makinegcare\n",
      "GT sentence: Ins: Mainecare\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Simple Grasp\n",
      "GT sentence: Simple Grasp\n",
      "\n",
      "Decoded sentence: Country:\n",
      "\n",
      "-\n",
      "Input sentence: Author: Souha Hakim, MD\n",
      "GT sentence: Author: Souha Hakim, MD\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: unum\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: Cumber Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Language Preference:\n",
      "GT sentence: Language Preference:\n",
      "\n",
      "Decoded sentence: Catient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Emplloyer Ndamej:\n",
      "GT sentence: Employer Name:\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Degree\n",
      "GT sentence: Degree\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: NON DISCRIMNATIN ONTIC & LANGhUAGESSISATNCE SERVICES\n",
      "GT sentence: NON DISCRIMINATION NOTICE & LANGUAGE ASSISTANCE SERVICES\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: TWIN iCITIES ORTHOPEDICS\n",
      "GT sentence: TWIN CITIES ORTHOPEDICS\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: AMOUNT\n",
      "GT sentence: AMOUNT\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Nevr 0%\n",
      "GT sentence: Never 0%\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: PsERFROM INFOcRMATION: Caudell, Susannah, vRN\n",
      "GT sentence: PERFORM INFORMATION: Caudell, Susannah, RN\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: BLILh Batu:\n",
      "GT sentence: Birth Date:\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: ACCIDENT CLIAM FORM\n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: Address Line 1:o\n",
      "GT sentence: Address Line 1:\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Pinco rGip\n",
      "GT sentence: Pinch Grip\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Last Payment Date: 01/24/2018 cGuaantor amyent Made Sicne Last Statment\n",
      "GT sentence: Last Payment Date: 01/24/2018 Guarantor Payments Made Since Last Statement:\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: Right:\n",
      "GT sentence: Right:\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Accounnt Summary\n",
      "GT sentence: Account Summary\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: (cusotmer copy)\n",
      "GT sentence: (customer copy)\n",
      "\n",
      "Decoded sentence: Current Name:\n",
      "\n",
      "-\n",
      "Input sentence: AccidentD ate\n",
      "GT sentence: Accident Date:\n",
      "\n",
      "Decoded sentence: Accident Information\n",
      "\n",
      "-\n",
      "Input sentence: Note: Ftinal rcost may vary sightgly due tor ounding ifference.\n",
      "GT sentence: Note: Final cost may vary slightly due to rounding differences.\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: Wednesday: Nu\n",
      "GT sentence: Wednesday: No\n",
      "\n",
      "Decoded sentence: City:\n",
      "\n",
      "-\n",
      "Input sentence: UNUM ILFE INUSANCE COMPANbYb OF AMERICA 2211 ONGRESS ST PORTLAND ME 04212-0002\n",
      "GT sentence: UNUM LIFE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Gruop lPoliyc #:\n",
      "GT sentence: Group Policy #:\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence:  N ohsignificat past medcial histaory\n",
      "GT sentence:  No significant past medical history\n",
      "\n",
      "Decoded sentence: Current Name:\n",
      "\n",
      "-\n",
      "Input sentence: Average hours per hweek - 40\n",
      "GT sentence: Average hours per week - 40\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: State:\n",
      "GT sentence: State:\n",
      "\n",
      "Decoded sentence: Cate Dischork of Sectide the to the patient to the and belate stress and by please prone and to the \n",
      "-\n",
      "Input sentence: mploymet Infmormation\n",
      "GT sentence: Employment Information\n",
      "\n",
      "Decoded sentence: Claim Date:\n",
      "\n",
      "-\n",
      "Input sentence: c Account ii:\n",
      "GT sentence: Account #:\n",
      "\n",
      "Decoded sentence: Action Date:\n",
      "\n",
      "-\n",
      "Input sentence: Postal Code\n",
      "\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: UNUM\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Page 4\n",
      "GT sentence: Page 4\n",
      "\n",
      "Decoded sentence: Catient Name:\n",
      "\n",
      "-\n",
      "Input sentence: C mmtry',\n",
      "GT sentence: Country:\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: B. Complete this secion for disability dclaims ony.\n",
      "GT sentence: B. Complete this section for disability claims only.\n",
      "\n",
      "Decoded sentence: Employee Poller Information\n",
      "\n",
      "-\n",
      "Input sentence: Provider Las tName: Francis\n",
      "GT sentence: Provider Last Name: Francis\n",
      "\n",
      "Decoded sentence: Provider Signed Information\n",
      "\n",
      "-\n",
      "Input sentence: This uis the totalamount billed fromt hwe dates of service of 012/4/201m8 thru 01/24/2018.\n",
      "GT sentence: This is the total amount billed from the dates of service of 01/24/2018 thru 01/24/2018.\n",
      "\n",
      "Decoded sentence: Procedure Provider Information\n",
      "\n",
      "-\n",
      "Input sentence: www.unum.mrn\n",
      "GT sentence: www.unum.com\n",
      "\n",
      "Decoded sentence: Current Name:\n",
      "\n",
      "-\n",
      "Input sentence: Reach Forwvard in inches\n",
      "GT sentence: Reach Forward in inches\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on separate tesseract corrected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain transfer from noisy spelling mistakes to OCR corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# TODO: Add Embedding for chars\n",
    "encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "#encoder = Bidirectional(GRU(latent_dim, return_state=True))\n",
    "#encoder = GRU(latent_dim, return_state=True)\n",
    "\n",
    "#encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs)\n",
    "#encoder_outputs, state_f, state_b = encoder(encoder_inputs)\n",
    "\n",
    "#state = Concatenate()([state_f, state_b])\n",
    "state_h = Concatenate()([state_f_h, state_b_h])\n",
    "state_c = Concatenate()([state_f_c, state_b_c])\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "#encoder_states = [state]\n",
    "#encoder_states = [state_f_h, state_f_c, state_b_h, state_b_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "#decoder_lstm = GRU(latent_dim*2, return_sequences=True, return_state=True)\n",
    "'''\n",
    "decoder_outputs, _, _ = Bidirectional(decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states))\n",
    "'''\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "\n",
    "#decoder_outputs, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on noisy spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on OCR correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "# Keep test data from the corrected OCR, as this what we care about\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# Vectorize train data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "# Vectorize test data\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.001# Reduce the learning rate for fine tuning\n",
    "model.load_weights('best_model.hdf5')\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "#decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "#decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "#decoder_state_input = Input(shape=(latent_dim*2,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "#decoder_states_inputs = [decoder_state_input]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "#decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "#decoder_states = [state]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
