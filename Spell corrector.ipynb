{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch (TODO)\n",
    "- Learning rate schedule (TODO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name):\n",
    "        if cnt < num_samples :\n",
    "            sents = row.split(\"\\t\")\n",
    "            input_text = sents[0]\n",
    "            \n",
    "            target_text = '\\t' + sents[1] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[1])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name):\n",
    "        if cnt < num_samples :\n",
    "            input_text = noise_maker(row, noise_threshold)\n",
    "            input_text = input_text[:-1]\n",
    "            \n",
    "            target_text = '\\t' + row + '\\n'            \n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(row)\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0\n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "    # Add special tokens to vocab_to_int\n",
    "    codes = ['\\t','\\n']\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t, vocab_to_int[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, vocab_to_int[char]] = 1.\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, vocab_to_int['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 40\n",
    "min_sent_len = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "threshold = 0.9\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = input_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902\n",
      "City. W’aukesa j\tCty :Wuakesha \n",
      " City. W’aukesa j\tCty :Wuakesha\n",
      "Cuntry\". jUS\tCountyr:  US \n",
      " Cuntry\". jUS\tCountyr:  US\n",
      "City. ‘Waukewsh \tCity: Waukescha \n",
      " City. ‘Waukewsh \tCity: Waukescha\n",
      "oCuntr. US \tCountry:US \n",
      " oCuntr. US \tCountry:US\n",
      "First Name:\tFirst Name: \n",
      " First Name:\tFirst Name:\n",
      "Last Name: \tLast Name: \n",
      " Last Name: \tLast Name:\n",
      "Birth Date: \tBirht Date: \n",
      " Birth Date: \tBirht Date:\n",
      "Gelnderk: t\tGenedr:q \n",
      " Gelnderk: t\tGenedr:q\n",
      "Addreuss Liwne 1: \tAddress Line 1: \n",
      " Addreuss Liwne 1: \tAddress Line 1:\n",
      "CW- \tCity: \n",
      " CW- \tCity:\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1902\n",
      "Number of unique input tokens: 105\n",
      "Number of unique output tokens: 105\n",
      "Max sequence length for inputs: 39\n",
      "Max sequence length for outputs: 39\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 105)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 105)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 370688      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 256),  370688      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 105)    26985       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 768,361\n",
      "Trainable params: 768,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# TODO: Add Embedding for chars\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1616 samples, validate on 286 samples\n",
      "Epoch 1/30\n",
      "1616/1616 [==============================] - 15s 9ms/step - loss: 2.0712 - categorical_accuracy: 0.0595 - val_loss: 1.9406 - val_categorical_accuracy: 0.0777\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.07773, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 1.8040 - categorical_accuracy: 0.0934 - val_loss: 1.6312 - val_categorical_accuracy: 0.1295\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.07773 to 0.12946, saving model to best_model.hdf5\n",
      "Epoch 3/30\n",
      "1616/1616 [==============================] - 13s 8ms/step - loss: 1.5097 - categorical_accuracy: 0.1464 - val_loss: 1.4427 - val_categorical_accuracy: 0.1614\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.12946 to 0.16138, saving model to best_model.hdf5\n",
      "Epoch 4/30\n",
      "1616/1616 [==============================] - 13s 8ms/step - loss: 1.3565 - categorical_accuracy: 0.1812 - val_loss: 1.3541 - val_categorical_accuracy: 0.1924\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.16138 to 0.19240, saving model to best_model.hdf5\n",
      "Epoch 5/30\n",
      "1616/1616 [==============================] - 13s 8ms/step - loss: 1.2496 - categorical_accuracy: 0.2170 - val_loss: 1.2572 - val_categorical_accuracy: 0.2266\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.19240 to 0.22665, saving model to best_model.hdf5\n",
      "Epoch 6/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 1.1267 - categorical_accuracy: 0.2505 - val_loss: 1.1782 - val_categorical_accuracy: 0.2441\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.22665 to 0.24413, saving model to best_model.hdf5\n",
      "Epoch 7/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 1.0140 - categorical_accuracy: 0.2820 - val_loss: 1.1100 - val_categorical_accuracy: 0.2647\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.24413 to 0.26475, saving model to best_model.hdf5\n",
      "Epoch 8/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.9115 - categorical_accuracy: 0.3103 - val_loss: 1.0495 - val_categorical_accuracy: 0.2874\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.26475 to 0.28743, saving model to best_model.hdf5\n",
      "Epoch 9/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.8278 - categorical_accuracy: 0.3315 - val_loss: 1.0372 - val_categorical_accuracy: 0.2944\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.28743 to 0.29442, saving model to best_model.hdf5\n",
      "Epoch 10/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.7588 - categorical_accuracy: 0.3492 - val_loss: 0.9797 - val_categorical_accuracy: 0.3101\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.29442 to 0.31011, saving model to best_model.hdf5\n",
      "Epoch 11/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.6782 - categorical_accuracy: 0.3767 - val_loss: 0.9681 - val_categorical_accuracy: 0.3159\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.31011 to 0.31594, saving model to best_model.hdf5\n",
      "Epoch 12/30\n",
      "1616/1616 [==============================] - 12s 8ms/step - loss: 0.6172 - categorical_accuracy: 0.3890 - val_loss: 0.9453 - val_categorical_accuracy: 0.3238\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.31594 to 0.32383, saving model to best_model.hdf5\n",
      "Epoch 13/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.5661 - categorical_accuracy: 0.4008 - val_loss: 0.9298 - val_categorical_accuracy: 0.3308\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.32383 to 0.33082, saving model to best_model.hdf5\n",
      "Epoch 14/30\n",
      "1616/1616 [==============================] - 15s 9ms/step - loss: 0.5168 - categorical_accuracy: 0.4158 - val_loss: 0.9291 - val_categorical_accuracy: 0.3332\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.33082 to 0.33315, saving model to best_model.hdf5\n",
      "Epoch 15/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.4749 - categorical_accuracy: 0.4219 - val_loss: 0.9338 - val_categorical_accuracy: 0.3372\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.33315 to 0.33719, saving model to best_model.hdf5\n",
      "Epoch 16/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.4408 - categorical_accuracy: 0.4326 - val_loss: 0.9385 - val_categorical_accuracy: 0.3393\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.33719 to 0.33934, saving model to best_model.hdf5\n",
      "Epoch 17/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.3942 - categorical_accuracy: 0.4458 - val_loss: 0.9294 - val_categorical_accuracy: 0.3423\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.33934 to 0.34230, saving model to best_model.hdf5\n",
      "Epoch 18/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.3651 - categorical_accuracy: 0.4535 - val_loss: 0.9431 - val_categorical_accuracy: 0.3427\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.34230 to 0.34275, saving model to best_model.hdf5\n",
      "Epoch 19/30\n",
      "1616/1616 [==============================] - 12s 8ms/step - loss: 0.3527 - categorical_accuracy: 0.4556 - val_loss: 0.9401 - val_categorical_accuracy: 0.3449\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy improved from 0.34275 to 0.34490, saving model to best_model.hdf5\n",
      "Epoch 20/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.3234 - categorical_accuracy: 0.4656 - val_loss: 0.9369 - val_categorical_accuracy: 0.3494\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.34490 to 0.34938, saving model to best_model.hdf5\n",
      "Epoch 21/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.3005 - categorical_accuracy: 0.4691 - val_loss: 0.9384 - val_categorical_accuracy: 0.3510\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy improved from 0.34938 to 0.35100, saving model to best_model.hdf5\n",
      "Epoch 22/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.2766 - categorical_accuracy: 0.4750 - val_loss: 0.9559 - val_categorical_accuracy: 0.3477\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.35100\n",
      "Epoch 23/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.2554 - categorical_accuracy: 0.4805 - val_loss: 0.9575 - val_categorical_accuracy: 0.3563\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy improved from 0.35100 to 0.35628, saving model to best_model.hdf5\n",
      "Epoch 24/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.2328 - categorical_accuracy: 0.4865 - val_loss: 0.9635 - val_categorical_accuracy: 0.3582\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy improved from 0.35628 to 0.35817, saving model to best_model.hdf5\n",
      "Epoch 25/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.2221 - categorical_accuracy: 0.4905 - val_loss: 0.9760 - val_categorical_accuracy: 0.3568\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.35817\n",
      "Epoch 26/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.2115 - categorical_accuracy: 0.4928 - val_loss: 0.9846 - val_categorical_accuracy: 0.3563\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.35817\n",
      "Epoch 27/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.2017 - categorical_accuracy: 0.4950 - val_loss: 1.0115 - val_categorical_accuracy: 0.3531\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.35817\n",
      "Epoch 28/30\n",
      "1616/1616 [==============================] - 11s 7ms/step - loss: 0.1978 - categorical_accuracy: 0.4948 - val_loss: 1.0023 - val_categorical_accuracy: 0.3512\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.35817\n",
      "Epoch 29/30\n",
      "1616/1616 [==============================] - 14s 9ms/step - loss: 0.1976 - categorical_accuracy: 0.4945 - val_loss: 1.0250 - val_categorical_accuracy: 0.3558\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.35817\n",
      "Epoch 30/30\n",
      "1616/1616 [==============================] - 12s 7ms/step - loss: 0.2001 - categorical_accuracy: 0.4944 - val_loss: 1.0373 - val_categorical_accuracy: 0.3535\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.35817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cd82b8668>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: ZIP: \n",
      "GT sentence: ZIP:\n",
      "\n",
      "Decoded sentence: Middle Name/Initial:\n",
      "\n",
      "-\n",
      "Input sentence: ounr:y US \tCoubntry: US\n",
      "GT sentence: unr:y US \tCoubntry: U\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: TOTAL DUE\n",
      "GT sentence: TOTAL DUE\n",
      "\n",
      "Decoded sentence: TOTAL DUE : 345.15\n",
      "\n",
      "-\n",
      "Input sentence: Narrative \n",
      "GT sentence: Narrative\n",
      "\n",
      "Decoded sentence: Transaction reference:\n",
      "\n",
      "-\n",
      "Input sentence: Claim No: ' _\" \n",
      "GT sentence: Claim No:\n",
      "\n",
      "Decoded sentence: Claim Event Information\n",
      "\n",
      "-\n",
      "Input sentence: Account Number \n",
      "GT sentence: Account Number:\n",
      "\n",
      "Decoded sentence: Accident Date: 03/09/2018\n",
      "\n",
      "-\n",
      "Input sentence: ACTIVE: albuterol sulfate \n",
      "GT sentence: ACTIVE: albuterol sulfate\n",
      "\n",
      "Decoded sentence: ACCIDENT DETAILS\n",
      "\n",
      "-\n",
      "Input sentence: Enznlngs Typu: Kuuxly\n",
      "GT sentence: Earnings Type: Hourly\n",
      "\n",
      "Decoded sentence: Earnings Type: Hourly\n",
      "\n",
      "-\n",
      "Input sentence: Employer: \tmployer:\n",
      "GT sentence: mployer: \tmployer\n",
      "Decoded sentence: Employee ID:\n",
      "\n",
      "-\n",
      "Input sentence: Country\". US \n",
      "GT sentence: Country: US\n",
      "\n",
      "Decoded sentence: Country: US\n",
      "\n",
      "-\n",
      "Input sentence: C lairn Event Identiﬁer: \n",
      "GT sentence: Claim Event Identifier:\n",
      "\n",
      "Decoded sentence: Claim Event Information\n",
      "\n",
      "-\n",
      "Input sentence: The Benefits Center \n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: The Benefits Center\n",
      "\n",
      "-\n",
      "Input sentence: Accident Date: 03/099018 \n",
      "GT sentence: Accident Date: 03/09/2018\n",
      "\n",
      "Decoded sentence: Accident Date: 03/09/2018\n",
      "\n",
      "-\n",
      "Input sentence: Policy No.: \n",
      "GT sentence: Policy No.:\n",
      "\n",
      "Decoded sentence: Policyholder/Owner Information\n",
      "\n",
      "-\n",
      "Input sentence: Signatues \tSigatures\n",
      "GT sentence: ignatues \tSigature\n",
      "Decoded sentence: St. Elizabeth\n",
      "\n",
      "-\n",
      "Input sentence: CL-1116 (11/14) \tCbL-1116 1(1/14)c\n",
      "GT sentence: L-1116 (11/14) \tCbL-1116 1(1/14)\n",
      "Decoded sentence: CL-1023 (06/13)\n",
      "\n",
      "-\n",
      "Input sentence: Ciotmtry. US\n",
      "GT sentence: Country: US\n",
      "\n",
      "Decoded sentence: Confirmation of Coverage\n",
      "\n",
      "-\n",
      "Input sentence: Claim Event Information \n",
      "GT sentence: Claim Event Information\n",
      "\n",
      "Decoded sentence: Claim Event Information\n",
      "\n",
      "-\n",
      "Input sentence: Organization/Facility — \n",
      "GT sentence: Organization/Facility -\n",
      "\n",
      "Decoded sentence: Primary Phone:\n",
      "\n",
      "-\n",
      "Input sentence: Accident Date: 02/170018 \n",
      "GT sentence: Accident Date: 02/17/2018\n",
      "\n",
      "Decoded sentence: Accident Date: 03/09/2018\n",
      "\n",
      "-\n",
      "Input sentence: unum\" \n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: BLILh Batu:f\tBirth Daet:\n",
      "GT sentence: LILh Batu:f\tBirth Daet\n",
      "Decoded sentence: Business Telephone:\n",
      "\n",
      "-\n",
      "Input sentence: yAnddress Lrine 1: \tAddress Linef 1:\n",
      "GT sentence: Anddress Lrine 1: \tAddress Linef 1\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Last Name: \n",
      "GT sentence: Last Name:\n",
      "\n",
      "Decoded sentence: Last Name:\n",
      "\n",
      "-\n",
      "Input sentence: SSN xxxexxnvmocx \tSSN xxx-xx-xxx\n",
      "\n",
      "GT sentence: SN xxxexxnvmocx \tSSN xxx-xx-xxx\n",
      "Decoded sentence: SSN xxx-xx-xxx\n",
      "\n",
      "-\n",
      "Input sentence: Chief Complaint \n",
      "GT sentence: Chief Complaint\n",
      "\n",
      "Decoded sentence: Chief Complaint\n",
      "\n",
      "-\n",
      "Input sentence: PATIENT iii. PATIENT NAME \n",
      "GT sentence: PATIENT # PATIENT NAME \n",
      "\n",
      "Decoded sentence: ACCIDENT DETAILS\n",
      "\n",
      "-\n",
      "Input sentence: Reuslt Status:t \tResult Status:\n",
      "GT sentence: euslt Status:t \tResult Status\n",
      "Decoded sentence: Report Group: 26\n",
      "\n",
      "-\n",
      "Input sentence: Rtpozt Gxou: 26\tReort Group: 26\n",
      "GT sentence: tpozt Gxou: 26\tReort Group: 2\n",
      "Decoded sentence: Report Group: 26\n",
      "\n",
      "-\n",
      "Input sentence: ORTHOPEIDCS \tORTHOPEDICS\n",
      "GT sentence: RTHOPEIDCS \tORTHOPEDIC\n",
      "Decoded sentence: ORTHOATLANTA, L.L.C.\n",
      "\n",
      "-\n",
      "Input sentence: Electronic Submission \n",
      "GT sentence: Electronic Submission\n",
      "\n",
      "Decoded sentence: Electronic Submission\n",
      "\n",
      "-\n",
      "Input sentence: Bsirth Date: B\tirpth Date:\n",
      "GT sentence: sirth Date: B\tirpth Date\n",
      "Decoded sentence: Business Telephone:\n",
      "\n",
      "-\n",
      "Input sentence: SII l‘g Ei')’ Information \n",
      "GT sentence: Surgery Information\n",
      "\n",
      "Decoded sentence: Surgery Information\n",
      "\n",
      "-\n",
      "Input sentence: hFo ne \tPhone\n",
      "GT sentence: Fo ne \tPhon\n",
      "Decoded sentence: Primary Address:\n",
      "\n",
      "-\n",
      "Input sentence: Fatient Name \n",
      "GT sentence: Patient Name:\n",
      "\n",
      "Decoded sentence: Fax number\n",
      "\n",
      "-\n",
      "Input sentence: Adadress Line 1: \tAddrees Ljine 1:\n",
      "GT sentence: dadress Line 1: \tAddrees Ljine 1\n",
      "Decoded sentence: Admission Date: 03/12/2018\n",
      "\n",
      "-\n",
      "Input sentence: Work schedule updated : Yes \n",
      "GT sentence: Work schedule updated : Yes\n",
      "\n",
      "Decoded sentence: Work schedule ganergis - nkion\n",
      "\n",
      "-\n",
      "Input sentence: Job Title: General Production \n",
      "GT sentence: Job Title: General Production\n",
      "\n",
      "Decoded sentence: Diagnosis Code (ICD)\n",
      "\n",
      "-\n",
      "Input sentence: The Bonoﬂts Center\n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: The Benefits Center\n",
      "\n",
      "-\n",
      "Input sentence: bilr'nazn: Orthopedic \n",
      "GT sentence: Service: Orthopedic\n",
      "\n",
      "Decoded sentence: Record number: \n",
      "\n",
      "-\n",
      "Input sentence: DiVLsLun:\tDivisfion:\n",
      "GT sentence: iVLsLun:\tDivisfion\n",
      "Decoded sentence: Division:\n",
      "\n",
      "-\n",
      "Input sentence: DP Notes \tOp Notes\n",
      "GT sentence: P Notes \tOp Note\n",
      "Decoded sentence: OPERATIVE REPORT - PAGE 18o\n",
      "\n",
      "-\n",
      "Input sentence: Electonically Signed Indicator: Yes \n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "\n",
      "Decoded sentence: Electronic Submission\n",
      "\n",
      "-\n",
      "Input sentence: Data of Birth (mmiduiyy) \n",
      "GT sentence: Date of Birth (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Date of Birth (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: w The Beneﬁts Cantor \n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: The Benefits Center\n",
      "\n",
      "-\n",
      "Input sentence: Nae \tName\n",
      "GT sentence: ae \tNam\n",
      "Decoded sentence: • Family history of Cancer (C80.1)\n",
      "\n",
      "-\n",
      "Input sentence: ClaimcDeqtails q\tlaim nDetas\n",
      "GT sentence: laimcDeqtails q\tlaim nDeta\n",
      "Decoded sentence: Claim Event Information\n",
      "\n",
      "-\n",
      "Input sentence: Systolic: 134, LUE, Sitting \n",
      "GT sentence: Systolic: 134, LUE, Sitting\n",
      "\n",
      "Decoded sentence: State/Province: OH\n",
      "\n",
      "-\n",
      "Input sentence: Troutmant Dates: \n",
      "GT sentence: Treatment Dates:\n",
      "\n",
      "Decoded sentence: Transaction reference:\n",
      "\n",
      "-\n",
      "Input sentence: Address iLne 1: \tAdress iLne 1\n",
      "GT sentence: ddress iLne 1: \tAdress iLne \n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Last Name: \n",
      "GT sentence: Last Name:\n",
      "\n",
      "Decoded sentence: Last Name:\n",
      "\n",
      "-\n",
      "Input sentence: FNIDNIGS: \tFNIDINGS:\n",
      "GT sentence: NIDNIGS: \tFNIDINGS\n",
      "Decoded sentence: FAYETTEVILLE printed\n",
      "\n",
      "-\n",
      "Input sentence: 0 Age reporting \n",
      "GT sentence: • Age reporting\n",
      "\n",
      "Decoded sentence: • Age reporting\n",
      "\n",
      "-\n",
      "Input sentence: [customer copy) \t(csutomerj cocpy) \n",
      "GT sentence: customer copy) \t(csutomerj cocpy)\n",
      "Decoded sentence: Business Telephone:\n",
      "\n",
      "-\n",
      "Input sentence: UNUM”\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: Earnings Mode: ConthMe\n",
      "\n",
      "-\n",
      "Input sentence: Adm: 3.\"16I2CI‘18! DIG: 3a‘1 £62013 \n",
      "GT sentence: Adm: 3/16/18, D/C: 3/16/18\n",
      "\n",
      "Decoded sentence: Admission Date: 03/12/2018\n",
      "\n",
      "-\n",
      "Input sentence: Customer Policy #: \n",
      "GT sentence: Customer Policy #:\n",
      "\n",
      "Decoded sentence: Customer Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: Email Address: \n",
      "GT sentence: Email Address:\n",
      "\n",
      "Decoded sentence: Employee Off-Job Acc January 1, 2017\n",
      "\n",
      "-\n",
      "Input sentence: Gender: \n",
      "GT sentence: Gender:\n",
      "\n",
      "Decoded sentence: Gender:\n",
      "\n",
      "-\n",
      "Input sentence: Type: Medical \n",
      "GT sentence: Type: Medical\n",
      "\n",
      "Decoded sentence: Type: Medical Histora\n",
      "\n",
      "-\n",
      "Input sentence: 0 2SAT: 98% \thOx2m SAT: 98%\n",
      "GT sentence:  2SAT: 98% \thOx2m SAT: 98\n",
      "Decoded sentence: • Age reporting\n",
      "\n",
      "-\n",
      "Input sentence: SugarVJaor affmqa Email:\n",
      "GT sentence: Supervisor Office Email:\n",
      "\n",
      "Decoded sentence: Surgery Information\n",
      "\n",
      "-\n",
      "Input sentence: Address \tAddress\n",
      "GT sentence: ddress \tAddres\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Chief Complait \tChife Complaint\n",
      "GT sentence: hief Complait \tChife Complain\n",
      "Decoded sentence: Chief Complaint\n",
      "\n",
      "-\n",
      "Input sentence: 1. Knee injury 1. Knnee injury\n",
      "GT sentence: . Knee injury 1. Knnee injur\n",
      "Decoded sentence: 1. Knee injury\n",
      "\n",
      "-\n",
      "Input sentence: Leave Reason — \n",
      "GT sentence: Leave Reason -\n",
      "\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: aNrrative \tNarrtaive\n",
      "GT sentence: Nrrative \tNarrtaiv\n",
      "Decoded sentence: Tate of Birth (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: unum‘D \tnum\n",
      "GT sentence: num‘D \tnu\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: First Name: \n",
      "GT sentence: First Name:\n",
      "\n",
      "Decoded sentence: First Name:\n",
      "\n",
      "-\n",
      "Input sentence: PRIMARY INSUR: UMR FISERV WI \n",
      "GT sentence: PRIMARY INSUR: UMR FISERV WI\n",
      "\n",
      "Decoded sentence: PROIED BIOR \n",
      "\n",
      "-\n",
      "Input sentence: DATE \n",
      "GT sentence: DATE\n",
      "\n",
      "Decoded sentence: DATE\n",
      "\n",
      "-\n",
      "Input sentence: fAccountNumber \tAccount Number:\n",
      "GT sentence: AccountNumber \tAccount Number\n",
      "Decoded sentence: After Tax:\n",
      "\n",
      "-\n",
      "Input sentence: Printed Name \n",
      "GT sentence: Printed Name \n",
      "\n",
      "Decoded sentence: Primary Phone:\n",
      "\n",
      "-\n",
      "Input sentence: MRI KNEE LEFT W0 CONTRAST — Details \n",
      "GT sentence: MRI KNEE LEFT WO CONTRAST - Details\n",
      "\n",
      "Decoded sentence: MRI left kneeagiont\n",
      "\n",
      "-\n",
      "Input sentence: Accident Work Related: N o \n",
      "GT sentence: Accident Work Related: No\n",
      "\n",
      "Decoded sentence: Accident Date: 03/09/2018\n",
      "\n",
      "-\n",
      "Input sentence: City. \tCty:\n",
      "GT sentence: ity. \tCty\n",
      "Decoded sentence: City: Burnsville\n",
      "\n",
      "-\n",
      "Input sentence: Last Updated ByzKim, Daniel;Ordered; \n",
      "GT sentence: Last Updated By:Kim, Daniel;Ordered;\n",
      "\n",
      "Decoded sentence: Last Name:\n",
      "\n",
      "-\n",
      "Input sentence: Electronic. Survicr. Reqester \n",
      "GT sentence: Electronic Service Requested\n",
      "\n",
      "Decoded sentence: Electronic Submission\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Dependent Inform mion \n",
      "GT sentence: Dependent Information\n",
      "\n",
      "Decoded sentence: Dependent Information\n",
      "\n",
      "-\n",
      "Input sentence: . O . ACCIDENT CLAIM FORM \n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "Decoded sentence: ACCIDENT DETAILS\n",
      "\n",
      "-\n",
      "Input sentence: MFIN: DO:B \tMRN :DOB:\n",
      "GT sentence: FIN: DO:B \tMRN :DOB\n",
      "Decoded sentence: Daytime Phone:\n",
      "\n",
      "-\n",
      "Input sentence: Subscriber \n",
      "GT sentence: Subscriber\n",
      "\n",
      "Decoded sentence: Submission Date: 03/12/2018\n",
      "\n",
      "-\n",
      "Input sentence: RESP: 18 breathslmin \n",
      "GT sentence: RESP: 18 breaths/min\n",
      "\n",
      "Decoded sentence: RESP: 18 breaths/min\n",
      "\n",
      "-\n",
      "Input sentence: CL-1116 (11114) \n",
      "GT sentence: CL-1116 (11/14)\n",
      "\n",
      "Decoded sentence: CL-1023 (06/13)\n",
      "\n",
      "-\n",
      "Input sentence: Slate/PrOVirlce: \n",
      "GT sentence: State/Province:\n",
      "\n",
      "Decoded sentence: State/Province: OH\n",
      "\n",
      "-\n",
      "Input sentence: Chief Complaint \n",
      "GT sentence: Chief Complaint\n",
      "\n",
      "Decoded sentence: Chief Complaint\n",
      "\n",
      "-\n",
      "Input sentence: Country: \tCountyr:\n",
      "GT sentence: ountry: \tCountyr\n",
      "Decoded sentence: Country: US\n",
      "\n",
      "-\n",
      "Input sentence: Patient Na me: \n",
      "GT sentence: Patient Name:\n",
      "\n",
      "Decoded sentence: Patient DOB:\n",
      "\n",
      "-\n",
      "Input sentence: cad rl a 4-. cp'er :\tCP TCoe:\n",
      "GT sentence: ad rl a 4-. cp'er :\tCP TCoe\n",
      "Decoded sentence: Marital Status: Conmaling\n",
      "\n",
      "-\n",
      "Input sentence: Employer N arm: \n",
      "GT sentence: Employer Name:\n",
      "\n",
      "Decoded sentence: Employee ID:\n",
      "\n",
      "-\n",
      "Input sentence: Average hours per week — 40 \n",
      "GT sentence: Average hours per week - 40\n",
      "\n",
      "Decoded sentence: Admission Date: 03/12/2018\n",
      "\n",
      "-\n",
      "Input sentence: eGnder: \tender:\n",
      "GT sentence: Gnder: \tender\n",
      "Decoded sentence: Gender:\n",
      "\n",
      "-\n",
      "Input sentence: Plan \n",
      "GT sentence: Plan\n",
      "\n",
      "Decoded sentence: Plan Earnings:\n",
      "\n",
      "-\n",
      "Input sentence: Fiexoin to 120 \tFlyexion toc 120\n",
      "GT sentence: iexoin to 120 \tFlyexion toc 12\n",
      "Decoded sentence: First Name:\n",
      "\n",
      "-\n",
      "Input sentence: State: \n",
      "GT sentence: State:\n",
      "\n",
      "Decoded sentence: State/Province: OH\n",
      "\n",
      "-\n",
      "Input sentence: TOTAL DUE jzgffa \n",
      "GT sentence: TOTAL DUE : 345.15\n",
      "\n",
      "Decoded sentence: TOTAL DUE : 345.15\n",
      "\n",
      "-\n",
      "Input sentence: PROCEDURE NOJREVENUE CODE \n",
      "GT sentence: PROCEDURE NO./REVENUE CODE\n",
      "\n",
      "Decoded sentence: PROCEDURE MODICE\n",
      "\n",
      "-\n",
      "Input sentence: Group Polic y#: \tGroup Poilcy #:\n",
      "GT sentence: roup Polic y#: \tGroup Poilcy #\n",
      "Decoded sentence: Group Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: OPERATIVE REPORT - PAGE 1 of 2 \n",
      "GT sentence: OPERATIVE REPORT - PAGE 1 of 2\n",
      "\n",
      "Decoded sentence: OPERATIVE REPORT - PAGE 18o\n",
      "\n",
      "-\n",
      "Input sentence: Proc edure : Cleaning, may: bandage\n",
      "GT sentence: Procedure: Cleaning, xray, bandage\n",
      "\n",
      "Decoded sentence: Provider Last Name:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Prowider Last Name: Holm \n",
      "GT sentence: Provider Last Name: Holm\n",
      "\n",
      "Decoded sentence: Provider Last Name:\n",
      "\n",
      "-\n",
      "Input sentence: Vitasl \tVital\n",
      "\n",
      "GT sentence: itasl \tVital\n",
      "Decoded sentence: Diagnosis Code (ICD)\n",
      "\n",
      "-\n",
      "Input sentence: amilAddss: \tEmail Address:\n",
      "GT sentence: milAddss: \tEmail Address\n",
      "Decoded sentence: Date of Birth (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: \tEmpyoer:\n",
      "GT sentence: Empyoer\n",
      "Decoded sentence: Employee Off-Job Acc January 1, 2017\n",
      "\n",
      "-\n",
      "Input sentence: Roomi‘Eed EDGSCCEEDGSCC \n",
      "GT sentence: Room/Bed EDGSCC/EDGSCC\n",
      "\n",
      "Decoded sentence: Provider Last dame - yes\n",
      "\n",
      "-\n",
      "Input sentence: GUARANT'UH NAME AND. ADDRESS \n",
      "GT sentence: GUARANTOR NAME AND ADDRESS\n",
      "\n",
      "Decoded sentence: GUARANTOR ID\n",
      "\n",
      "-\n",
      "Input sentence: Active Poblems \tActivP roblems\n",
      "GT sentence: ctive Poblems \tActivP roblem\n",
      "Decoded sentence: Acct #:\n",
      "\n",
      "-\n",
      "Input sentence: My Snpous:e f\"— \tM Spuose: \n",
      "GT sentence: y Snpous:e f\"— \tM Spuose:\n",
      "Decoded sentence: MRI lISETOTOVILLE printed\n",
      "\n",
      "-\n",
      "Input sentence: Hospital Name Telephone Number \n",
      "GT sentence: Hospital Name Telephone Number\n",
      "\n",
      "Decoded sentence: Hospital Ef Date:\n",
      "\n",
      "-\n",
      "Input sentence: Complete Site: Chattanooga \n",
      "GT sentence: Complete Site: Chattanooga\n",
      "\n",
      "Decoded sentence: Country: US\n",
      "\n",
      "-\n",
      "Input sentence: Postal Codes 4 92 02 \n",
      "GT sentence: Postal Codes: 49202\n",
      "\n",
      "Decoded sentence: Postal Code:\n",
      "\n",
      "-\n",
      "Input sentence: . j . ACCIDENT CLAIM FORM \n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "Decoded sentence: ACCIDENT DETAILS\n",
      "\n",
      "-\n",
      "Input sentence: Exp:\tAcct# Exp:\n",
      "GT sentence: xp:\tAcct# Exp\n",
      "Decoded sentence: Earnings Type: Hourly\n",
      "\n",
      "-\n",
      "Input sentence: Jaso Holm, M.D. \tJason Holm, M.D.\n",
      "GT sentence: aso Holm, M.D. \tJason Holm, M.D\n",
      "Decoded sentence: Date of Birth (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: City: \tCity:\n",
      "GT sentence: ity: \tCity\n",
      "Decoded sentence: City: Burnsville\n",
      "\n",
      "-\n",
      "Input sentence: C ity. OH \n",
      "GT sentence: City: OH\n",
      "\n",
      "Decoded sentence: City: Burnsville\n",
      "\n",
      "-\n",
      "Input sentence: Completed Date:  \n",
      "GT sentence: Completed Date: \n",
      "\n",
      "Decoded sentence: Country: US\n",
      "\n",
      "-\n",
      "Input sentence: C . O The Benefits Center \n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: C. Signature Date\n",
      "\n",
      "-\n",
      "Input sentence: Earlnngs Mode: \tEarnings Mode:\n",
      "GT sentence: arlnngs Mode: \tEarnings Mode\n",
      "Decoded sentence: Eff Date:\n",
      "\n",
      "-\n",
      "Input sentence: Amount $\n",
      "GT sentence: Amount $\n",
      "\n",
      "Decoded sentence: Amount Insurinted\n",
      "\n",
      "-\n",
      "Input sentence: C Dually. US \n",
      "GT sentence: Country: US\n",
      "\n",
      "Decoded sentence: Customer Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: Ftient Name \tPatient Name:\n",
      "GT sentence: tient Name \tPatient Name\n",
      "Decoded sentence: Patient DOB:\n",
      "\n",
      "-\n",
      "Input sentence: unum‘t \n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: TEMP: 98.5 \ttTEMP: 8.5\n",
      "GT sentence: EMP: 98.5 \ttTEMP: 8.\n",
      "Decoded sentence: TELEPHONE\n",
      "\n",
      "-\n",
      "Input sentence: Leave start date \n",
      "GT sentence: Leave start date\n",
      "\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: FisrtN me: \tFirst Naeme:\n",
      "GT sentence: isrtN me: \tFirst Naeme\n",
      "Decoded sentence: First Name:\n",
      "\n",
      "-\n",
      "Input sentence: Vitals i\tVistalcs\n",
      "GT sentence: itals i\tVistalc\n",
      "Decoded sentence: Diagnosis Code (ICD)\n",
      "\n",
      "-\n",
      "Input sentence: Patient Demographics .\n",
      "GT sentence: Patient Demographics\n",
      "\n",
      "Decoded sentence: Patient DOB:\n",
      "\n",
      "-\n",
      "Input sentence: Total Monthly Premium: $24.40\n",
      "GT sentence: Total Monthly Premium: $24.40\n",
      "\n",
      "Decoded sentence: Total Monthly Premium: $33.81\n",
      "\n",
      "-\n",
      "Input sentence: Patient Date ofBlrth (mmlddlyy) \n",
      "GT sentence: Patient Date of Birth (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Patient DOB:\n",
      "\n",
      "-\n",
      "Input sentence: Deductjmns \n",
      "GT sentence: Deductions\n",
      "\n",
      "Decoded sentence: Dependent Information\n",
      "\n",
      "-\n",
      "Input sentence: Service Date: 03:0912013\n",
      "GT sentence: Service Date: 03/09/2018\n",
      "\n",
      "Decoded sentence: Service: Orthopedic\n",
      "\n",
      "-\n",
      "Input sentence: TIER 2 Family  \n",
      "GT sentence: TIER 2 Family Deductible\n",
      "\n",
      "Decoded sentence: TIER 1 Family MOOP Max \n",
      "\n",
      "-\n",
      "Input sentence: Employer Nana;\n",
      "GT sentence: Employer Name:\n",
      "\n",
      "Decoded sentence: Employee ID:\n",
      "\n",
      "-\n",
      "Input sentence: Postal Code: \n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code:\n",
      "\n",
      "-\n",
      "Input sentence: Last Name: \n",
      "GT sentence: Last Name:\n",
      "\n",
      "Decoded sentence: Last Name:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample output from test data\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
