{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index].strip() + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_noise(all_texts, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for rev in all_texts:\n",
    "            sents = sent_tokenize(rev) # Get review sentences.\n",
    "            #print(len(sents))\n",
    "            sent = sents[np.random.randint(0, max(1, len(sents) - 1))] # Pick some random sentence.\n",
    "            if cnt < num_samples :                \n",
    "                   \n",
    "                input_text = noise_maker(sent, noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sent.strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1].strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        if len(input_text) > max_encoder_seq_length or len(target_text) > max_encoder_seq_length:\n",
    "            continue\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_words_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data = np.zeros(\n",
    "    (len(input_texts), max_words_seq_length, max_chars_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    decoder_word_input_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length),\n",
    "        dtype='float32')\n",
    "    \n",
    "    decoder_word_target_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length, num_word_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[i, j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)# word_tokenize removes the \\t and \\n, we need them to start and end a sequence\n",
    "        words_lst.insert(0, '\\t')\n",
    "        words_lst.append('\\n')        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            processed_word = process_word(word)\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[i, j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[i, j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "    return encoder_char_input_data, decoder_word_input_data, decoder_word_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences_data(input_texts, target_labels, max_sents_per_doc, max_words_per_sent, max_chars_per_word, \n",
    "                             num_classes, char2int):\n",
    "\n",
    "    \n",
    "    \n",
    "    hier_input_data = np.zeros((len(input_texts), \n",
    "                                max_sents_per_doc, \n",
    "                                max_words_per_sent, \n",
    "                                max_chars_per_word), dtype='float32')\n",
    "    \n",
    "        \n",
    "    hier_target_data = np.zeros((len(input_texts), num_classes), dtype='float32')\n",
    "\n",
    "    \n",
    "    for i, (input_text, target_label) in enumerate(zip(input_texts, target_labels)):\n",
    "        #sents_lst = sent_tokenize(clean_str(BeautifulSoup(input_text).get_text())) # TODO: Move to clean str\n",
    "        sents_lst = sent_tokenize(input_text)\n",
    "        \n",
    "        \n",
    "        if len(sents_lst) > max_sents_per_doc:\n",
    "            continue\n",
    "        \n",
    "        for j, sent in enumerate(sents_lst):\n",
    "                \n",
    "            words_lst = word_tokenize(input_text)\n",
    "            \n",
    "            if(len(words_lst) > max_words_per_sent):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            for k, word in enumerate(words_lst):\n",
    "                \n",
    "                \n",
    "                if(len(word) > max_chars_per_word):\n",
    "                    continue\n",
    "                \n",
    "                for l, char in enumerate(word):\n",
    "                    # c0..cn\n",
    "                    if(char in char2int):\n",
    "                        hier_input_data[i, j, k, l] = char2int[char]\n",
    "                        hier_target_data[i, target_label] = 1\n",
    "\n",
    "                \n",
    "    return hier_input_data, hier_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_words_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_words_seq_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "       \n",
    "        \n",
    "        #orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(word_tokenize(decoded_sentence)) > max_words_seq_len):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        '''\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, max_words_seq_len))\n",
    "        if i < max_words_seq_len:\n",
    "            target_seq[0, i] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "        #if i > 48:\n",
    "        #    i = 0\n",
    "        \n",
    "\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_encoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars2word_model_simple_BiLSTM(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    " \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len, \n",
    "                           num_word_tokens, \n",
    "                           num_char_tokens, \n",
    "                           latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    #print(inputs.shape)\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "        \n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_sentence_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "\n",
    "    return encoder_sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars2word_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "    #print(encoder_outputs.shape)\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words2sent_model(encoder_word_embedding_model, \n",
    "                           max_words_seq_len, \n",
    "                           max_char_seq_len, \n",
    "                           num_word_tokens, \n",
    "                           num_char_tokens, \n",
    "                           latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    '''\n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "    '''\n",
    "    #input_words = TimeDistributed(Dense(10))(inputs)\n",
    "\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=inputs, output=[encoder_outputs] + encoder_states)\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_sentence_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(max_words_seq_len, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_words, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, encoder_sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def build_sent2doc_model(encoder_word_embedding_model, #? Already inside encoder_sentence_embedding_model?\n",
    "                         encoder_sentence_embedding_model, \n",
    "                         max_sents_seq_len, \n",
    "                         max_words_seq_len, \n",
    "                         max_char_seq_len, \n",
    "                         char2word_latent_dim,#?\n",
    "                         word2sent_latent_dim,\n",
    "                         sent2doc_latent_dim):\n",
    "    \n",
    "    inputs = Input(shape=(max_sents_seq_len, max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    \n",
    "    sents_states = []\n",
    "    \n",
    "    for s in range(max_sents_seq_len):\n",
    "        \n",
    "        encoder_words_inputs = Lambda(lambda x: x[:,s,:,:])(inputs)\n",
    "        #print(encoder_words_inputs.shape)\n",
    "        encoder_words_outputs = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        encoder_words_outputs = Reshape((1,word2sent_latent_dim*2))(encoder_words_outputs)\n",
    "        #_, h, c = encoder_sentence_embedding_model(encoder_words_inputs)\n",
    "        '''\n",
    "        input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "        encoder_inputs_ = input_words   \n",
    "        #encoder_inputs = Input(shape=(None, char_vocab_size))\n",
    "        encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "        encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "\n",
    "        encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        #encoder_words_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        #encoder_words_states = Reshape((1,word2sent_latent_dim*4))(encoder_words_states)\n",
    "        print(encoder_words_outputs.shape)\n",
    "        sents_states.append(encoder_words_outputs)\n",
    "    #print(sents_states)\n",
    "    input_sents = Concatenate(axis=-2)(sents_states)\n",
    "    print(input_sents.shape)\n",
    "    encoder_inputs_ = input_sents   \n",
    "    encoder = Bidirectional(LSTM(sent2doc_latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_42 (InputLayer)        (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, None, 28)          784       \n",
      "_________________________________________________________________\n",
      "bidirectional_36 (Bidirectio [(None, None, 1024), (Non 2215936   \n",
      "_________________________________________________________________\n",
      "lambda_103 (Lambda)          (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 2,216,720\n",
      "Trainable params: 2,215,936\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n",
      "None\n",
      "(?, 24, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_43 (InputLayer)        (None, 24, 5)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_15 (TimeDis (None, 24, 1024)          2216720   \n",
      "_________________________________________________________________\n",
      "bidirectional_37 (Bidirectio [(None, 24, 512), (None,  2623488   \n",
      "_________________________________________________________________\n",
      "lambda_104 (Lambda)          (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 4,840,208\n",
      "Trainable params: 4,839,424\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n",
      "None\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n",
      "(?, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "char2word_latent_dim = 512\n",
    "word2sent_latent_dim = 256\n",
    "sent2doc_latent_dim = 128\n",
    "char_vocab_size = 28\n",
    "word_vocab_size = 10000\n",
    "MAX_SENTS_PER_DOC = 11\n",
    "MAX_WORDS_PER_SENT = 24\n",
    "MAX_CHARS_PER_WORD = 5\n",
    "#_, _, _, encoder_word_embedding_model = build_chars2word_model(num_encoder_tokens=char_vocab_size, latent_dim=chars2word_latent_dim)\n",
    "encoder_word_embedding_model = build_chars2word_model_simple_BiLSTM(num_encoder_tokens=char_vocab_size, latent_dim=char2word_latent_dim)\n",
    "print(encoder_word_embedding_model.summary())\n",
    "'''\n",
    "_, _, _, encoder_sentence_embedding_model = build_words2sent_model(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                                   num_word_tokens=word_vocab_size, \n",
    "                                                                   num_char_tokens=char_vocab_size, \n",
    "                                                                   latent_dim=words2sent_latent_dim)\n",
    "'''\n",
    "encoder_sentence_embedding_model = build_words2sent_model_simple_BiLSTM(encoder_word_embedding_model, \n",
    "                                                                   max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                                   max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                                   num_word_tokens=word_vocab_size, \n",
    "                                                                   num_char_tokens=char_vocab_size, \n",
    "                                                                   latent_dim=word2sent_latent_dim)\n",
    "print(encoder_sentence_embedding_model.summary())\n",
    "\n",
    "encoder_review_embedding = build_sent2doc_model(encoder_word_embedding_model, #? Already inside encoder_sentence_embedding_model?\n",
    "                                                 encoder_sentence_embedding_model, \n",
    "                                                 max_sents_seq_len=MAX_SENTS_PER_DOC, \n",
    "                                                 max_words_seq_len=MAX_WORDS_PER_SENT, \n",
    "                                                 max_char_seq_len=MAX_CHARS_PER_WORD, \n",
    "                                                 char2word_latent_dim=char2word_latent_dim,#?\n",
    "                                                 word2sent_latent_dim=word2sent_latent_dim,\n",
    "                                                 sent2doc_latent_dim=sent2doc_latent_dim)\n",
    "print(encoder_review_embedding.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_words_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        #processed_word = word.lower()\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []\n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata_path = '../../dat/'\\nmax_sent_len = 50\\nmin_sent_len = 0\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 1000\\ninput_texts = []\\ntarget_texts = []\\nfiles_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\\ndesired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\\nnoise_threshold = 0.9\\n\\nfor file_name, num_file_samples in zip(files_list, desired_file_sizes):\\n    tess_correction_data = os.path.join(data_path, file_name)\\n    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\\n\\n    input_texts += input_texts_OCR\\n    target_texts += target_texts_OCR\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 1000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "data_file = 'imdb/labeledTrainData.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor text in data_train.review:\\n    for sent in sent_tokenize(clean_str(BeautifulSoup(text).get_text())):\\n        print(sent + '\\n')\\n        for word in word_tokenize(sent):\\n            print(word + '\\n')\\n    print('****************\\n')\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for text in data_train.review:\n",
    "    for sent in sent_tokenize(clean_str(BeautifulSoup(text).get_text())):\n",
    "        print(sent + '\\n')\n",
    "        for word in word_tokenize(sent):\n",
    "            print(word + '\\n')\n",
    "    print('****************\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "sents_per_docs_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in data_train.review:\n",
    "    \n",
    "    sents = sent_tokenize(clean_str(BeautifulSoup(text).get_text()))\n",
    "    sents_per_docs_lengths.append(len(sents))\n",
    "    for sent in sents:       \n",
    "    \n",
    "        words = word_tokenize(text)\n",
    "        words_per_sents_lengths.append(len(words))\n",
    "        for word in words:\n",
    "            chars_per_words_lengths.append(len(word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3779c14be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_s = plt.hist(sents_per_docs_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.700519999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.1049387246048941"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(sents_per_docs_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f377006e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.985993204068588"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.567778477735885"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(words_per_sents_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3770250198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1450829220930512"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5863593461489964"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(chars_per_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "all_texts = list(data_train.review.apply(BeautifulSoup).apply(BeautifulSoup.get_text).apply(clean_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "#np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in all_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 25000\n",
      "Number of unique input tokens: 142\n",
      "Number of unique output tokens: 142\n",
      "Max sequence length for inputs: 13546\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(all_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '!': 37,\n",
       " '#': 68,\n",
       " '$': 57,\n",
       " '%': 59,\n",
       " '&': 56,\n",
       " '(': 35,\n",
       " ')': 36,\n",
       " '*': 54,\n",
       " '+': 71,\n",
       " ',': 23,\n",
       " '-': 39,\n",
       " '.': 27,\n",
       " '/': 49,\n",
       " '0': 31,\n",
       " '1': 41,\n",
       " '2': 30,\n",
       " '3': 53,\n",
       " '4': 48,\n",
       " '5': 45,\n",
       " '6': 42,\n",
       " '7': 50,\n",
       " '8': 46,\n",
       " '9': 44,\n",
       " ':': 40,\n",
       " ';': 43,\n",
       " '<': 97,\n",
       " '=': 58,\n",
       " '>': 91,\n",
       " '?': 34,\n",
       " '@': 67,\n",
       " 'UNK': 0,\n",
       " '[': 62,\n",
       " ']': 63,\n",
       " '^': 82,\n",
       " '_': 69,\n",
       " '`': 55,\n",
       " 'a': 8,\n",
       " 'b': 28,\n",
       " 'c': 22,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 12,\n",
       " 'g': 13,\n",
       " 'h': 7,\n",
       " 'i': 5,\n",
       " 'j': 19,\n",
       " 'k': 26,\n",
       " 'l': 9,\n",
       " 'm': 18,\n",
       " 'n': 15,\n",
       " 'o': 14,\n",
       " 'p': 29,\n",
       " 'q': 33,\n",
       " 'r': 21,\n",
       " 's': 10,\n",
       " 't': 6,\n",
       " 'u': 11,\n",
       " 'v': 20,\n",
       " 'w': 4,\n",
       " 'x': 32,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " '{': 86,\n",
       " '|': 109,\n",
       " '}': 87,\n",
       " '~': 65,\n",
       " '\\x80': 134,\n",
       " '\\x84': 66,\n",
       " '\\x85': 64,\n",
       " '\\x8d': 121,\n",
       " '\\x8e': 130,\n",
       " '\\x91': 95,\n",
       " '\\x95': 126,\n",
       " '\\x96': 47,\n",
       " '\\x97': 70,\n",
       " '\\x9a': 135,\n",
       " '\\x9e': 131,\n",
       " '\\xa0': 124,\n",
       " '¡': 113,\n",
       " '¢': 120,\n",
       " '£': 51,\n",
       " '¤': 133,\n",
       " '¦': 106,\n",
       " '§': 128,\n",
       " '¨': 38,\n",
       " '«': 117,\n",
       " '\\xad': 127,\n",
       " '®': 77,\n",
       " '°': 139,\n",
       " '³': 125,\n",
       " '´': 52,\n",
       " '·': 132,\n",
       " 'º': 138,\n",
       " '»': 118,\n",
       " '½': 78,\n",
       " '¾': 140,\n",
       " '¿': 100,\n",
       " 'ß': 136,\n",
       " 'à': 84,\n",
       " 'á': 83,\n",
       " 'â': 107,\n",
       " 'ã': 103,\n",
       " 'ä': 72,\n",
       " 'å': 108,\n",
       " 'æ': 104,\n",
       " 'ç': 80,\n",
       " 'è': 79,\n",
       " 'é': 61,\n",
       " 'ê': 60,\n",
       " 'ë': 94,\n",
       " 'ì': 96,\n",
       " 'í': 98,\n",
       " 'î': 102,\n",
       " 'ï': 76,\n",
       " 'ð': 105,\n",
       " 'ñ': 101,\n",
       " 'ò': 99,\n",
       " 'ó': 85,\n",
       " 'ô': 93,\n",
       " 'õ': 122,\n",
       " 'ö': 90,\n",
       " 'ø': 89,\n",
       " 'ù': 81,\n",
       " 'ú': 119,\n",
       " 'û': 110,\n",
       " 'ü': 88,\n",
       " 'ý': 116,\n",
       " 'č': 129,\n",
       " 'ı': 137,\n",
       " 'ō': 141,\n",
       " 'ř': 114,\n",
       " '–': 74,\n",
       " '‘': 75,\n",
       " '’': 73,\n",
       " '“': 111,\n",
       " '”': 112,\n",
       " '…': 115,\n",
       " '₤': 92,\n",
       " '\\uf0b7': 123}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'w',\n",
       " 5: 'i',\n",
       " 6: 't',\n",
       " 7: 'h',\n",
       " 8: 'a',\n",
       " 9: 'l',\n",
       " 10: 's',\n",
       " 11: 'u',\n",
       " 12: 'f',\n",
       " 13: 'g',\n",
       " 14: 'o',\n",
       " 15: 'n',\n",
       " 16: 'd',\n",
       " 17: 'e',\n",
       " 18: 'm',\n",
       " 19: 'j',\n",
       " 20: 'v',\n",
       " 21: 'r',\n",
       " 22: 'c',\n",
       " 23: ',',\n",
       " 24: 'y',\n",
       " 25: 'z',\n",
       " 26: 'k',\n",
       " 27: '.',\n",
       " 28: 'b',\n",
       " 29: 'p',\n",
       " 30: '2',\n",
       " 31: '0',\n",
       " 32: 'x',\n",
       " 33: 'q',\n",
       " 34: '?',\n",
       " 35: '(',\n",
       " 36: ')',\n",
       " 37: '!',\n",
       " 38: '¨',\n",
       " 39: '-',\n",
       " 40: ':',\n",
       " 41: '1',\n",
       " 42: '6',\n",
       " 43: ';',\n",
       " 44: '9',\n",
       " 45: '5',\n",
       " 46: '8',\n",
       " 47: '\\x96',\n",
       " 48: '4',\n",
       " 49: '/',\n",
       " 50: '7',\n",
       " 51: '£',\n",
       " 52: '´',\n",
       " 53: '3',\n",
       " 54: '*',\n",
       " 55: '`',\n",
       " 56: '&',\n",
       " 57: '$',\n",
       " 58: '=',\n",
       " 59: '%',\n",
       " 60: 'ê',\n",
       " 61: 'é',\n",
       " 62: '[',\n",
       " 63: ']',\n",
       " 64: '\\x85',\n",
       " 65: '~',\n",
       " 66: '\\x84',\n",
       " 67: '@',\n",
       " 68: '#',\n",
       " 69: '_',\n",
       " 70: '\\x97',\n",
       " 71: '+',\n",
       " 72: 'ä',\n",
       " 73: '’',\n",
       " 74: '–',\n",
       " 75: '‘',\n",
       " 76: 'ï',\n",
       " 77: '®',\n",
       " 78: '½',\n",
       " 79: 'è',\n",
       " 80: 'ç',\n",
       " 81: 'ù',\n",
       " 82: '^',\n",
       " 83: 'á',\n",
       " 84: 'à',\n",
       " 85: 'ó',\n",
       " 86: '{',\n",
       " 87: '}',\n",
       " 88: 'ü',\n",
       " 89: 'ø',\n",
       " 90: 'ö',\n",
       " 91: '>',\n",
       " 92: '₤',\n",
       " 93: 'ô',\n",
       " 94: 'ë',\n",
       " 95: '\\x91',\n",
       " 96: 'ì',\n",
       " 97: '<',\n",
       " 98: 'í',\n",
       " 99: 'ò',\n",
       " 100: '¿',\n",
       " 101: 'ñ',\n",
       " 102: 'î',\n",
       " 103: 'ã',\n",
       " 104: 'æ',\n",
       " 105: 'ð',\n",
       " 106: '¦',\n",
       " 107: 'â',\n",
       " 108: 'å',\n",
       " 109: '|',\n",
       " 110: 'û',\n",
       " 111: '“',\n",
       " 112: '”',\n",
       " 113: '¡',\n",
       " 114: 'ř',\n",
       " 115: '…',\n",
       " 116: 'ý',\n",
       " 117: '«',\n",
       " 118: '»',\n",
       " 119: 'ú',\n",
       " 120: '¢',\n",
       " 121: '\\x8d',\n",
       " 122: 'õ',\n",
       " 123: '\\uf0b7',\n",
       " 124: '\\xa0',\n",
       " 125: '³',\n",
       " 126: '\\x95',\n",
       " 127: '\\xad',\n",
       " 128: '§',\n",
       " 129: 'č',\n",
       " 130: '\\x8e',\n",
       " 131: '\\x9e',\n",
       " 132: '·',\n",
       " 133: '¤',\n",
       " 134: '\\x80',\n",
       " 135: '\\x9a',\n",
       " 136: 'ß',\n",
       " 137: 'ı',\n",
       " 138: 'º',\n",
       " 139: '°',\n",
       " 140: '¾',\n",
       " 141: 'ō'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate noisy data for pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "noise_threshold = 0.9\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0\n",
    "max_encoder_seq_length = max_sent_len\n",
    "input_texts, target_texts, _ = generate_data_with_noise(all_texts.copy(), num_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bceause mj oevrheard his plans\n",
      "\n",
      "\tbecause mj overheard his plans?\n",
      "\n",
      "\n",
      "gayr busey kicks their butts, of ousre\n",
      "\n",
      "\tgary busey kicks their butts, of course.\n",
      "\n",
      "\n",
      "a very firne fim\n",
      "\n",
      "\ta very fine film.\n",
      "\n",
      "\n",
      "motlym an amateur film with lame fx\n",
      "\n",
      "\tmostly an amateur film with lame fx.\n",
      "\n",
      "\n",
      "isit funn\n",
      "\n",
      "\tis it funny?\n",
      "\n",
      "\n",
      ")funny, ro by\n",
      "\n",
      "\t)funny, or b.\n",
      "\n",
      "\n",
      "even rwhe it ha alreayd been writtetn for tbem\n",
      "\n",
      "\teven when it has already been written for them!\n",
      "\n",
      "\n",
      "but wow, this movie was absoutley awfl\n",
      "\n",
      "\tbut wow, this movie was absolutely awful.\n",
      "\n",
      "\n",
      "qveoricoa mras isa ohre\n",
      "\n",
      "\tveronica mars is a bore.\n",
      "\n",
      "\n",
      "what an absolutely crappy film this is\n",
      "\n",
      "\twhat an absolutely crappy film this is.\n",
      "\n",
      "\n",
      "but iyt could hvae been mucyh bette\n",
      "\n",
      "\tbut it could have been much better.\n",
      "\n",
      "\n",
      "backwards evil messaqgse played on vinyl\n",
      "\n",
      "\tbackwards evil messages played on vinyl!\n",
      "\n",
      "\n",
      "the sstory is absorbing\n",
      "\n",
      "\tthe story is absorbing.\n",
      "\n",
      "\n",
      "forget histrical reality too\n",
      "\n",
      "\tforget historical reality too.\n",
      "\n",
      "\n",
      "tcoo long,to oborin\n",
      "\n",
      "\ttoo long,too boring.\n",
      "\n",
      "\n",
      "i pdidnt ruead te boo\n",
      "\n",
      "\ti didnt read the book.\n",
      "\n",
      "\n",
      "well dne al ore\n",
      "\n",
      "\twell done al gore!\n",
      "\n",
      "\n",
      "it is notfzunny\n",
      "\n",
      "\tit is not funny.\n",
      "\n",
      "\n",
      "yuo yalso certanly owndgred how things went\n",
      "\n",
      "\tyou also certainly wondered how things went.\n",
      "\n",
      "\n",
      "but actions speavk loder than words\n",
      "\n",
      "\tbut actions speak louder than words!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(input_texts[i] + '\\n')\n",
    "    print(target_texts[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize char data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_char_data(input_texts=all_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 142)    20164       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  817152      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 142)    20164       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1341440     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 142)    145550      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,344,470\n",
      "Trainable params: 2,304,142\n",
      "Non-trainable params: 40,328\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model, encoder_word_embedding_model = build_chars2word_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 142)    20164       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  817152      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 837,316\n",
      "Trainable params: 817,152\n",
      "Non-trainable params: 20,164\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 116s 6ms/step - loss: nan - categorical_accuracy: nan - val_loss: nan - val_categorical_accuracy: nan\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy did not improve from -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36a35cbc50>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1  \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_char-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model_file = 'encoder_char_model-{}.hdf5'\n",
    "decoder_char_model_file = 'decoder_char_model-{}.hdf5'\n",
    "encoder_model.save('encoder_char_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_char_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_seq_len=15\n",
    "max_chars_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len, max_chars_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_chars_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'because': 4,\n",
       " 'mj': 5,\n",
       " 'overheard': 6,\n",
       " 'his': 7,\n",
       " 'plans': 8,\n",
       " 'gary': 9,\n",
       " 'busey': 10,\n",
       " 'kicks': 11,\n",
       " 'their': 12,\n",
       " 'butts': 13,\n",
       " 'of': 14,\n",
       " 'course': 15,\n",
       " 'a': 16,\n",
       " 'very': 17,\n",
       " 'fine': 18,\n",
       " 'film': 19,\n",
       " 'mostly': 20,\n",
       " 'an': 21,\n",
       " 'amateur': 22,\n",
       " 'with': 23,\n",
       " 'lame': 24,\n",
       " 'fx': 25,\n",
       " 'is': 26,\n",
       " 'it': 27,\n",
       " 'funny': 28,\n",
       " 'or': 29,\n",
       " 'b': 30,\n",
       " 'even': 31,\n",
       " 'when': 32,\n",
       " 'has': 33,\n",
       " 'already': 34,\n",
       " 'been': 35,\n",
       " 'written': 36,\n",
       " 'for': 37,\n",
       " 'them': 38,\n",
       " '!': 39,\n",
       " 'but': 40,\n",
       " 'wow': 41,\n",
       " 'this': 42,\n",
       " 'movie': 43,\n",
       " 'was': 44,\n",
       " 'absolutely': 45,\n",
       " 'awful': 46,\n",
       " 'veronica': 47,\n",
       " 'mars': 48,\n",
       " 'bore': 49,\n",
       " 'what': 50,\n",
       " 'crappy': 51,\n",
       " 'could': 52,\n",
       " 'have': 53,\n",
       " 'much': 54,\n",
       " 'better': 55,\n",
       " 'backwards': 56,\n",
       " 'evil': 57,\n",
       " 'messages': 58,\n",
       " 'played': 59,\n",
       " 'on': 60,\n",
       " 'vinyl': 61,\n",
       " 'the': 62,\n",
       " 'story': 63,\n",
       " 'absorbing': 64,\n",
       " 'forget': 65,\n",
       " 'historical': 66,\n",
       " 'reality': 67,\n",
       " 'too': 68,\n",
       " 'long': 69,\n",
       " 'boring': 70,\n",
       " 'i': 71,\n",
       " 'didnt': 72,\n",
       " 'read': 73,\n",
       " 'book': 74,\n",
       " 'well': 75,\n",
       " 'done': 76,\n",
       " 'al': 77,\n",
       " 'gore': 78,\n",
       " 'not': 79,\n",
       " 'you': 80,\n",
       " 'also': 81,\n",
       " 'certainly': 82,\n",
       " 'wondered': 83,\n",
       " 'how': 84,\n",
       " 'things': 85,\n",
       " 'went': 86,\n",
       " 'actions': 87,\n",
       " 'speak': 88,\n",
       " 'louder': 89,\n",
       " 'than': 90,\n",
       " 'words': 91,\n",
       " 'its': 92,\n",
       " 'good': 93,\n",
       " 'minor': 94,\n",
       " 'flaw': 95,\n",
       " 'easily': 96,\n",
       " 'ignored': 97,\n",
       " 'yes': 98,\n",
       " 'way': 99,\n",
       " 'does': 100,\n",
       " 'show': 101,\n",
       " 'bad': 102,\n",
       " 'times': 103,\n",
       " 'and': 104,\n",
       " 'actors': 105,\n",
       " 'baaaaaad': 106,\n",
       " 'line': 107,\n",
       " 'script': 108,\n",
       " 'monstervision': 109,\n",
       " 'grew': 110,\n",
       " 'up': 111,\n",
       " 'something': 112,\n",
       " 'weird': 113,\n",
       " 'definitely': 114,\n",
       " 'cheaply': 115,\n",
       " 'made': 116,\n",
       " 'doesnt': 117,\n",
       " 'stop': 118,\n",
       " 'there': 119,\n",
       " 'sadly': 120,\n",
       " 'in': 121,\n",
       " 'my': 122,\n",
       " 'case': 123,\n",
       " 'dvd': 124,\n",
       " 'enjoyed': 125,\n",
       " 'acting': 126,\n",
       " 'moved': 127,\n",
       " 'along': 128,\n",
       " 'spring': 129,\n",
       " 'byington': 130,\n",
       " 'seen': 131,\n",
       " 'as': 132,\n",
       " 'emily': 133,\n",
       " 'mother': 134,\n",
       " 'however': 135,\n",
       " 'being': 136,\n",
       " 'fan': 137,\n",
       " 'couldnt': 138,\n",
       " 'resist': 139,\n",
       " 'only': 140,\n",
       " 'really': 141,\n",
       " 'denero': 142,\n",
       " 'fans': 143,\n",
       " 'bar': 144,\n",
       " 'none': 145,\n",
       " 'best': 146,\n",
       " 'ive': 147,\n",
       " 'ever': 148,\n",
       " 'to': 149,\n",
       " 'answer': 150,\n",
       " 'your': 151,\n",
       " 'question': 152,\n",
       " 'he': 153,\n",
       " 'definately': 154,\n",
       " 'one': 155,\n",
       " 'all': 156,\n",
       " 'time': 157,\n",
       " 'greats': 158,\n",
       " 'domestic': 159,\n",
       " 'import': 160,\n",
       " 'great': 161,\n",
       " 'thought': 162,\n",
       " 'fantastic': 163,\n",
       " 'second': 164,\n",
       " 'cliched': 165,\n",
       " 'that': 166,\n",
       " 'necessary': 167,\n",
       " 'never': 168,\n",
       " 'little': 169,\n",
       " 'plot': 170,\n",
       " 'makes': 171,\n",
       " 'any': 172,\n",
       " 'sense': 173,\n",
       " 'mplayer': 174,\n",
       " 'managed': 175,\n",
       " 'run': 176,\n",
       " 'flawlessly': 177,\n",
       " 'did': 178,\n",
       " 'she': 179,\n",
       " 'make': 180,\n",
       " 'cover': 181,\n",
       " 'who': 182,\n",
       " 'came': 183,\n",
       " 'idea': 184,\n",
       " 'fun': 185,\n",
       " 'crap': 186,\n",
       " 'entertaining': 187,\n",
       " 'heartwarming': 188,\n",
       " 'mark': 189,\n",
       " 'dixon': 190,\n",
       " 'many': 191,\n",
       " 'troops': 192,\n",
       " 'just': 193,\n",
       " 'flat': 194,\n",
       " 'out': 195,\n",
       " 'unwatchable': 196,\n",
       " 'soothing': 197,\n",
       " 'otherwise': 198,\n",
       " 'sucked': 199,\n",
       " 'repugnant': 200,\n",
       " 'bronson': 201,\n",
       " 'thriller': 202,\n",
       " 'most': 203,\n",
       " 'comments': 204,\n",
       " 'favorite': 205,\n",
       " 'columbo': 206,\n",
       " 'episodes': 207,\n",
       " 'more': 208,\n",
       " 'extreme': 209,\n",
       " 'loved': 210,\n",
       " 'since': 211,\n",
       " 'young': 212,\n",
       " 'please': 213,\n",
       " 'watch': 214,\n",
       " 'responsibly': 215,\n",
       " 'racist': 216,\n",
       " 'offensive': 217,\n",
       " 'midgets': 218,\n",
       " 'go': 219,\n",
       " 'figure': 220,\n",
       " 'pretty': 221,\n",
       " 'terrible': 222,\n",
       " 'stuff': 223,\n",
       " 'characters': 224,\n",
       " 'are': 225,\n",
       " 'payroll': 226,\n",
       " 'apparently': 227,\n",
       " 'essence': 228,\n",
       " 'dao': 229,\n",
       " 'uses': 230,\n",
       " 'nice': 231,\n",
       " 'overall': 232,\n",
       " 'worth': 233,\n",
       " 'no': 234,\n",
       " 'value': 235,\n",
       " 'watching': 236,\n",
       " 'whole': 237,\n",
       " 'family': 238,\n",
       " 'special': 239,\n",
       " 'surprise': 240,\n",
       " 'found': 241,\n",
       " 'enjoyable': 242,\n",
       " 'girl': 243,\n",
       " 'okay': 244,\n",
       " 'job': 245,\n",
       " 'remember': 246,\n",
       " 'promise': 247,\n",
       " 'nuclear': 248,\n",
       " 'energy': 249,\n",
       " 'me': 250,\n",
       " 'taking': 251,\n",
       " 'chance': 252,\n",
       " 'total': 253,\n",
       " 'fabrication': 254,\n",
       " 'emma': 255,\n",
       " 'roberts': 256,\n",
       " 'adorable': 257,\n",
       " 'title': 258,\n",
       " 'role': 259,\n",
       " 'creative': 260,\n",
       " 'talented': 261,\n",
       " 'cast': 262,\n",
       " 'hundred': 263,\n",
       " 'worse': 264,\n",
       " 'waste': 265,\n",
       " 'money': 266,\n",
       " 'celluloid': 267,\n",
       " 'jean': 268,\n",
       " 'arthur': 269,\n",
       " 'shines': 270,\n",
       " 'looks': 271,\n",
       " 'at': 272,\n",
       " 'powell': 273,\n",
       " 'matthias': 274,\n",
       " 'hues': 275,\n",
       " 'always': 276,\n",
       " 'theres': 277,\n",
       " 'substance': 278,\n",
       " 'underneath': 279,\n",
       " 'believe': 280,\n",
       " 'wrong': 281,\n",
       " 'choice': 282,\n",
       " 'songs': 283,\n",
       " 'dont': 284,\n",
       " 'scottish': 285,\n",
       " 'locale': 286,\n",
       " 'wasted': 287,\n",
       " 'rest': 288,\n",
       " 'frazetta': 289,\n",
       " 'motion': 290,\n",
       " 'kudos': 291,\n",
       " 'everyone': 292,\n",
       " 'involved': 293,\n",
       " 'see': 294,\n",
       " 'yourself': 295,\n",
       " 'plenty': 296,\n",
       " 'cameos': 297,\n",
       " 'by': 298,\n",
       " 'real': 299,\n",
       " 'life': 300,\n",
       " 'porno': 301,\n",
       " 'stars': 302,\n",
       " 'indian': 303,\n",
       " 'summer': 304,\n",
       " 'flawless': 305,\n",
       " 'remains': 306,\n",
       " 'gray': 307,\n",
       " 'far': 308,\n",
       " 'true': 309,\n",
       " 'decameron': 310,\n",
       " 'wicker': 311,\n",
       " 'man': 312,\n",
       " 'handled': 313,\n",
       " 'badly': 314,\n",
       " 'people': 315,\n",
       " 'love': 316,\n",
       " 'these': 317,\n",
       " 'after': 318,\n",
       " 'had': 319,\n",
       " 'landau': 320,\n",
       " 'cant': 321,\n",
       " 'salvage': 322,\n",
       " 'complete': 323,\n",
       " 'superb': 324,\n",
       " 'blair': 325,\n",
       " 'witch': 326,\n",
       " 'proved': 327,\n",
       " 'everything': 328,\n",
       " 'documentary': 329,\n",
       " 'be': 330,\n",
       " 'horrible': 331,\n",
       " 'schlock': 332,\n",
       " 'natural': 333,\n",
       " 'genuine': 334,\n",
       " 'might': 335,\n",
       " 'first': 336,\n",
       " 'episode': 337,\n",
       " 'ask': 338,\n",
       " 'pointless': 339,\n",
       " 'points': 340,\n",
       " 'magnificent': 341,\n",
       " 'music': 342,\n",
       " 'score': 343,\n",
       " 'surprised': 344,\n",
       " 'falling': 345,\n",
       " 'pencil': 346,\n",
       " 'extra': 347,\n",
       " 'impact': 348,\n",
       " 'will': 349,\n",
       " 'god': 350,\n",
       " 'forgive': 351,\n",
       " 'us': 352,\n",
       " 'weve': 353,\n",
       " 'some': 354,\n",
       " 'punishments': 355,\n",
       " 'were': 356,\n",
       " 'so': 357,\n",
       " 'ridiculous': 358,\n",
       " 'tony': 359,\n",
       " 'hides': 360,\n",
       " 'her': 361,\n",
       " 'west': 362,\n",
       " 'moment': 363,\n",
       " 'hollywood': 364,\n",
       " 'riots': 365,\n",
       " 'between': 366,\n",
       " 'robots': 367,\n",
       " 'mankind': 368,\n",
       " 'quite': 369,\n",
       " 'year': 370,\n",
       " 'old': 371,\n",
       " 'lyrical': 372,\n",
       " 'relax': 373,\n",
       " 'low': 374,\n",
       " 'budget': 375,\n",
       " 'david': 376,\n",
       " 'lean': 377,\n",
       " 'knew': 378,\n",
       " 'films': 379,\n",
       " 'from': 380,\n",
       " 'wont': 381,\n",
       " 'get': 382,\n",
       " 'into': 383,\n",
       " 'demonicus': 384,\n",
       " 'turned': 385,\n",
       " 'video': 386,\n",
       " 'game': 387,\n",
       " 'refuse': 388,\n",
       " 'check': 389,\n",
       " 'watched': 390,\n",
       " 'hulu': 391,\n",
       " 'commercials': 392,\n",
       " 'know': 393,\n",
       " 'do': 394,\n",
       " 'eddie': 395,\n",
       " 'murphy': 396,\n",
       " 'peaks': 397,\n",
       " 'during': 398,\n",
       " 'movies': 399,\n",
       " 'here': 400,\n",
       " 'can': 401,\n",
       " 'tell': 402,\n",
       " 'arent': 403,\n",
       " 'right': 404,\n",
       " 'police': 405,\n",
       " 'sounds': 406,\n",
       " 'rather': 407,\n",
       " 'spoken': 408,\n",
       " 'beautifully': 409,\n",
       " 'shot': 410,\n",
       " 'think': 411,\n",
       " 'leave': 412,\n",
       " 'laughing': 413,\n",
       " 'diana': 414,\n",
       " 'guzman': 415,\n",
       " 'angry': 416,\n",
       " 'woman': 417,\n",
       " 'move': 418,\n",
       " 'tv': 419,\n",
       " 'last': 420,\n",
       " 'night': 421,\n",
       " 'thing': 422,\n",
       " 'remarkable': 423,\n",
       " 'about': 424,\n",
       " 'someone': 425,\n",
       " 'owed': 426,\n",
       " 'royalties': 427,\n",
       " 'raw': 428,\n",
       " 'look': 429,\n",
       " 'changes': 430,\n",
       " 'obsession': 431,\n",
       " 'grows': 432,\n",
       " 'outside': 433,\n",
       " 'gaming': 434,\n",
       " 'session': 435,\n",
       " 'proves': 436,\n",
       " 'theory': 437,\n",
       " 'star': 438,\n",
       " 'power': 439,\n",
       " 'mob': 440,\n",
       " 'three': 441,\n",
       " 'recommended': 442,\n",
       " 'friend': 443,\n",
       " 'connecting': 444,\n",
       " 'tedious': 445,\n",
       " 'twilight': 446,\n",
       " 'zone': 447,\n",
       " 'got': 448,\n",
       " 'like': 449,\n",
       " 'blade': 450,\n",
       " 'runner': 451,\n",
       " 'liked': 452,\n",
       " 'especially': 453,\n",
       " 'lexa': 454,\n",
       " 'doigs': 455,\n",
       " 'downhill': 456,\n",
       " 'russian': 457,\n",
       " 'norwegian': 458,\n",
       " 'why': 459,\n",
       " 'garbage': 460,\n",
       " 'afghanistan': 461,\n",
       " 'kid': 462,\n",
       " 'videos': 463,\n",
       " 'avoid': 464,\n",
       " 'lavish': 465,\n",
       " 'fantasy': 466,\n",
       " 'style': 467,\n",
       " 'hate': 468,\n",
       " 'talent': 469,\n",
       " 'kept': 470,\n",
       " 'attention': 471,\n",
       " 'start': 472,\n",
       " 'finish': 473,\n",
       " 'theyre': 474,\n",
       " 'both': 475,\n",
       " 'damn': 476,\n",
       " 'listen': 477,\n",
       " 'knox': 478,\n",
       " 'nazi': 479,\n",
       " 'brilliant': 480,\n",
       " 'where': 481,\n",
       " 'we': 482,\n",
       " 'heard': 483,\n",
       " 'before': 484,\n",
       " 'am': 485,\n",
       " 'bakhtiari': 486,\n",
       " 'need': 487,\n",
       " 'industry': 488,\n",
       " 'hard': 489,\n",
       " 'come': 490,\n",
       " 'hermann': 491,\n",
       " 'levi': 492,\n",
       " 'conductor': 493,\n",
       " 'sound': 494,\n",
       " 'sometimes': 495,\n",
       " 'dubbed': 496,\n",
       " 'describe': 497,\n",
       " 'exactly': 498,\n",
       " 'worthy': 499,\n",
       " 'barrymores': 500,\n",
       " 'beauty': 501,\n",
       " 'sounded': 502,\n",
       " 'asian': 503,\n",
       " 'jewish': 504,\n",
       " 'marshall': 505,\n",
       " 'miles': 506,\n",
       " 'safe': 507,\n",
       " 'disappointing': 508,\n",
       " 'norris': 509,\n",
       " 'beg': 510,\n",
       " 'roslyn': 511,\n",
       " 'sanchez': 512,\n",
       " 'looked': 513,\n",
       " 'forward': 514,\n",
       " 'those': 515,\n",
       " 'days': 516,\n",
       " 'spit': 517,\n",
       " 'say': 518,\n",
       " 'superior': 519,\n",
       " 'feel': 520,\n",
       " 'couple': 521,\n",
       " 'hours': 522,\n",
       " 'impossible': 523,\n",
       " 'greetings': 524,\n",
       " 'again': 525,\n",
       " 'darkness': 526,\n",
       " 'evening': 527,\n",
       " 'disappointed': 528,\n",
       " 'framing': 529,\n",
       " 'thats': 530,\n",
       " 'cheap': 531,\n",
       " 'endearing': 532,\n",
       " 'xizao': 533,\n",
       " 'rare': 534,\n",
       " 'sad': 535,\n",
       " 'around': 536,\n",
       " 'revisiting': 537,\n",
       " 'childhood': 538,\n",
       " 'mum': 539,\n",
       " 'would': 540,\n",
       " 'yelling': 541,\n",
       " 'atlanta': 542,\n",
       " 'nope': 543,\n",
       " 'wonder': 544,\n",
       " 'saw': 545,\n",
       " 'mill': 546,\n",
       " 'valley': 547,\n",
       " 'festival': 548,\n",
       " 'freshman': 549,\n",
       " 'rhetoric': 550,\n",
       " 'race': 551,\n",
       " 'gender': 552,\n",
       " 'then': 553,\n",
       " 'disprove': 554,\n",
       " 'beats': 555,\n",
       " 'cute': 556,\n",
       " 'playful': 557,\n",
       " 'levels': 558,\n",
       " 'tough': 559,\n",
       " 'other': 560,\n",
       " 'hand': 561,\n",
       " 'adored': 562,\n",
       " 'literary': 563,\n",
       " 'asked': 564,\n",
       " 'refund': 565,\n",
       " 'oh': 566,\n",
       " 'yeah': 567,\n",
       " 'robotic': 568,\n",
       " 'regimented': 569,\n",
       " 'werent': 570,\n",
       " 'enough': 571,\n",
       " 'bo': 572,\n",
       " 'welch': 573,\n",
       " 'should': 574,\n",
       " 'fired': 575,\n",
       " 'writer': 576,\n",
       " 'shows': 577,\n",
       " 'another': 578,\n",
       " 'final': 579,\n",
       " 'justice': 580,\n",
       " 'word': 581,\n",
       " 'amazing': 582,\n",
       " 'heavens': 583,\n",
       " 'im': 584,\n",
       " 'sure': 585,\n",
       " 'lot': 586,\n",
       " 'writers': 587,\n",
       " 'television': 588,\n",
       " 'cinema': 589,\n",
       " 'pia': 590,\n",
       " 'barely': 591,\n",
       " 'talk': 592,\n",
       " 'let': 593,\n",
       " 'alone': 594,\n",
       " 'write': 595,\n",
       " 'unspeakably': 596,\n",
       " 'anything': 597,\n",
       " 'over': 598,\n",
       " 'supposed': 599,\n",
       " 'prove': 600,\n",
       " 'anyway': 601,\n",
       " 'may': 602,\n",
       " 'laughed': 603,\n",
       " 'jim': 604,\n",
       " 'carrey': 605,\n",
       " 'beautiful': 606,\n",
       " 'become': 607,\n",
       " 'naive': 608,\n",
       " 'string': 609,\n",
       " 'several': 610,\n",
       " 'jokes': 611,\n",
       " 'acted': 612,\n",
       " 'sexual': 613,\n",
       " 'tension': 614,\n",
       " 'sooo': 615,\n",
       " 'scary': 616,\n",
       " 'devilish': 617,\n",
       " 'guess': 618,\n",
       " 'bayless': 619,\n",
       " 'they': 620,\n",
       " 'seek': 621,\n",
       " 'suppress': 622,\n",
       " 'invention': 623,\n",
       " 'conventional': 624,\n",
       " 'means': 625,\n",
       " 'doctor': 626,\n",
       " 'possibilities': 627,\n",
       " 'says': 628,\n",
       " 'bookkeeper': 629,\n",
       " 'steve': 630,\n",
       " 'noisy': 631,\n",
       " 'starts': 632,\n",
       " 'singing': 633,\n",
       " 'glad': 634,\n",
       " 'air': 635,\n",
       " 'through': 636,\n",
       " 'end': 637,\n",
       " 'pure': 638,\n",
       " 'unadulterated': 639,\n",
       " 'violence': 640,\n",
       " 'take': 641,\n",
       " 'him': 642,\n",
       " 'seriously': 643,\n",
       " 'winner': 644,\n",
       " 'give': 645,\n",
       " 'ten': 646,\n",
       " 'ice': 647,\n",
       " 'cream': 648,\n",
       " 'spread': 649,\n",
       " 'crackers': 650,\n",
       " 'told': 651,\n",
       " 'thousand': 652,\n",
       " 'setting': 653,\n",
       " 'mentioning': 654,\n",
       " 'two': 655,\n",
       " 'lead': 656,\n",
       " 'roles': 657,\n",
       " 'performed': 658,\n",
       " 'perfection': 659,\n",
       " 'school': 660,\n",
       " 'outdated': 661,\n",
       " 'nothing': 662,\n",
       " 'remotely': 663,\n",
       " 'interesting': 664,\n",
       " 'happens': 665,\n",
       " 'due': 666,\n",
       " 'jamie': 667,\n",
       " 'foxx': 668,\n",
       " 'huzzah': 669,\n",
       " 'that´s': 670,\n",
       " 'basically': 671,\n",
       " 'direction': 672,\n",
       " 'must': 673,\n",
       " 'faulted': 674,\n",
       " 'if': 675,\n",
       " 'wouldve': 676,\n",
       " 'try': 677,\n",
       " 'such': 678,\n",
       " 'piece': 679,\n",
       " 'page': 680,\n",
       " 'point': 681,\n",
       " 'eludes': 682,\n",
       " 'scenes': 683,\n",
       " 'getters': 684,\n",
       " 'sherry': 685,\n",
       " 'baby': 686,\n",
       " 'week': 687,\n",
       " 'freaked': 688,\n",
       " 'verdict': 689,\n",
       " 'james': 690,\n",
       " 'garner': 691,\n",
       " 'addition': 692,\n",
       " 'sent': 693,\n",
       " 'stunt': 694,\n",
       " 'double': 695,\n",
       " 'high': 696,\n",
       " 'hopes': 697,\n",
       " 'won': 698,\n",
       " 'raves': 699,\n",
       " 'disappeared': 700,\n",
       " 'ramsey': 701,\n",
       " 'bring': 702,\n",
       " 'callar': 703,\n",
       " 'appears': 704,\n",
       " 'england': 705,\n",
       " 'dull': 706,\n",
       " 'doorknob': 707,\n",
       " 'dust': 708,\n",
       " 'world': 709,\n",
       " 'place': 710,\n",
       " 'voices': 711,\n",
       " 'often': 712,\n",
       " 'distorted': 713,\n",
       " 'reason': 714,\n",
       " 'stellan': 715,\n",
       " 'skarsgård': 716,\n",
       " 'cameo': 717,\n",
       " 'pleasantly': 718,\n",
       " 'flaws': 719,\n",
       " 'spill': 720,\n",
       " 'hilarious': 721,\n",
       " 'fear': 722,\n",
       " 'black': 723,\n",
       " 'hat': 724,\n",
       " 'superbly': 725,\n",
       " 'crafted': 726,\n",
       " 'pleased': 727,\n",
       " 'hair': 728,\n",
       " 'mean': 729,\n",
       " 'snipes': 730,\n",
       " 'greg': 731,\n",
       " 'kinnear': 732,\n",
       " 'chicago': 733,\n",
       " 'international': 734,\n",
       " 'kolchak': 735,\n",
       " 'scenery': 736,\n",
       " 'wheres': 737,\n",
       " 'kleinfeld': 738,\n",
       " 'father': 739,\n",
       " 'cares': 740,\n",
       " 'taken': 741,\n",
       " 'back': 742,\n",
       " 'asks': 743,\n",
       " 'touched': 744,\n",
       " 'years': 745,\n",
       " 'few': 746,\n",
       " 'updates': 747,\n",
       " 'seemed': 748,\n",
       " 'suspicious': 749,\n",
       " 'results': 750,\n",
       " 'unabsorbing': 751,\n",
       " 'tracking': 752,\n",
       " 'cuts': 753,\n",
       " 'slow': 754,\n",
       " 'sequel': 755,\n",
       " 'murder': 756,\n",
       " 'small': 757,\n",
       " 'town': 758,\n",
       " 'performance': 759,\n",
       " 'drive': 760,\n",
       " 'typical': 761,\n",
       " 'jaipur': 762,\n",
       " 'streets': 763,\n",
       " 'worst': 764,\n",
       " 'general': 765,\n",
       " 'phantasm': 766,\n",
       " 'ii': 767,\n",
       " 'begin': 768,\n",
       " 'understood': 769,\n",
       " 'dialog': 770,\n",
       " 'want': 771,\n",
       " 'colors': 772,\n",
       " 'cinematography': 773,\n",
       " 'dreadful': 774,\n",
       " 'now': 775,\n",
       " 'which': 776,\n",
       " 'caused': 777,\n",
       " 'downfall': 778,\n",
       " 'production': 779,\n",
       " 'values': 780,\n",
       " 'care': 781,\n",
       " 'jane': 782,\n",
       " 'rochester': 783,\n",
       " 'collide': 784,\n",
       " 'playing': 785,\n",
       " 'different': 786,\n",
       " 'lesson': 787,\n",
       " 'abject': 788,\n",
       " 'failure': 789,\n",
       " 'difficult': 790,\n",
       " 'categorize': 791,\n",
       " 'havent': 792,\n",
       " 'explained': 793,\n",
       " 'itself': 794,\n",
       " 'bit': 795,\n",
       " 'moving': 796,\n",
       " 'enjoy': 797,\n",
       " 'kind': 798,\n",
       " 'sitcom': 799,\n",
       " 'excellent': 800,\n",
       " 'combat': 801,\n",
       " 'fighting': 802,\n",
       " 'wasnt': 803,\n",
       " 'wanted': 804,\n",
       " 'still': 805,\n",
       " 'mugs': 806,\n",
       " 'carries': 807,\n",
       " 'fit': 808,\n",
       " 'gung': 809,\n",
       " 'ho': 810,\n",
       " 'least': 811,\n",
       " 'effects': 812,\n",
       " 'crossed': 813,\n",
       " 'lost': 814,\n",
       " 'genius': 815,\n",
       " 'delivered': 816,\n",
       " 'heck': 817,\n",
       " 'pans': 818,\n",
       " 'labyrinth': 819,\n",
       " 'scarier': 820,\n",
       " 'highly': 821,\n",
       " 'recommend': 822,\n",
       " 'seeing': 823,\n",
       " 'while': 824,\n",
       " 'sleeping': 825,\n",
       " 'happen': 826,\n",
       " 'indeed': 827,\n",
       " 'revolt': 828,\n",
       " 'zombies': 829,\n",
       " 'dynamics': 830,\n",
       " 'directing': 831,\n",
       " 'wonderful': 832,\n",
       " 'vow': 833,\n",
       " 'cherish': 834,\n",
       " 'spoilers': 835,\n",
       " 'ahead': 836,\n",
       " 'saying': 837,\n",
       " 'dies': 838,\n",
       " 'hints': 839,\n",
       " 'welcome': 840,\n",
       " 'same': 841,\n",
       " 'viewing': 842,\n",
       " 'boobs': 843,\n",
       " 'solid': 844,\n",
       " 'unremarkable': 845,\n",
       " 'stay': 846,\n",
       " 'works': 847,\n",
       " 'history': 848,\n",
       " 'butch': 849,\n",
       " 'straight': 850,\n",
       " 'white': 851,\n",
       " 'male': 852,\n",
       " 'esp': 853,\n",
       " 'tee': 854,\n",
       " 'hee': 855,\n",
       " 'research': 856,\n",
       " 'virtually': 857,\n",
       " 'pop': 858,\n",
       " 'punk': 859,\n",
       " 'rock': 860,\n",
       " 'maybe': 861,\n",
       " 'isnt': 862,\n",
       " 'happened': 863,\n",
       " 'jesus': 864,\n",
       " 'responsible': 865,\n",
       " 'wins': 866,\n",
       " 'neighbor': 867,\n",
       " 'killing': 868,\n",
       " 'ugh': 869,\n",
       " 'michael': 870,\n",
       " 'caine': 871,\n",
       " 'perfect': 872,\n",
       " 'bartender': 873,\n",
       " 'bollywood': 874,\n",
       " 'needs': 875,\n",
       " 'madhuri': 876,\n",
       " 'kajol': 877,\n",
       " 'uh': 878,\n",
       " 'ehh': 879,\n",
       " 'drawn': 880,\n",
       " 'vonneguts': 881,\n",
       " 'experienced': 882,\n",
       " 'paper': 883,\n",
       " 'stealing': 884,\n",
       " 'rockin': 885,\n",
       " 'rollers': 886,\n",
       " 'ready': 887,\n",
       " 'nicely': 888,\n",
       " 'photographed': 889,\n",
       " 'directed': 890,\n",
       " 'checks': 891,\n",
       " 'lay': 892,\n",
       " 'land': 893,\n",
       " 'dialogue': 894,\n",
       " 'hokey': 895,\n",
       " 'embarrassment': 896,\n",
       " 'every': 897,\n",
       " 'scene': 898,\n",
       " 'filthy': 899,\n",
       " 'entertained': 900,\n",
       " 'finally': 901,\n",
       " 'home': 902,\n",
       " 'beyond': 903,\n",
       " 'fright': 904,\n",
       " 'ensues': 905,\n",
       " 'fulci': 906,\n",
       " 'experiments': 907,\n",
       " 'sci': 908,\n",
       " 'fi': 909,\n",
       " 'fails': 910,\n",
       " 'grader': 911,\n",
       " 'complaints': 912,\n",
       " 'meryl': 913,\n",
       " 'streep': 914,\n",
       " 'incredible': 915,\n",
       " 'canceled': 916,\n",
       " 'work': 917,\n",
       " 'expecting': 918,\n",
       " 'big': 919,\n",
       " 'huge': 920,\n",
       " 'glasses': 921,\n",
       " 'nifty': 922,\n",
       " 'heart': 923,\n",
       " 'pounding': 924,\n",
       " 'ok': 925,\n",
       " 'else': 926,\n",
       " 'offense': 927,\n",
       " 'impressed': 928,\n",
       " 'greatly': 929,\n",
       " 'agree': 930,\n",
       " 'answers': 931,\n",
       " 'chase': 932,\n",
       " 'murders': 933,\n",
       " 'occurring': 934,\n",
       " 'texas': 935,\n",
       " 'desert': 936,\n",
       " 'comment': 937,\n",
       " 'ended': 938,\n",
       " 'ago': 939,\n",
       " 'seventies': 940,\n",
       " 'nails': 941,\n",
       " 'premise': 942,\n",
       " 'price': 943,\n",
       " 'italian': 944,\n",
       " 'poor': 945,\n",
       " 'difference': 946,\n",
       " 'outtakes': 947,\n",
       " 'met': 948,\n",
       " 'expectations': 949,\n",
       " 'poe': 950,\n",
       " 'romantic': 951,\n",
       " 'intently': 952,\n",
       " 'horrendous': 953,\n",
       " 'almost': 954,\n",
       " 'pray': 955,\n",
       " 'deaths': 956,\n",
       " 'felt': 957,\n",
       " 'drop': 958,\n",
       " 'emotion': 959,\n",
       " 'throughout': 960,\n",
       " 'material': 961,\n",
       " 'mess': 962,\n",
       " 'fill': 963,\n",
       " 'collinwood': 964,\n",
       " 'disaster': 965,\n",
       " 'ways': 966,\n",
       " 'randolph': 967,\n",
       " 'scott': 968,\n",
       " 'famous': 969,\n",
       " 'expression': 970,\n",
       " 'wowsers': 971,\n",
       " 'cry': 972,\n",
       " 'rushed': 973,\n",
       " 'utter': 974,\n",
       " 'uttter': 975,\n",
       " 'unbelievable': 976,\n",
       " 'plan': 977,\n",
       " 'resources': 978,\n",
       " 'ones': 979,\n",
       " 'either': 980,\n",
       " 'suspect': 981,\n",
       " 'naked': 982,\n",
       " 'blood': 983,\n",
       " 'drink': 984,\n",
       " 'wonderfully': 985,\n",
       " 'politically': 986,\n",
       " 'incorrect': 987,\n",
       " 'favourite': 988,\n",
       " 'poking': 989,\n",
       " 'corny': 990,\n",
       " 'cases': 991,\n",
       " 'appalling': 992,\n",
       " 'spellbinding': 993,\n",
       " 'matters': 994,\n",
       " 'going': 995,\n",
       " 'loving': 996,\n",
       " 'judy': 997,\n",
       " 'garland': 998,\n",
       " 'extremely': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'because',\n",
       " 5: 'mj',\n",
       " 6: 'overheard',\n",
       " 7: 'his',\n",
       " 8: 'plans',\n",
       " 9: 'gary',\n",
       " 10: 'busey',\n",
       " 11: 'kicks',\n",
       " 12: 'their',\n",
       " 13: 'butts',\n",
       " 14: 'of',\n",
       " 15: 'course',\n",
       " 16: 'a',\n",
       " 17: 'very',\n",
       " 18: 'fine',\n",
       " 19: 'film',\n",
       " 20: 'mostly',\n",
       " 21: 'an',\n",
       " 22: 'amateur',\n",
       " 23: 'with',\n",
       " 24: 'lame',\n",
       " 25: 'fx',\n",
       " 26: 'is',\n",
       " 27: 'it',\n",
       " 28: 'funny',\n",
       " 29: 'or',\n",
       " 30: 'b',\n",
       " 31: 'even',\n",
       " 32: 'when',\n",
       " 33: 'has',\n",
       " 34: 'already',\n",
       " 35: 'been',\n",
       " 36: 'written',\n",
       " 37: 'for',\n",
       " 38: 'them',\n",
       " 39: '!',\n",
       " 40: 'but',\n",
       " 41: 'wow',\n",
       " 42: 'this',\n",
       " 43: 'movie',\n",
       " 44: 'was',\n",
       " 45: 'absolutely',\n",
       " 46: 'awful',\n",
       " 47: 'veronica',\n",
       " 48: 'mars',\n",
       " 49: 'bore',\n",
       " 50: 'what',\n",
       " 51: 'crappy',\n",
       " 52: 'could',\n",
       " 53: 'have',\n",
       " 54: 'much',\n",
       " 55: 'better',\n",
       " 56: 'backwards',\n",
       " 57: 'evil',\n",
       " 58: 'messages',\n",
       " 59: 'played',\n",
       " 60: 'on',\n",
       " 61: 'vinyl',\n",
       " 62: 'the',\n",
       " 63: 'story',\n",
       " 64: 'absorbing',\n",
       " 65: 'forget',\n",
       " 66: 'historical',\n",
       " 67: 'reality',\n",
       " 68: 'too',\n",
       " 69: 'long',\n",
       " 70: 'boring',\n",
       " 71: 'i',\n",
       " 72: 'didnt',\n",
       " 73: 'read',\n",
       " 74: 'book',\n",
       " 75: 'well',\n",
       " 76: 'done',\n",
       " 77: 'al',\n",
       " 78: 'gore',\n",
       " 79: 'not',\n",
       " 80: 'you',\n",
       " 81: 'also',\n",
       " 82: 'certainly',\n",
       " 83: 'wondered',\n",
       " 84: 'how',\n",
       " 85: 'things',\n",
       " 86: 'went',\n",
       " 87: 'actions',\n",
       " 88: 'speak',\n",
       " 89: 'louder',\n",
       " 90: 'than',\n",
       " 91: 'words',\n",
       " 92: 'its',\n",
       " 93: 'good',\n",
       " 94: 'minor',\n",
       " 95: 'flaw',\n",
       " 96: 'easily',\n",
       " 97: 'ignored',\n",
       " 98: 'yes',\n",
       " 99: 'way',\n",
       " 100: 'does',\n",
       " 101: 'show',\n",
       " 102: 'bad',\n",
       " 103: 'times',\n",
       " 104: 'and',\n",
       " 105: 'actors',\n",
       " 106: 'baaaaaad',\n",
       " 107: 'line',\n",
       " 108: 'script',\n",
       " 109: 'monstervision',\n",
       " 110: 'grew',\n",
       " 111: 'up',\n",
       " 112: 'something',\n",
       " 113: 'weird',\n",
       " 114: 'definitely',\n",
       " 115: 'cheaply',\n",
       " 116: 'made',\n",
       " 117: 'doesnt',\n",
       " 118: 'stop',\n",
       " 119: 'there',\n",
       " 120: 'sadly',\n",
       " 121: 'in',\n",
       " 122: 'my',\n",
       " 123: 'case',\n",
       " 124: 'dvd',\n",
       " 125: 'enjoyed',\n",
       " 126: 'acting',\n",
       " 127: 'moved',\n",
       " 128: 'along',\n",
       " 129: 'spring',\n",
       " 130: 'byington',\n",
       " 131: 'seen',\n",
       " 132: 'as',\n",
       " 133: 'emily',\n",
       " 134: 'mother',\n",
       " 135: 'however',\n",
       " 136: 'being',\n",
       " 137: 'fan',\n",
       " 138: 'couldnt',\n",
       " 139: 'resist',\n",
       " 140: 'only',\n",
       " 141: 'really',\n",
       " 142: 'denero',\n",
       " 143: 'fans',\n",
       " 144: 'bar',\n",
       " 145: 'none',\n",
       " 146: 'best',\n",
       " 147: 'ive',\n",
       " 148: 'ever',\n",
       " 149: 'to',\n",
       " 150: 'answer',\n",
       " 151: 'your',\n",
       " 152: 'question',\n",
       " 153: 'he',\n",
       " 154: 'definately',\n",
       " 155: 'one',\n",
       " 156: 'all',\n",
       " 157: 'time',\n",
       " 158: 'greats',\n",
       " 159: 'domestic',\n",
       " 160: 'import',\n",
       " 161: 'great',\n",
       " 162: 'thought',\n",
       " 163: 'fantastic',\n",
       " 164: 'second',\n",
       " 165: 'cliched',\n",
       " 166: 'that',\n",
       " 167: 'necessary',\n",
       " 168: 'never',\n",
       " 169: 'little',\n",
       " 170: 'plot',\n",
       " 171: 'makes',\n",
       " 172: 'any',\n",
       " 173: 'sense',\n",
       " 174: 'mplayer',\n",
       " 175: 'managed',\n",
       " 176: 'run',\n",
       " 177: 'flawlessly',\n",
       " 178: 'did',\n",
       " 179: 'she',\n",
       " 180: 'make',\n",
       " 181: 'cover',\n",
       " 182: 'who',\n",
       " 183: 'came',\n",
       " 184: 'idea',\n",
       " 185: 'fun',\n",
       " 186: 'crap',\n",
       " 187: 'entertaining',\n",
       " 188: 'heartwarming',\n",
       " 189: 'mark',\n",
       " 190: 'dixon',\n",
       " 191: 'many',\n",
       " 192: 'troops',\n",
       " 193: 'just',\n",
       " 194: 'flat',\n",
       " 195: 'out',\n",
       " 196: 'unwatchable',\n",
       " 197: 'soothing',\n",
       " 198: 'otherwise',\n",
       " 199: 'sucked',\n",
       " 200: 'repugnant',\n",
       " 201: 'bronson',\n",
       " 202: 'thriller',\n",
       " 203: 'most',\n",
       " 204: 'comments',\n",
       " 205: 'favorite',\n",
       " 206: 'columbo',\n",
       " 207: 'episodes',\n",
       " 208: 'more',\n",
       " 209: 'extreme',\n",
       " 210: 'loved',\n",
       " 211: 'since',\n",
       " 212: 'young',\n",
       " 213: 'please',\n",
       " 214: 'watch',\n",
       " 215: 'responsibly',\n",
       " 216: 'racist',\n",
       " 217: 'offensive',\n",
       " 218: 'midgets',\n",
       " 219: 'go',\n",
       " 220: 'figure',\n",
       " 221: 'pretty',\n",
       " 222: 'terrible',\n",
       " 223: 'stuff',\n",
       " 224: 'characters',\n",
       " 225: 'are',\n",
       " 226: 'payroll',\n",
       " 227: 'apparently',\n",
       " 228: 'essence',\n",
       " 229: 'dao',\n",
       " 230: 'uses',\n",
       " 231: 'nice',\n",
       " 232: 'overall',\n",
       " 233: 'worth',\n",
       " 234: 'no',\n",
       " 235: 'value',\n",
       " 236: 'watching',\n",
       " 237: 'whole',\n",
       " 238: 'family',\n",
       " 239: 'special',\n",
       " 240: 'surprise',\n",
       " 241: 'found',\n",
       " 242: 'enjoyable',\n",
       " 243: 'girl',\n",
       " 244: 'okay',\n",
       " 245: 'job',\n",
       " 246: 'remember',\n",
       " 247: 'promise',\n",
       " 248: 'nuclear',\n",
       " 249: 'energy',\n",
       " 250: 'me',\n",
       " 251: 'taking',\n",
       " 252: 'chance',\n",
       " 253: 'total',\n",
       " 254: 'fabrication',\n",
       " 255: 'emma',\n",
       " 256: 'roberts',\n",
       " 257: 'adorable',\n",
       " 258: 'title',\n",
       " 259: 'role',\n",
       " 260: 'creative',\n",
       " 261: 'talented',\n",
       " 262: 'cast',\n",
       " 263: 'hundred',\n",
       " 264: 'worse',\n",
       " 265: 'waste',\n",
       " 266: 'money',\n",
       " 267: 'celluloid',\n",
       " 268: 'jean',\n",
       " 269: 'arthur',\n",
       " 270: 'shines',\n",
       " 271: 'looks',\n",
       " 272: 'at',\n",
       " 273: 'powell',\n",
       " 274: 'matthias',\n",
       " 275: 'hues',\n",
       " 276: 'always',\n",
       " 277: 'theres',\n",
       " 278: 'substance',\n",
       " 279: 'underneath',\n",
       " 280: 'believe',\n",
       " 281: 'wrong',\n",
       " 282: 'choice',\n",
       " 283: 'songs',\n",
       " 284: 'dont',\n",
       " 285: 'scottish',\n",
       " 286: 'locale',\n",
       " 287: 'wasted',\n",
       " 288: 'rest',\n",
       " 289: 'frazetta',\n",
       " 290: 'motion',\n",
       " 291: 'kudos',\n",
       " 292: 'everyone',\n",
       " 293: 'involved',\n",
       " 294: 'see',\n",
       " 295: 'yourself',\n",
       " 296: 'plenty',\n",
       " 297: 'cameos',\n",
       " 298: 'by',\n",
       " 299: 'real',\n",
       " 300: 'life',\n",
       " 301: 'porno',\n",
       " 302: 'stars',\n",
       " 303: 'indian',\n",
       " 304: 'summer',\n",
       " 305: 'flawless',\n",
       " 306: 'remains',\n",
       " 307: 'gray',\n",
       " 308: 'far',\n",
       " 309: 'true',\n",
       " 310: 'decameron',\n",
       " 311: 'wicker',\n",
       " 312: 'man',\n",
       " 313: 'handled',\n",
       " 314: 'badly',\n",
       " 315: 'people',\n",
       " 316: 'love',\n",
       " 317: 'these',\n",
       " 318: 'after',\n",
       " 319: 'had',\n",
       " 320: 'landau',\n",
       " 321: 'cant',\n",
       " 322: 'salvage',\n",
       " 323: 'complete',\n",
       " 324: 'superb',\n",
       " 325: 'blair',\n",
       " 326: 'witch',\n",
       " 327: 'proved',\n",
       " 328: 'everything',\n",
       " 329: 'documentary',\n",
       " 330: 'be',\n",
       " 331: 'horrible',\n",
       " 332: 'schlock',\n",
       " 333: 'natural',\n",
       " 334: 'genuine',\n",
       " 335: 'might',\n",
       " 336: 'first',\n",
       " 337: 'episode',\n",
       " 338: 'ask',\n",
       " 339: 'pointless',\n",
       " 340: 'points',\n",
       " 341: 'magnificent',\n",
       " 342: 'music',\n",
       " 343: 'score',\n",
       " 344: 'surprised',\n",
       " 345: 'falling',\n",
       " 346: 'pencil',\n",
       " 347: 'extra',\n",
       " 348: 'impact',\n",
       " 349: 'will',\n",
       " 350: 'god',\n",
       " 351: 'forgive',\n",
       " 352: 'us',\n",
       " 353: 'weve',\n",
       " 354: 'some',\n",
       " 355: 'punishments',\n",
       " 356: 'were',\n",
       " 357: 'so',\n",
       " 358: 'ridiculous',\n",
       " 359: 'tony',\n",
       " 360: 'hides',\n",
       " 361: 'her',\n",
       " 362: 'west',\n",
       " 363: 'moment',\n",
       " 364: 'hollywood',\n",
       " 365: 'riots',\n",
       " 366: 'between',\n",
       " 367: 'robots',\n",
       " 368: 'mankind',\n",
       " 369: 'quite',\n",
       " 370: 'year',\n",
       " 371: 'old',\n",
       " 372: 'lyrical',\n",
       " 373: 'relax',\n",
       " 374: 'low',\n",
       " 375: 'budget',\n",
       " 376: 'david',\n",
       " 377: 'lean',\n",
       " 378: 'knew',\n",
       " 379: 'films',\n",
       " 380: 'from',\n",
       " 381: 'wont',\n",
       " 382: 'get',\n",
       " 383: 'into',\n",
       " 384: 'demonicus',\n",
       " 385: 'turned',\n",
       " 386: 'video',\n",
       " 387: 'game',\n",
       " 388: 'refuse',\n",
       " 389: 'check',\n",
       " 390: 'watched',\n",
       " 391: 'hulu',\n",
       " 392: 'commercials',\n",
       " 393: 'know',\n",
       " 394: 'do',\n",
       " 395: 'eddie',\n",
       " 396: 'murphy',\n",
       " 397: 'peaks',\n",
       " 398: 'during',\n",
       " 399: 'movies',\n",
       " 400: 'here',\n",
       " 401: 'can',\n",
       " 402: 'tell',\n",
       " 403: 'arent',\n",
       " 404: 'right',\n",
       " 405: 'police',\n",
       " 406: 'sounds',\n",
       " 407: 'rather',\n",
       " 408: 'spoken',\n",
       " 409: 'beautifully',\n",
       " 410: 'shot',\n",
       " 411: 'think',\n",
       " 412: 'leave',\n",
       " 413: 'laughing',\n",
       " 414: 'diana',\n",
       " 415: 'guzman',\n",
       " 416: 'angry',\n",
       " 417: 'woman',\n",
       " 418: 'move',\n",
       " 419: 'tv',\n",
       " 420: 'last',\n",
       " 421: 'night',\n",
       " 422: 'thing',\n",
       " 423: 'remarkable',\n",
       " 424: 'about',\n",
       " 425: 'someone',\n",
       " 426: 'owed',\n",
       " 427: 'royalties',\n",
       " 428: 'raw',\n",
       " 429: 'look',\n",
       " 430: 'changes',\n",
       " 431: 'obsession',\n",
       " 432: 'grows',\n",
       " 433: 'outside',\n",
       " 434: 'gaming',\n",
       " 435: 'session',\n",
       " 436: 'proves',\n",
       " 437: 'theory',\n",
       " 438: 'star',\n",
       " 439: 'power',\n",
       " 440: 'mob',\n",
       " 441: 'three',\n",
       " 442: 'recommended',\n",
       " 443: 'friend',\n",
       " 444: 'connecting',\n",
       " 445: 'tedious',\n",
       " 446: 'twilight',\n",
       " 447: 'zone',\n",
       " 448: 'got',\n",
       " 449: 'like',\n",
       " 450: 'blade',\n",
       " 451: 'runner',\n",
       " 452: 'liked',\n",
       " 453: 'especially',\n",
       " 454: 'lexa',\n",
       " 455: 'doigs',\n",
       " 456: 'downhill',\n",
       " 457: 'russian',\n",
       " 458: 'norwegian',\n",
       " 459: 'why',\n",
       " 460: 'garbage',\n",
       " 461: 'afghanistan',\n",
       " 462: 'kid',\n",
       " 463: 'videos',\n",
       " 464: 'avoid',\n",
       " 465: 'lavish',\n",
       " 466: 'fantasy',\n",
       " 467: 'style',\n",
       " 468: 'hate',\n",
       " 469: 'talent',\n",
       " 470: 'kept',\n",
       " 471: 'attention',\n",
       " 472: 'start',\n",
       " 473: 'finish',\n",
       " 474: 'theyre',\n",
       " 475: 'both',\n",
       " 476: 'damn',\n",
       " 477: 'listen',\n",
       " 478: 'knox',\n",
       " 479: 'nazi',\n",
       " 480: 'brilliant',\n",
       " 481: 'where',\n",
       " 482: 'we',\n",
       " 483: 'heard',\n",
       " 484: 'before',\n",
       " 485: 'am',\n",
       " 486: 'bakhtiari',\n",
       " 487: 'need',\n",
       " 488: 'industry',\n",
       " 489: 'hard',\n",
       " 490: 'come',\n",
       " 491: 'hermann',\n",
       " 492: 'levi',\n",
       " 493: 'conductor',\n",
       " 494: 'sound',\n",
       " 495: 'sometimes',\n",
       " 496: 'dubbed',\n",
       " 497: 'describe',\n",
       " 498: 'exactly',\n",
       " 499: 'worthy',\n",
       " 500: 'barrymores',\n",
       " 501: 'beauty',\n",
       " 502: 'sounded',\n",
       " 503: 'asian',\n",
       " 504: 'jewish',\n",
       " 505: 'marshall',\n",
       " 506: 'miles',\n",
       " 507: 'safe',\n",
       " 508: 'disappointing',\n",
       " 509: 'norris',\n",
       " 510: 'beg',\n",
       " 511: 'roslyn',\n",
       " 512: 'sanchez',\n",
       " 513: 'looked',\n",
       " 514: 'forward',\n",
       " 515: 'those',\n",
       " 516: 'days',\n",
       " 517: 'spit',\n",
       " 518: 'say',\n",
       " 519: 'superior',\n",
       " 520: 'feel',\n",
       " 521: 'couple',\n",
       " 522: 'hours',\n",
       " 523: 'impossible',\n",
       " 524: 'greetings',\n",
       " 525: 'again',\n",
       " 526: 'darkness',\n",
       " 527: 'evening',\n",
       " 528: 'disappointed',\n",
       " 529: 'framing',\n",
       " 530: 'thats',\n",
       " 531: 'cheap',\n",
       " 532: 'endearing',\n",
       " 533: 'xizao',\n",
       " 534: 'rare',\n",
       " 535: 'sad',\n",
       " 536: 'around',\n",
       " 537: 'revisiting',\n",
       " 538: 'childhood',\n",
       " 539: 'mum',\n",
       " 540: 'would',\n",
       " 541: 'yelling',\n",
       " 542: 'atlanta',\n",
       " 543: 'nope',\n",
       " 544: 'wonder',\n",
       " 545: 'saw',\n",
       " 546: 'mill',\n",
       " 547: 'valley',\n",
       " 548: 'festival',\n",
       " 549: 'freshman',\n",
       " 550: 'rhetoric',\n",
       " 551: 'race',\n",
       " 552: 'gender',\n",
       " 553: 'then',\n",
       " 554: 'disprove',\n",
       " 555: 'beats',\n",
       " 556: 'cute',\n",
       " 557: 'playful',\n",
       " 558: 'levels',\n",
       " 559: 'tough',\n",
       " 560: 'other',\n",
       " 561: 'hand',\n",
       " 562: 'adored',\n",
       " 563: 'literary',\n",
       " 564: 'asked',\n",
       " 565: 'refund',\n",
       " 566: 'oh',\n",
       " 567: 'yeah',\n",
       " 568: 'robotic',\n",
       " 569: 'regimented',\n",
       " 570: 'werent',\n",
       " 571: 'enough',\n",
       " 572: 'bo',\n",
       " 573: 'welch',\n",
       " 574: 'should',\n",
       " 575: 'fired',\n",
       " 576: 'writer',\n",
       " 577: 'shows',\n",
       " 578: 'another',\n",
       " 579: 'final',\n",
       " 580: 'justice',\n",
       " 581: 'word',\n",
       " 582: 'amazing',\n",
       " 583: 'heavens',\n",
       " 584: 'im',\n",
       " 585: 'sure',\n",
       " 586: 'lot',\n",
       " 587: 'writers',\n",
       " 588: 'television',\n",
       " 589: 'cinema',\n",
       " 590: 'pia',\n",
       " 591: 'barely',\n",
       " 592: 'talk',\n",
       " 593: 'let',\n",
       " 594: 'alone',\n",
       " 595: 'write',\n",
       " 596: 'unspeakably',\n",
       " 597: 'anything',\n",
       " 598: 'over',\n",
       " 599: 'supposed',\n",
       " 600: 'prove',\n",
       " 601: 'anyway',\n",
       " 602: 'may',\n",
       " 603: 'laughed',\n",
       " 604: 'jim',\n",
       " 605: 'carrey',\n",
       " 606: 'beautiful',\n",
       " 607: 'become',\n",
       " 608: 'naive',\n",
       " 609: 'string',\n",
       " 610: 'several',\n",
       " 611: 'jokes',\n",
       " 612: 'acted',\n",
       " 613: 'sexual',\n",
       " 614: 'tension',\n",
       " 615: 'sooo',\n",
       " 616: 'scary',\n",
       " 617: 'devilish',\n",
       " 618: 'guess',\n",
       " 619: 'bayless',\n",
       " 620: 'they',\n",
       " 621: 'seek',\n",
       " 622: 'suppress',\n",
       " 623: 'invention',\n",
       " 624: 'conventional',\n",
       " 625: 'means',\n",
       " 626: 'doctor',\n",
       " 627: 'possibilities',\n",
       " 628: 'says',\n",
       " 629: 'bookkeeper',\n",
       " 630: 'steve',\n",
       " 631: 'noisy',\n",
       " 632: 'starts',\n",
       " 633: 'singing',\n",
       " 634: 'glad',\n",
       " 635: 'air',\n",
       " 636: 'through',\n",
       " 637: 'end',\n",
       " 638: 'pure',\n",
       " 639: 'unadulterated',\n",
       " 640: 'violence',\n",
       " 641: 'take',\n",
       " 642: 'him',\n",
       " 643: 'seriously',\n",
       " 644: 'winner',\n",
       " 645: 'give',\n",
       " 646: 'ten',\n",
       " 647: 'ice',\n",
       " 648: 'cream',\n",
       " 649: 'spread',\n",
       " 650: 'crackers',\n",
       " 651: 'told',\n",
       " 652: 'thousand',\n",
       " 653: 'setting',\n",
       " 654: 'mentioning',\n",
       " 655: 'two',\n",
       " 656: 'lead',\n",
       " 657: 'roles',\n",
       " 658: 'performed',\n",
       " 659: 'perfection',\n",
       " 660: 'school',\n",
       " 661: 'outdated',\n",
       " 662: 'nothing',\n",
       " 663: 'remotely',\n",
       " 664: 'interesting',\n",
       " 665: 'happens',\n",
       " 666: 'due',\n",
       " 667: 'jamie',\n",
       " 668: 'foxx',\n",
       " 669: 'huzzah',\n",
       " 670: 'that´s',\n",
       " 671: 'basically',\n",
       " 672: 'direction',\n",
       " 673: 'must',\n",
       " 674: 'faulted',\n",
       " 675: 'if',\n",
       " 676: 'wouldve',\n",
       " 677: 'try',\n",
       " 678: 'such',\n",
       " 679: 'piece',\n",
       " 680: 'page',\n",
       " 681: 'point',\n",
       " 682: 'eludes',\n",
       " 683: 'scenes',\n",
       " 684: 'getters',\n",
       " 685: 'sherry',\n",
       " 686: 'baby',\n",
       " 687: 'week',\n",
       " 688: 'freaked',\n",
       " 689: 'verdict',\n",
       " 690: 'james',\n",
       " 691: 'garner',\n",
       " 692: 'addition',\n",
       " 693: 'sent',\n",
       " 694: 'stunt',\n",
       " 695: 'double',\n",
       " 696: 'high',\n",
       " 697: 'hopes',\n",
       " 698: 'won',\n",
       " 699: 'raves',\n",
       " 700: 'disappeared',\n",
       " 701: 'ramsey',\n",
       " 702: 'bring',\n",
       " 703: 'callar',\n",
       " 704: 'appears',\n",
       " 705: 'england',\n",
       " 706: 'dull',\n",
       " 707: 'doorknob',\n",
       " 708: 'dust',\n",
       " 709: 'world',\n",
       " 710: 'place',\n",
       " 711: 'voices',\n",
       " 712: 'often',\n",
       " 713: 'distorted',\n",
       " 714: 'reason',\n",
       " 715: 'stellan',\n",
       " 716: 'skarsgård',\n",
       " 717: 'cameo',\n",
       " 718: 'pleasantly',\n",
       " 719: 'flaws',\n",
       " 720: 'spill',\n",
       " 721: 'hilarious',\n",
       " 722: 'fear',\n",
       " 723: 'black',\n",
       " 724: 'hat',\n",
       " 725: 'superbly',\n",
       " 726: 'crafted',\n",
       " 727: 'pleased',\n",
       " 728: 'hair',\n",
       " 729: 'mean',\n",
       " 730: 'snipes',\n",
       " 731: 'greg',\n",
       " 732: 'kinnear',\n",
       " 733: 'chicago',\n",
       " 734: 'international',\n",
       " 735: 'kolchak',\n",
       " 736: 'scenery',\n",
       " 737: 'wheres',\n",
       " 738: 'kleinfeld',\n",
       " 739: 'father',\n",
       " 740: 'cares',\n",
       " 741: 'taken',\n",
       " 742: 'back',\n",
       " 743: 'asks',\n",
       " 744: 'touched',\n",
       " 745: 'years',\n",
       " 746: 'few',\n",
       " 747: 'updates',\n",
       " 748: 'seemed',\n",
       " 749: 'suspicious',\n",
       " 750: 'results',\n",
       " 751: 'unabsorbing',\n",
       " 752: 'tracking',\n",
       " 753: 'cuts',\n",
       " 754: 'slow',\n",
       " 755: 'sequel',\n",
       " 756: 'murder',\n",
       " 757: 'small',\n",
       " 758: 'town',\n",
       " 759: 'performance',\n",
       " 760: 'drive',\n",
       " 761: 'typical',\n",
       " 762: 'jaipur',\n",
       " 763: 'streets',\n",
       " 764: 'worst',\n",
       " 765: 'general',\n",
       " 766: 'phantasm',\n",
       " 767: 'ii',\n",
       " 768: 'begin',\n",
       " 769: 'understood',\n",
       " 770: 'dialog',\n",
       " 771: 'want',\n",
       " 772: 'colors',\n",
       " 773: 'cinematography',\n",
       " 774: 'dreadful',\n",
       " 775: 'now',\n",
       " 776: 'which',\n",
       " 777: 'caused',\n",
       " 778: 'downfall',\n",
       " 779: 'production',\n",
       " 780: 'values',\n",
       " 781: 'care',\n",
       " 782: 'jane',\n",
       " 783: 'rochester',\n",
       " 784: 'collide',\n",
       " 785: 'playing',\n",
       " 786: 'different',\n",
       " 787: 'lesson',\n",
       " 788: 'abject',\n",
       " 789: 'failure',\n",
       " 790: 'difficult',\n",
       " 791: 'categorize',\n",
       " 792: 'havent',\n",
       " 793: 'explained',\n",
       " 794: 'itself',\n",
       " 795: 'bit',\n",
       " 796: 'moving',\n",
       " 797: 'enjoy',\n",
       " 798: 'kind',\n",
       " 799: 'sitcom',\n",
       " 800: 'excellent',\n",
       " 801: 'combat',\n",
       " 802: 'fighting',\n",
       " 803: 'wasnt',\n",
       " 804: 'wanted',\n",
       " 805: 'still',\n",
       " 806: 'mugs',\n",
       " 807: 'carries',\n",
       " 808: 'fit',\n",
       " 809: 'gung',\n",
       " 810: 'ho',\n",
       " 811: 'least',\n",
       " 812: 'effects',\n",
       " 813: 'crossed',\n",
       " 814: 'lost',\n",
       " 815: 'genius',\n",
       " 816: 'delivered',\n",
       " 817: 'heck',\n",
       " 818: 'pans',\n",
       " 819: 'labyrinth',\n",
       " 820: 'scarier',\n",
       " 821: 'highly',\n",
       " 822: 'recommend',\n",
       " 823: 'seeing',\n",
       " 824: 'while',\n",
       " 825: 'sleeping',\n",
       " 826: 'happen',\n",
       " 827: 'indeed',\n",
       " 828: 'revolt',\n",
       " 829: 'zombies',\n",
       " 830: 'dynamics',\n",
       " 831: 'directing',\n",
       " 832: 'wonderful',\n",
       " 833: 'vow',\n",
       " 834: 'cherish',\n",
       " 835: 'spoilers',\n",
       " 836: 'ahead',\n",
       " 837: 'saying',\n",
       " 838: 'dies',\n",
       " 839: 'hints',\n",
       " 840: 'welcome',\n",
       " 841: 'same',\n",
       " 842: 'viewing',\n",
       " 843: 'boobs',\n",
       " 844: 'solid',\n",
       " 845: 'unremarkable',\n",
       " 846: 'stay',\n",
       " 847: 'works',\n",
       " 848: 'history',\n",
       " 849: 'butch',\n",
       " 850: 'straight',\n",
       " 851: 'white',\n",
       " 852: 'male',\n",
       " 853: 'esp',\n",
       " 854: 'tee',\n",
       " 855: 'hee',\n",
       " 856: 'research',\n",
       " 857: 'virtually',\n",
       " 858: 'pop',\n",
       " 859: 'punk',\n",
       " 860: 'rock',\n",
       " 861: 'maybe',\n",
       " 862: 'isnt',\n",
       " 863: 'happened',\n",
       " 864: 'jesus',\n",
       " 865: 'responsible',\n",
       " 866: 'wins',\n",
       " 867: 'neighbor',\n",
       " 868: 'killing',\n",
       " 869: 'ugh',\n",
       " 870: 'michael',\n",
       " 871: 'caine',\n",
       " 872: 'perfect',\n",
       " 873: 'bartender',\n",
       " 874: 'bollywood',\n",
       " 875: 'needs',\n",
       " 876: 'madhuri',\n",
       " 877: 'kajol',\n",
       " 878: 'uh',\n",
       " 879: 'ehh',\n",
       " 880: 'drawn',\n",
       " 881: 'vonneguts',\n",
       " 882: 'experienced',\n",
       " 883: 'paper',\n",
       " 884: 'stealing',\n",
       " 885: 'rockin',\n",
       " 886: 'rollers',\n",
       " 887: 'ready',\n",
       " 888: 'nicely',\n",
       " 889: 'photographed',\n",
       " 890: 'directed',\n",
       " 891: 'checks',\n",
       " 892: 'lay',\n",
       " 893: 'land',\n",
       " 894: 'dialogue',\n",
       " 895: 'hokey',\n",
       " 896: 'embarrassment',\n",
       " 897: 'every',\n",
       " 898: 'scene',\n",
       " 899: 'filthy',\n",
       " 900: 'entertained',\n",
       " 901: 'finally',\n",
       " 902: 'home',\n",
       " 903: 'beyond',\n",
       " 904: 'fright',\n",
       " 905: 'ensues',\n",
       " 906: 'fulci',\n",
       " 907: 'experiments',\n",
       " 908: 'sci',\n",
       " 909: 'fi',\n",
       " 910: 'fails',\n",
       " 911: 'grader',\n",
       " 912: 'complaints',\n",
       " 913: 'meryl',\n",
       " 914: 'streep',\n",
       " 915: 'incredible',\n",
       " 916: 'canceled',\n",
       " 917: 'work',\n",
       " 918: 'expecting',\n",
       " 919: 'big',\n",
       " 920: 'huge',\n",
       " 921: 'glasses',\n",
       " 922: 'nifty',\n",
       " 923: 'heart',\n",
       " 924: 'pounding',\n",
       " 925: 'ok',\n",
       " 926: 'else',\n",
       " 927: 'offense',\n",
       " 928: 'impressed',\n",
       " 929: 'greatly',\n",
       " 930: 'agree',\n",
       " 931: 'answers',\n",
       " 932: 'chase',\n",
       " 933: 'murders',\n",
       " 934: 'occurring',\n",
       " 935: 'texas',\n",
       " 936: 'desert',\n",
       " 937: 'comment',\n",
       " 938: 'ended',\n",
       " 939: 'ago',\n",
       " 940: 'seventies',\n",
       " 941: 'nails',\n",
       " 942: 'premise',\n",
       " 943: 'price',\n",
       " 944: 'italian',\n",
       " 945: 'poor',\n",
       " 946: 'difference',\n",
       " 947: 'outtakes',\n",
       " 948: 'met',\n",
       " 949: 'expectations',\n",
       " 950: 'poe',\n",
       " 951: 'romantic',\n",
       " 952: 'intently',\n",
       " 953: 'horrendous',\n",
       " 954: 'almost',\n",
       " 955: 'pray',\n",
       " 956: 'deaths',\n",
       " 957: 'felt',\n",
       " 958: 'drop',\n",
       " 959: 'emotion',\n",
       " 960: 'throughout',\n",
       " 961: 'material',\n",
       " 962: 'mess',\n",
       " 963: 'fill',\n",
       " 964: 'collinwood',\n",
       " 965: 'disaster',\n",
       " 966: 'ways',\n",
       " 967: 'randolph',\n",
       " 968: 'scott',\n",
       " 969: 'famous',\n",
       " 970: 'expression',\n",
       " 971: 'wowsers',\n",
       " 972: 'cry',\n",
       " 973: 'rushed',\n",
       " 974: 'utter',\n",
       " 975: 'uttter',\n",
       " 976: 'unbelievable',\n",
       " 977: 'plan',\n",
       " 978: 'resources',\n",
       " 979: 'ones',\n",
       " 980: 'either',\n",
       " 981: 'suspect',\n",
       " 982: 'naked',\n",
       " 983: 'blood',\n",
       " 984: 'drink',\n",
       " 985: 'wonderfully',\n",
       " 986: 'politically',\n",
       " 987: 'incorrect',\n",
       " 988: 'favourite',\n",
       " 989: 'poking',\n",
       " 990: 'corny',\n",
       " 991: 'cases',\n",
       " 992: 'appalling',\n",
       " 993: 'spellbinding',\n",
       " 994: 'matters',\n",
       " 995: 'going',\n",
       " 996: 'loving',\n",
       " 997: 'judy',\n",
       " 998: 'garland',\n",
       " 999: 'extremely',\n",
       " ...}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize words data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_words_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load char encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model = load_model(encoder_char_model_file.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 142)         20164     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection [(None, None, 512), (None 817152    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 837,316\n",
      "Trainable params: 817,152\n",
      "Non-trainable params: 20,164\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_word_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 15, 512)      837316      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 1574912     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1573888     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[0][0]                     \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[0][0]               \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 15, 1024)     0           dot_4[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1537)     1575425     concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,709,317\n",
      "Trainable params: 8,689,153\n",
      "Non-trainable params: 20,164\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model, encoder_sentence_embedding_model = build_words2sent_model(encoder_word_embedding_model=encoder_word_embedding_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 15, 512)      837316      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 1574912     time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 2,412,228\n",
      "Trainable params: 2,392,064\n",
      "Non-trainable params: 20,164\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1573888     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 15, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[1][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[1][0]               \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 15, 1024)     0           dot_4[1][0]                      \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1537)     1575425     concatenate_6[1][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,297,089\n",
      "Trainable params: 6,297,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 15, 20)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 512)           837316    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection [(None, 15, 512), (None,  1574912   \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 2,412,228\n",
      "Trainable params: 2,392,064\n",
      "Non-trainable params: 20,164\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_sentence_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 5.3642 - categorical_accuracy: 0.1617 - val_loss: 4.7901 - val_categorical_accuracy: 0.2818\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.28183, saving model to best_hier_model-15-20.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_4/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_5/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f37581b7978>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_chars_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: bceause mj oevrheard his plans\n",
      "GT sentence: because mj overheard his plans?\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: gayr busey kicks their butts, of ousre\n",
      "GT sentence: gary busey kicks their butts, of course.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: a very firne fim\n",
      "GT sentence: a very fine film.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: motlym an amateur film with lame fx\n",
      "GT sentence: mostly an amateur film with lame fx.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: isit funn\n",
      "GT sentence: is it funny?\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: )funny, ro by\n",
      "GT sentence: )funny, or b.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: even rwhe it ha alreayd been writtetn for tbem\n",
      "GT sentence: even when it has already been written for them!\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: but wow, this movie was absoutley awfl\n",
      "GT sentence: but wow, this movie was absolutely awful.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: qveoricoa mras isa ohre\n",
      "GT sentence: veronica mars is a bore.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: what an absolutely crappy film this is\n",
      "GT sentence: what an absolutely crappy film this is.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: but iyt could hvae been mucyh bette\n",
      "GT sentence: but it could have been much better.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: backwards evil messaqgse played on vinyl\n",
      "GT sentence: backwards evil messages played on vinyl!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: the sstory is absorbing\n",
      "GT sentence: the story is absorbing.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: forget histrical reality too\n",
      "GT sentence: forget historical reality too.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: tcoo long,to oborin\n",
      "GT sentence: too long,too boring.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: i pdidnt ruead te boo\n",
      "GT sentence: i didnt read the book.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: well dne al ore\n",
      "GT sentence: well done al gore!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: it is notfzunny\n",
      "GT sentence: it is not funny.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: yuo yalso certanly owndgred how things went\n",
      "GT sentence: you also certainly wondered how things went.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: but actions speavk loder than words\n",
      "GT sentence: but actions speak louder than words!\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: its good\n",
      "GT sentence: its good.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: this minor flaw setasily ignored\n",
      "GT sentence: this minor flaw is easily ignored.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: yes way\n",
      "GT sentence: yes way.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: the movie doesn ot even sho wbda times\n",
      "GT sentence: the movie does not even show bad times.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: nad of cousre the actors\n",
      "GT sentence: and, of course the actors!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: but a baaaaaad tsory lie, a ad script\n",
      "GT sentence: but a baaaaaad story line, a bad script.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: mnotservison was a jhow i grew upn wirth\n",
      "GT sentence: monstervision was a show i grew up with.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: somethin weird is definitely cheaply tmade\n",
      "GT sentence: something weird is definitely cheaply made.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: bukt it doesnt sto pthebre\n",
      "GT sentence: but it doesnt stop there!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: sdaly, in my case, i hav the idvid\n",
      "GT sentence: sadly, in my case, i have the dvd.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: i emnjoyed the acting and it moved alomngw ell\n",
      "GT sentence: i enjoyed the acting and it moved along well.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: srping byington riso seena s emily ,fthe mother\n",
      "GT sentence: spring byington is seen as emily, the mother.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ohwever bing  fan i couldnt rresxist\n",
      "GT sentence: however being a fan i couldnt resist.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ba.\n",
      "GT sentence: bad.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: olnyf or really di-ehardd enero fans\n",
      "GT sentence: only for really die-hard denero fans!\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: this is bar none tthe best mvie iv ever sene\n",
      "GT sentence: this is bar none the best movie ive ever seen.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: to answer your question, yes\n",
      "GT sentence: to answer your question, yes!\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: he is dfeiynateuly one on the all htrime greats\n",
      "GT sentence: he is definately one of the all time greats.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: doemstic import wasa k great movi.\n",
      "GT sentence: domestic import was a great movie.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: i thoughtthis movie was fwantastic\n",
      "GT sentence: i thought this movie was fantastic.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: tbhe secaond story, i thougzht, wasz cliched\n",
      "GT sentence: the second story, i thought, was cliched.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: was that necessary?\n",
      "GT sentence: was that necessary?!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: never\n",
      "GT sentence: never.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: lttle in the plo tmakesany sense\n",
      "GT sentence: little in the plot makes any sense.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: mpmlayer managed oto urn it flawelssly\n",
      "GT sentence: mplayer managed to run it flawlessly.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: how did she mak eit on the cover!\n",
      "GT sentence: how did she make it on the cover!?\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: who gcae u with the idea to make thi\n",
      "GT sentence: who came up with the idea to make this?\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: whatu fun\n",
      "GT sentence: what fun!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: its wel lcrap\n",
      "GT sentence: its well crap.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: etnertaining acndheartwamrng\n",
      "GT sentence: entertaining and heartwarming.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: mark dixon\n",
      "GT sentence: mark dixon.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: how mkany troops\n",
      "GT sentence: how many troops?\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: this ris just flakt out unwatnchablem\n",
      "GT sentence: this is just flat out unwatchable.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence:  very soothinng show\n",
      "GT sentence: a very soothing show.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: but otherwise it sucked\n",
      "GT sentence: but otherwise it sucked.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: uepugnadnt bronson trilbler\n",
      "GT sentence: repugnant bronson thriller.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: iev rea mjos wof the comments on hs moie\n",
      "GT sentence: ive read most of the comments on this movie.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: in 1948 this was m al-tim efavorite movie\n",
      "GT sentence: in 1948 this was my all-time favorite movie.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: fthis is one of hte best columbo episodes ever\n",
      "GT sentence: this is one of the best columbo episodes ever.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: more extreme than necessary\n",
      "GT sentence: more extreme than necessary.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: this movie ve olved since  ws young\n",
      "GT sentence: this movie ive loved since i was young!\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: and please, owatch roesponsibly\n",
      "GT sentence: and please, watch responsibly.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: it is raist aknd egry offensivceto midgets\n",
      "GT sentence: it is racist and very offensive to midgets.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: guo figure\n",
      "GT sentence: go figure!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: prettyterribles tuff\n",
      "GT sentence: pretty terrible stuff.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: thke characters are great\n",
      "GT sentence: the characters are great.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ngo ton the payroll apparenly\n",
      "GT sentence: not on the payroll apparently.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: is the esesce of the dao\n",
      "GT sentence: is the essence of the dao.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: thef ilm uesw (nihce!\n",
      "GT sentence: the film uses (nice!)\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: wlel, yes, it is, buyt it isalso great nonp-.c\n",
      "GT sentence: well, yes, it is, but it is also great non-p.c.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ovearll well worth wthe watch!\n",
      "GT sentence: overall well worth the watch!.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: wthere is absolutely no value qi nwatching it\n",
      "GT sentence: there is absolutely no value in watching it.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: it is a gresat movie for the whole amily\n",
      "GT sentence: it is a great movie for the whole family.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: retty specila\n",
      "GT sentence: pretty special.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: tm ysurprisei  found uit enjyable\n",
      "GT sentence: to my surprise i found it enjoyable.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: the girld did na okya jb\n",
      "GT sentence: the girl did an okay job.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: remember teh promise of nfuclea energy\n",
      "GT sentence: remember the promise of nuclear energy?\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: and e, xfr taking a chance onthis\n",
      "GT sentence: and me, for taking a chance on this.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: tota lfabrication\n",
      "GT sentence: total fabrication.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: emma rvbertsw as aorable in thet itle rpole\n",
      "GT sentence: emma roberts was adorable in the title role.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: cerative story line wth  avery talentde casxt\n",
      "GT sentence: creative story line with a very talented cast.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: no, it dwas  ahundred tmies worse\n",
      "GT sentence: no, it was a hundred times worse.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: a aste of money, a waste of celluloid\n",
      "GT sentence: a waste of money, a waste of celluloid.\n",
      "Decoded sentence: i it UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: jae arthur shines whlen she looks at powel.\n",
      "GT sentence: jean arthur shines when she looks at powell.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: matthhias husealso did a good jb, as always\n",
      "GT sentence: matthias hues also did a good job, as always.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: however ,teres libttle bustancqe underneath\n",
      "GT sentence: however, theres little substance underneath.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: belpieve me , he made the wrong choice\n",
      "GT sentence: believe me , he made the wrong choice!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: the songs biite.look,dont waste yourime\n",
      "GT sentence: the songs bite.look, dont waste your time.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: theb scottish loclae was wasited\n",
      "GT sentence: the scottish locale was wasted.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: th erest of the fivlm doesn ot\n",
      "GT sentence: the rest of the film does not.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: ewll, yes\n",
      "GT sentence: well, yes.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: oww\n",
      "GT sentence: wow!\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: tihs is frazetat.in motion\n",
      "GT sentence: this is frazetta...in motion.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: kuos to everyone nivolved wikth htis bfilm\n",
      "GT sentence: kudos to everyone involved with this film.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: yozu just have to sere for yourself\n",
      "GT sentence: you just have to see for yourself.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: plenty of cameos by real life porsno stars\n",
      "GT sentence: plenty of cameos by real life porno stars.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: idnia summer is a ogod fli\n",
      "GT sentence: indian summer is a good film.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: the actin i flawless\n",
      "GT sentence: the acting is flawless.\n",
      "Decoded sentence: this is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: thhey mvie remains in the ary for art oo oln.\n",
      "GT sentence: the movie remains in the gray for far too long.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: and this, too, is tru eof decamelron\n",
      "GT sentence: and this, too, is true of decameron.\n",
      "Decoded sentence: it is UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_words_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train sentences model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_SENTS_PER_DOC = 11\n",
      "\n",
      "MAX_WORDS_PER_SENT = 24\n",
      "\n",
      "MAX_CHARS_PER_WORD = 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_SENTS_PER_DOC = int(np.mean(sents_per_docs_lengths)) + 1\n",
    "MAX_WORDS_PER_SENT = int(np.mean(words_per_sents_lengths)) + 1\n",
    "MAX_CHARS_PER_WORD = int(np.mean(chars_per_words_lengths)) + 1\n",
    "print('MAX_SENTS_PER_DOC = ' + str(MAX_SENTS_PER_DOC) + '\\n')\n",
    "print('MAX_WORDS_PER_SENT = ' + str(MAX_WORDS_PER_SENT) + '\\n')\n",
    "print('MAX_CHARS_PER_WORD = ' + str(MAX_CHARS_PER_WORD) + '\\n')\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_input_data, hier_input_targets = vectorize_sentences_data(input_texts=all_texts, \n",
    "                                                               target_labels=list(data_train.sentiment), \n",
    "                                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                                               num_classes=NUM_CLASSES, \n",
    "                                                               char2int=char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 11, 24, 5)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hier_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hier_input_targets.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
