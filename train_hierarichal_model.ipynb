{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_terms_with_noise(json_file, num_samples, noise_threshold):\n",
    "    with open(json_file) as f:\n",
    "        med_terms_dict = json.load(f)\n",
    "    med_terms = list(med_terms_dict.keys())\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    cnt = 0\n",
    "    while cnt < num_samples:\n",
    "        for term in med_terms:\n",
    "            if cnt < num_samples :\n",
    "                input_text = noise_maker(term, noise_threshold)\n",
    "                input_text = input_text[:-1]   \n",
    "\n",
    "                target_text = '\\t' + term + '\\n'\n",
    "\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])        \n",
    "                cnt += 1\n",
    "    return input_texts, target_texts, gt_texts, med_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accidents_terms_with_noise(file_name, limit, num_samples, noise_threshold):\n",
    "\n",
    "    f = open(file_name, encoding='utf8')\n",
    "    line = 0    \n",
    "    med_terms = []\n",
    "    try:\n",
    "        for r in f:\n",
    "            if(line < limit):\n",
    "\n",
    "                med_terms.extend(r.split('|'))\n",
    "                line += 1\n",
    "    except:\n",
    "        print('finished')\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    cnt = 0\n",
    "    while cnt < num_samples:\n",
    "        for term in med_terms:\n",
    "            if cnt < num_samples :\n",
    "                input_text = noise_maker(term, noise_threshold)\n",
    "                input_text = input_text[:-1]   \n",
    "\n",
    "                target_text = '\\t' + term + '\\n'\n",
    "\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])        \n",
    "                cnt += 1\n",
    "                \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_procedures_tests_with_noise(file_name, num_samples, noise_threshold):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                \n",
    "                input_text = noise_maker(row, noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + row + '\\n'            \n",
    "\n",
    "                cnt += 1\n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_hier_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data = np.zeros(\n",
    "    (len(input_texts), max_words_seq_length, max_chars_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    decoder_word_input_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length),\n",
    "        dtype='float32')\n",
    "    \n",
    "    decoder_word_target_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length, num_word_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[i, j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            processed_word = process_word(word)\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[i, j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[i, j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "    return encoder_char_input_data, decoder_word_input_data, decoder_word_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence():\n",
    "    #TODO\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(encoder_char_model, max_words_seq_len, max_char_seq_len, num_word_tokens, num_char_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    \n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "\n",
    "    \n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\\nhier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\n",
    "hier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = Input(shape=(50,))\\nx = Reshape((1,50))(x)\\nprint(x)\\nl = []\\nl.append(x)\\nl.append(x)\\nprint(l)\\ny = Concatenate(axis=-2)(l)\\nprint(y)\\nz = Reshape((-1,2,50))(y)\\nprint(z)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = Input(shape=(50,))\n",
    "x = Reshape((1,50))(x)\n",
    "print(x)\n",
    "l = []\n",
    "l.append(x)\n",
    "l.append(x)\n",
    "print(l)\n",
    "y = Concatenate(axis=-2)(l)\n",
    "print(y)\n",
    "z = Reshape((-1,2,50))(y)\n",
    "print(z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        processed_word = word.lower()\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in (input_texts_OCR+target_texts_OCR):\n",
    "    words = word_tokenize(text)\n",
    "    #words_per_sents_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        chars_per_words_lengths.append(len(word))\n",
    "\n",
    "# Words in sent should be on target only        \n",
    "for text in target_texts_OCR:\n",
    "    words = word_tokenize(text)\n",
    "    words_per_sents_lengths.append(len(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7b4a16198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7b4182320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 40000\n",
      "Number of unique input tokens: 99\n",
      "Number of unique output tokens: 99\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '\"': 95,\n",
       " '#': 67,\n",
       " '$': 80,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 48,\n",
       " '/': 29,\n",
       " '0': 54,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 57,\n",
       " '4': 56,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 70,\n",
       " '8': 61,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '=': 94,\n",
       " '?': 60,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 45,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 52,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 50,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 46,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 49,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 51,\n",
       " 'X': 79,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '[': 91,\n",
       " '\\\\': 97,\n",
       " ']': 92,\n",
       " '^': 86,\n",
       " '_': 98,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 59,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 58,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 62,\n",
       " 'y': 10,\n",
       " 'z': 63,\n",
       " '|': 82,\n",
       " '°': 90,\n",
       " '–': 93,\n",
       " '’': 66,\n",
       " '”': 89,\n",
       " '•': 84,\n",
       " '●': 87,\n",
       " '✓': 96,\n",
       " 'ﬁ': 88}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'E',\n",
       " 46: 'R',\n",
       " 47: 'Y',\n",
       " 48: '.',\n",
       " 49: 'U',\n",
       " 50: 'K',\n",
       " 51: 'W',\n",
       " 52: 'H',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: '4',\n",
       " 57: '3',\n",
       " 58: 'q',\n",
       " 59: 'k',\n",
       " 60: '?',\n",
       " 61: '8',\n",
       " 62: 'x',\n",
       " 63: 'z',\n",
       " 64: '(',\n",
       " 65: ')',\n",
       " 66: '’',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: 'X',\n",
       " 80: '$',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '•',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '●',\n",
       " 88: 'ﬁ',\n",
       " 89: '”',\n",
       " 90: '°',\n",
       " 91: '[',\n",
       " 92: ']',\n",
       " 93: '–',\n",
       " 94: '=',\n",
       " 95: '\"',\n",
       " 96: '✓',\n",
       " 97: '\\\\',\n",
       " 98: '_'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize char data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_char_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 99)     9801        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  729088      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 99)     9801        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1253376     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 99)     101475      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,103,541\n",
      "Trainable params: 2,083,939\n",
      "Non-trainable params: 19,602\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_char_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "32000/32000 [==============================] - 180s 6ms/step - loss: 0.8602 - categorical_accuracy: 0.7387 - val_loss: 0.2195 - val_categorical_accuracy: 0.8949\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.89493, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "32000/32000 [==============================] - 179s 6ms/step - loss: 0.1093 - categorical_accuracy: 0.9292 - val_loss: 0.1235 - val_categorical_accuracy: 0.9212\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.89493 to 0.92117, saving model to best_model_char-50.hdf5\n",
      "Epoch 3/10\n",
      "32000/32000 [==============================] - 179s 6ms/step - loss: 0.0652 - categorical_accuracy: 0.9405 - val_loss: 0.1017 - val_categorical_accuracy: 0.9261\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.92117 to 0.92615, saving model to best_model_char-50.hdf5\n",
      "Epoch 4/10\n",
      "32000/32000 [==============================] - 178s 6ms/step - loss: 0.0510 - categorical_accuracy: 0.9440 - val_loss: 0.0956 - val_categorical_accuracy: 0.9269\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.92615 to 0.92694, saving model to best_model_char-50.hdf5\n",
      "Epoch 5/10\n",
      " 6016/32000 [====>.........................] - ETA: 2:13 - loss: 0.0395 - categorical_accuracy: 0.9470"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-1ded3733babb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_char-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model_file = 'encoder_char_model-{}.hdf5'\n",
    "decoder_char_model_file = 'decoder_char_model-{}.hdf5'\n",
    "encoder_model.save('encoder_char_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_char_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Hierarichal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_seq_len=15\n",
    "max_chars_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len,max_char_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_char_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'claim': 4,\n",
       " 'type': 5,\n",
       " 'vb': 6,\n",
       " 'accident': 7,\n",
       " 'accidental': 8,\n",
       " 'injury': 9,\n",
       " 'information': 10,\n",
       " 'first': 11,\n",
       " 'name': 12,\n",
       " 'middle': 13,\n",
       " 'last': 14,\n",
       " 'social': 15,\n",
       " 'security': 16,\n",
       " 'number': 17,\n",
       " 'birth': 18,\n",
       " 'date': 19,\n",
       " 'gender': 20,\n",
       " 'language': 21,\n",
       " 'preference': 22,\n",
       " 'address': 23,\n",
       " 'line': 24,\n",
       " 'city': 25,\n",
       " 'postal': 26,\n",
       " 'code': 27,\n",
       " 'country': 28,\n",
       " 'email': 29,\n",
       " 'page': 30,\n",
       " 'radiology': 31,\n",
       " 'report': 32,\n",
       " 'patient': 33,\n",
       " 'mrn': 34,\n",
       " 'accession': 35,\n",
       " 'ref': 36,\n",
       " 'physician': 37,\n",
       " 'unknown': 38,\n",
       " 'study': 39,\n",
       " 'hospital': 40,\n",
       " 'dob': 41,\n",
       " 'technique': 42,\n",
       " 'views': 43,\n",
       " 'left': 44,\n",
       " 'wrist': 45,\n",
       " 'cormarison': 46,\n",
       " 'none': 47,\n",
       " 'availabie': 48,\n",
       " 'comparison': 49,\n",
       " 'available': 50,\n",
       " 'findings': 51,\n",
       " 'impression': 52,\n",
       " 'acute': 53,\n",
       " 'osseous': 54,\n",
       " 'abnormality': 55,\n",
       " 'identified': 56,\n",
       " 'daytime': 57,\n",
       " 'phone': 58,\n",
       " 'event': 59,\n",
       " 'stopped': 60,\n",
       " 'working': 61,\n",
       " 'yes': 62,\n",
       " 'physically': 63,\n",
       " 'work': 64,\n",
       " 'hours': 65,\n",
       " 'worked': 66,\n",
       " 'day': 67,\n",
       " 'scheduled': 68,\n",
       " 'missed': 69,\n",
       " 'returned': 70,\n",
       " 'related': 71,\n",
       " 'time': 72,\n",
       " 'surgery': 73,\n",
       " 'required': 74,\n",
       " 'indicator': 75,\n",
       " 'outpatient': 76,\n",
       " 'medical': 77,\n",
       " 'provider': 78,\n",
       " 'roles': 79,\n",
       " 'treating': 80,\n",
       " 'patrick': 81,\n",
       " 'emerson': 82,\n",
       " 'business': 83,\n",
       " 'telephone': 84,\n",
       " 'fax': 85,\n",
       " 'visit': 86,\n",
       " 'next': 87,\n",
       " 'hospitalization': 88,\n",
       " 'discharge': 89,\n",
       " 'procedure': 90,\n",
       " 'arthiscopic': 91,\n",
       " 'employment': 92,\n",
       " 'employer': 93,\n",
       " 'policy': 94,\n",
       " 'electronic': 95,\n",
       " 'submission': 96,\n",
       " 'identifier': 97,\n",
       " 'electronically': 98,\n",
       " 'signed': 99,\n",
       " 'fraud': 100,\n",
       " 'statements': 101,\n",
       " 'reviewed': 102,\n",
       " 'unum': 103,\n",
       " 'benefits': 104,\n",
       " 'center': 105,\n",
       " 'fmla': 106,\n",
       " 'requests': 107,\n",
       " 'insured': 108,\n",
       " '’': 109,\n",
       " 'signature': 110,\n",
       " 'printed': 111,\n",
       " 'confirmation': 112,\n",
       " 'coverage': 113,\n",
       " 'group': 114,\n",
       " 'customer': 115,\n",
       " 'ee': 116,\n",
       " 'effective': 117,\n",
       " 'employee': 118,\n",
       " 'acc': 119,\n",
       " 'january': 120,\n",
       " 'wellness': 121,\n",
       " 'benefit': 122,\n",
       " 'total': 123,\n",
       " 'monthly': 124,\n",
       " 'premium': 125,\n",
       " 'montly': 126,\n",
       " 'payroll': 127,\n",
       " 'deduction': 128,\n",
       " 'account': 129,\n",
       " 'f': 130,\n",
       " 'exam': 131,\n",
       " 'referring': 132,\n",
       " 'phys': 133,\n",
       " 'stephen': 134,\n",
       " 'gelovich': 135,\n",
       " 'tax': 136,\n",
       " 'mri': 137,\n",
       " 'without': 138,\n",
       " 'contrast': 139,\n",
       " 'results': 140,\n",
       " 'faxed': 141,\n",
       " 'bravo': 142,\n",
       " 'dependent': 143,\n",
       " 'detail': 144,\n",
       " 'zachary': 145,\n",
       " 'jager': 146,\n",
       " 'billed': 147,\n",
       " 'amounts': 148,\n",
       " 'contract': 149,\n",
       " 'adjustment': 150,\n",
       " 'allowed': 151,\n",
       " 'amount': 152,\n",
       " 'covered': 153,\n",
       " 'reason': 154,\n",
       " 'deductible': 155,\n",
       " 'carrier': 156,\n",
       " 'paid': 157,\n",
       " 'responsibility': 158,\n",
       " 'totals': 159,\n",
       " 'appeals': 160,\n",
       " 'rights': 161,\n",
       " 'important': 162,\n",
       " 'appeal': 163,\n",
       " 'languages': 164,\n",
       " 'contact': 165,\n",
       " 'know': 166,\n",
       " 'specialty': 167,\n",
       " 'orthopedic': 168,\n",
       " 'surgeon': 169,\n",
       " 'kari': 170,\n",
       " 'lund': 171,\n",
       " 'orthopedist': 172,\n",
       " 'dan': 173,\n",
       " 'palmer': 174,\n",
       " '&': 175,\n",
       " 'may': 176,\n",
       " 'spouse': 177,\n",
       " 'child': 178,\n",
       " 'black': 179,\n",
       " 'hills': 180,\n",
       " 'pc': 181,\n",
       " 'pmt': 182,\n",
       " 'due': 183,\n",
       " 'statement': 184,\n",
       " 'ins': 185,\n",
       " 'description': 186,\n",
       " 'e': 187,\n",
       " 'm': 188,\n",
       " 'new': 189,\n",
       " 'moderat': 190,\n",
       " 'clo': 191,\n",
       " 'tx': 192,\n",
       " 'phalangealfx': 193,\n",
       " 'finger': 194,\n",
       " 'splint': 195,\n",
       " 'offic': 196,\n",
       " 'cons': 197,\n",
       " 'moderate': 198,\n",
       " 'sever': 199,\n",
       " 'rad': 200,\n",
       " 'mini': 201,\n",
       " 'applic': 202,\n",
       " 'hand': 203,\n",
       " 'lower': 204,\n",
       " 'forearm': 205,\n",
       " 'fiberglass': 206,\n",
       " 'gauntlet': 207,\n",
       " 'cast': 208,\n",
       " 'yrs': 209,\n",
       " 'charge': 210,\n",
       " 'pat': 211,\n",
       " 'adjust': 212,\n",
       " 'current': 213,\n",
       " 'days': 214,\n",
       " 'balance': 215,\n",
       " 'pending': 216,\n",
       " 'message': 217,\n",
       " 'make': 218,\n",
       " 'checks': 219,\n",
       " 'payable': 220,\n",
       " 'billing': 221,\n",
       " 'questions': 222,\n",
       " 'choice': 223,\n",
       " 'health': 224,\n",
       " 'administrators': 225,\n",
       " 'forwarding': 226,\n",
       " 'service': 227,\n",
       " 'requested': 228,\n",
       " 'regional': 229,\n",
       " 'inc': 230,\n",
       " 'participant': 231,\n",
       " 'id': 232,\n",
       " 'original': 233,\n",
       " 'print': 234,\n",
       " 'website': 235,\n",
       " 'individual': 236,\n",
       " 'summary': 237,\n",
       " 'plan': 238,\n",
       " 'status': 239,\n",
       " 'period': 240,\n",
       " 'pocket': 241,\n",
       " 'explanation': 242,\n",
       " 'retain': 243,\n",
       " 'purposes': 244,\n",
       " 'family': 245,\n",
       " 'network': 246,\n",
       " 'karl': 247,\n",
       " 'services': 248,\n",
       " 'modifiers': 249,\n",
       " 'tc': 250,\n",
       " 'rt': 251,\n",
       " 'qualified': 252,\n",
       " 'sign': 253,\n",
       " 'interpreters': 254,\n",
       " 'written': 255,\n",
       " 'jacquelin': 256,\n",
       " 'brainard': 257,\n",
       " 'compliance': 258,\n",
       " 'officer': 259,\n",
       " 'mail': 260,\n",
       " 'department': 261,\n",
       " 'human': 262,\n",
       " 'complaint': 263,\n",
       " 'forms': 264,\n",
       " 'dakota': 265,\n",
       " 'ez': 266,\n",
       " 'ways': 267,\n",
       " 'pay': 268,\n",
       " 'automated': 269,\n",
       " 'attendant': 270,\n",
       " 'payments': 271,\n",
       " 'please': 272,\n",
       " 'call': 273,\n",
       " 'upon': 274,\n",
       " 'receipt': 275,\n",
       " 'improved': 276,\n",
       " 'online': 277,\n",
       " 'experience': 278,\n",
       " '|': 279,\n",
       " 'update': 280,\n",
       " 'info': 281,\n",
       " 'see': 282,\n",
       " 'details': 283,\n",
       " 'back': 284,\n",
       " 'show': 285,\n",
       " 'proc': 286,\n",
       " 'units': 287,\n",
       " 'charges': 288,\n",
       " 'insur': 289,\n",
       " 'thorac': 290,\n",
       " 'spine': 291,\n",
       " 'commercial': 292,\n",
       " 'non': 293,\n",
       " 'ct': 294,\n",
       " 'abd': 295,\n",
       " 'pelv': 296,\n",
       " 'payment': 297,\n",
       " 'chest': 298,\n",
       " 'harges': 299,\n",
       " 'digit': 300,\n",
       " 'today': 301,\n",
       " \"'s\": 302,\n",
       " 'ethnicity': 303,\n",
       " 'hispanic': 304,\n",
       " 'latino': 305,\n",
       " 'preferred': 306,\n",
       " 'english': 307,\n",
       " 'suzanne': 308,\n",
       " 'newsom': 309,\n",
       " 'cnp': 310,\n",
       " '•': 311,\n",
       " 'lethargy': 312,\n",
       " 'cough': 313,\n",
       " 'vitals': 314,\n",
       " 'lbs': 315,\n",
       " 'kg': 316,\n",
       " 'wt': 317,\n",
       " 'temp': 318,\n",
       " 'hr': 319,\n",
       " 'oxygen': 320,\n",
       " 'sat': 321,\n",
       " 'allergies': 322,\n",
       " 'amoxicillin': 323,\n",
       " 'rash': 324,\n",
       " 'possible': 325,\n",
       " 'hives': 326,\n",
       " 'active': 327,\n",
       " 'diagnoses': 328,\n",
       " 'include': 329,\n",
       " 'frontal': 330,\n",
       " 'sinusitis': 331,\n",
       " 'unspecified': 332,\n",
       " 'dizziness': 333,\n",
       " 'giddiness': 334,\n",
       " 'medication': 335,\n",
       " 'list': 336,\n",
       " 'medications': 337,\n",
       " 'taking': 338,\n",
       " 'zyrtec': 339,\n",
       " 'childrens': 340,\n",
       " 'allergy': 341,\n",
       " 'notes': 342,\n",
       " 'tests': 343,\n",
       " 'labs': 344,\n",
       " 'illumigene': 345,\n",
       " 'myco': 346,\n",
       " 'http': 347,\n",
       " 'basic': 348,\n",
       " 'metabolic': 349,\n",
       " 'sodium': 350,\n",
       " 'range': 351,\n",
       " 'potassium': 352,\n",
       " 'chloride': 353,\n",
       " 'glucose': 354,\n",
       " 'bun': 355,\n",
       " 'creatinine': 356,\n",
       " 'calcium': 357,\n",
       " 'crea': 358,\n",
       " 'ratio': 359,\n",
       " 'anion': 360,\n",
       " 'gap': 361,\n",
       " 'calc': 362,\n",
       " 'cbc': 363,\n",
       " 'diff': 364,\n",
       " 'wbc': 365,\n",
       " 'rbc': 366,\n",
       " 'hgb': 367,\n",
       " 'hct': 368,\n",
       " 'mcv': 369,\n",
       " 'fl': 370,\n",
       " 'mch': 371,\n",
       " 'pg': 372,\n",
       " 'mchc': 373,\n",
       " 'mpv': 374,\n",
       " 'platelets': 375,\n",
       " 'neutrophils': 376,\n",
       " 'lymphocytes': 377,\n",
       " 'monocytes': 378,\n",
       " 'conditions': 379,\n",
       " 'problem': 380,\n",
       " 'idiopathic': 381,\n",
       " 'urticaria': 382,\n",
       " 'document': 383,\n",
       " 'wish': 384,\n",
       " 'keep': 385,\n",
       " 'policyholder': 386,\n",
       " 'owner': 387,\n",
       " 'eastside': 388,\n",
       " 'acct': 389,\n",
       " 'jasminder': 390,\n",
       " 'singh': 391,\n",
       " 'dev': 392,\n",
       " 'pa': 393,\n",
       " 'excuse': 394,\n",
       " 'east': 395,\n",
       " 'side': 396,\n",
       " 'april': 397,\n",
       " 'weekly': 398,\n",
       " 'ph': 399,\n",
       " 'mr': 400,\n",
       " 'primary': 401,\n",
       " 'thoracic': 402,\n",
       " 'strain': 403,\n",
       " 'strained': 404,\n",
       " 'following': 405,\n",
       " 'occurs': 406,\n",
       " 'feel': 407,\n",
       " 'weakness': 408,\n",
       " 'arms': 409,\n",
       " 'legs': 410,\n",
       " 'severe': 411,\n",
       " 'increase': 412,\n",
       " 'pain': 413,\n",
       " 'lumbosacral': 414,\n",
       " 'weak': 415,\n",
       " 'becomes': 416,\n",
       " 'follow': 417,\n",
       " 'take': 418,\n",
       " 'directed': 419,\n",
       " 'additional': 420,\n",
       " 'prescriptions': 421,\n",
       " 'prescriber': 422,\n",
       " 'paper': 423,\n",
       " 'prescription': 424,\n",
       " 'given': 425,\n",
       " 'preventative': 426,\n",
       " 'instructions': 427,\n",
       " 'diagnosis': 428,\n",
       " 'knee': 429,\n",
       " 'david': 430,\n",
       " 'bruce': 431,\n",
       " 'identiﬁer': 432,\n",
       " 'june': 433,\n",
       " 'concussion': 434,\n",
       " 'devin': 435,\n",
       " 'conrad': 436,\n",
       " 'september': 437,\n",
       " 'form': 438,\n",
       " 'attending': 439,\n",
       " 'part': 440,\n",
       " 'completed': 441,\n",
       " 'icd': 442,\n",
       " 'unable': 443,\n",
       " 'expected': 444,\n",
       " 'delivery': 445,\n",
       " 'actual': 446,\n",
       " 'vaginal': 447,\n",
       " 'per': 448,\n",
       " 'continued': 449,\n",
       " 'facility': 450,\n",
       " 'state': 451,\n",
       " 'zip': 452,\n",
       " 'performed': 453,\n",
       " 'surgical': 454,\n",
       " 'cpt': 455,\n",
       " 'degree': 456,\n",
       " 'check': 457,\n",
       " 'filing': 458,\n",
       " 'b': 459,\n",
       " 'suffix': 460,\n",
       " 'mi': 461,\n",
       " 'spanish': 462,\n",
       " 'short': 463,\n",
       " 'term': 464,\n",
       " 'disability': 465,\n",
       " 'long': 466,\n",
       " 'life': 467,\n",
       " 'insurance': 468,\n",
       " 'voluntary': 469,\n",
       " 'motor': 470,\n",
       " 'vehicle': 471,\n",
       " 'physicians': 472,\n",
       " 'hospitals': 473,\n",
       " 'considerations': 474,\n",
       " 'male': 475,\n",
       " 'female': 476,\n",
       " 'x': 477,\n",
       " 'member': 478,\n",
       " 'relationship': 479,\n",
       " 'person': 480,\n",
       " 'marital': 481,\n",
       " 'single': 482,\n",
       " 'occ': 483,\n",
       " 'title': 484,\n",
       " 'resinmixer': 485,\n",
       " 'hire': 486,\n",
       " 'termination': 487,\n",
       " 'atw': 488,\n",
       " 'limitations': 489,\n",
       " 'permitted': 490,\n",
       " 'months': 491,\n",
       " 'office': 492,\n",
       " 'crane': 493,\n",
       " 'composites': 494,\n",
       " 'florence': 495,\n",
       " 'earn': 496,\n",
       " 'change': 497,\n",
       " 'leave': 498,\n",
       " 'absence': 499,\n",
       " 'record': 500,\n",
       " 'loaded': 501,\n",
       " 'residence': 502,\n",
       " 'physical': 503,\n",
       " 'access': 504,\n",
       " 'home': 505,\n",
       " 'supervisor': 506,\n",
       " 'coverages': 507,\n",
       " 'product': 508,\n",
       " 'flex': 509,\n",
       " 'funding': 510,\n",
       " 'fully': 511,\n",
       " 'division': 512,\n",
       " 'peg': 513,\n",
       " 'eff': 514,\n",
       " 'earnings': 515,\n",
       " 'hourly': 516,\n",
       " 'mode': 517,\n",
       " 'aso': 518,\n",
       " 'self': 519,\n",
       " 'probable': 520,\n",
       " 'duration': 521,\n",
       " 'condition': 522,\n",
       " 'dates': 523,\n",
       " 'admission': 524,\n",
       " 'treatment': 525,\n",
       " 'seen': 526,\n",
       " 'past': 527,\n",
       " 'anticipated': 528,\n",
       " 'nature': 529,\n",
       " 'estimated': 530,\n",
       " 'treatments': 531,\n",
       " 'job': 532,\n",
       " 'attached': 533,\n",
       " 'checked': 534,\n",
       " 'episodic': 535,\n",
       " 'flare': 536,\n",
       " 'ups': 537,\n",
       " 'practice': 538,\n",
       " 'care': 539,\n",
       " 'including': 540,\n",
       " 'confinement': 541,\n",
       " 'provide': 542,\n",
       " 'advice': 543,\n",
       " 'stop': 544,\n",
       " 'mgmt': 545,\n",
       " 'svc': 546,\n",
       " 'applicable': 547,\n",
       " 'deductions': 548,\n",
       " 'schedule': 549,\n",
       " 'week': 550,\n",
       " 'sick': 551,\n",
       " 'variable': 552,\n",
       " 'sunday': 553,\n",
       " 'monday': 554,\n",
       " 'tuesday': 555,\n",
       " 'wednesday': 556,\n",
       " 'thursday': 557,\n",
       " 'friday': 558,\n",
       " 'saturday': 559,\n",
       " 'hospitalized': 560,\n",
       " 'explain': 561,\n",
       " 'height': 562,\n",
       " 'weight': 563,\n",
       " 'secondary': 564,\n",
       " 'functional': 565,\n",
       " 'capacity': 566,\n",
       " 'restrictions': 567,\n",
       " 'elizabeth': 568,\n",
       " 'edgewood': 569,\n",
       " 'facesheet': 570,\n",
       " 'sex': 571,\n",
       " 'adm': 572,\n",
       " 'demographics': 573,\n",
       " 'ssn': 574,\n",
       " 'reg': 575,\n",
       " 'verified': 576,\n",
       " 'pcp': 577,\n",
       " 'renew': 578,\n",
       " 'admitting': 579,\n",
       " 'larkin': 580,\n",
       " 'john': 581,\n",
       " 'md': 582,\n",
       " 'elective': 583,\n",
       " 'incomplete': 584,\n",
       " 'area': 585,\n",
       " 'edg': 586,\n",
       " 'sc': 587,\n",
       " 'crestview': 588,\n",
       " 'discharged': 589,\n",
       " 'confirmed': 590,\n",
       " 'hose': 591,\n",
       " 'ital': 592,\n",
       " 'class': 593,\n",
       " 'guarantor': 594,\n",
       " 'relation': 595,\n",
       " 'pt': 596,\n",
       " 'seh': 597,\n",
       " 'precert': 598,\n",
       " 'subscriber': 599,\n",
       " 'operative': 600,\n",
       " 'brief': 601,\n",
       " 'op': 602,\n",
       " 'note': 603,\n",
       " 'author': 604,\n",
       " 'j': 605,\n",
       " 'filed': 606,\n",
       " 'editor': 607,\n",
       " 'healthcare': 608,\n",
       " 'body': 609,\n",
       " 'mass': 610,\n",
       " 'index': 611,\n",
       " 'role': 612,\n",
       " 'anesthesia': 613,\n",
       " 'general': 614,\n",
       " 'specimens': 615,\n",
       " 'log': 616,\n",
       " 'blood': 617,\n",
       " 'loss': 618,\n",
       " 'course': 619,\n",
       " 'pacu': 620,\n",
       " 'operation': 621,\n",
       " 'best': 622,\n",
       " 'reached': 623,\n",
       " 'broken': 624,\n",
       " 'big': 625,\n",
       " 'toe': 626,\n",
       " 'foot': 627,\n",
       " 'todd': 628,\n",
       " 'francis': 629,\n",
       " 'podiatrist': 630,\n",
       " 'ryan': 631,\n",
       " 'kish': 632,\n",
       " 'toledo': 633,\n",
       " 'er': 634,\n",
       " 'xray': 635,\n",
       " 'july': 636,\n",
       " 'paramount': 637,\n",
       " 'promedica': 638,\n",
       " 'authorization': 639,\n",
       " 'modifier': 640,\n",
       " 'lt': 641,\n",
       " 'indicates': 642,\n",
       " 'c': 643,\n",
       " 'dpm': 644,\n",
       " 'ems': 645,\n",
       " 'christine': 646,\n",
       " 'nolen': 647,\n",
       " 'waukesha': 648,\n",
       " 'memorial': 649,\n",
       " 'cleaning': 650,\n",
       " 'bandage': 651,\n",
       " 'montano': 652,\n",
       " 'ci': 653,\n",
       " 'web': 654,\n",
       " 'user': 655,\n",
       " 'prohealth': 656,\n",
       " 'umr': 657,\n",
       " 'adjustments': 658,\n",
       " 'patients': 659,\n",
       " 'invoice': 660,\n",
       " 'previous': 661,\n",
       " 'return': 662,\n",
       " 'portion': 663,\n",
       " 'mastercard': 664,\n",
       " 'discover': 665,\n",
       " 'visa': 666,\n",
       " 'american': 667,\n",
       " 'express': 668,\n",
       " 'card': 669,\n",
       " 'enclosed': 670,\n",
       " 'emergency': 671,\n",
       " 'associates': 672,\n",
       " 'using': 673,\n",
       " 'addressee': 674,\n",
       " 'industrial': 675,\n",
       " 'loop': 676,\n",
       " 'dr': 677,\n",
       " 'ps': 678,\n",
       " 'dept': 679,\n",
       " 'ppo': 680,\n",
       " 'adj': 681,\n",
       " 'applied': 682,\n",
       " 'rendered': 683,\n",
       " 'fiserv': 684,\n",
       " 'wi': 685,\n",
       " 'stmt': 686,\n",
       " 'doctor': 687,\n",
       " 'legend': 688,\n",
       " 'd': 689,\n",
       " 'comments': 690,\n",
       " 'souha': 691,\n",
       " 'hakim': 692,\n",
       " 'medexpress': 693,\n",
       " 'codes': 694,\n",
       " 'urgent': 695,\n",
       " 'clairn': 696,\n",
       " 'vijay': 697,\n",
       " 'patel': 698,\n",
       " 'holder': 699,\n",
       " 'qty': 700,\n",
       " 'clinical': 701,\n",
       " 'chief': 702,\n",
       " 'taken': 703,\n",
       " 'bp': 704,\n",
       " 'mmhg': 705,\n",
       " 'pulse': 706,\n",
       " 'bpm': 707,\n",
       " 'resp': 708,\n",
       " 'lb': 709,\n",
       " 'ft': 710,\n",
       " 'bmi': 711,\n",
       " 'meds': 712,\n",
       " 'albuterol': 713,\n",
       " 'sulfate': 714,\n",
       " 'encounter': 715,\n",
       " 'progress': 716,\n",
       " 'subjective': 717,\n",
       " 'history': 718,\n",
       " 'provided': 719,\n",
       " 'dad': 720,\n",
       " 'interpreter': 721,\n",
       " 'used': 722,\n",
       " 'presents': 723,\n",
       " 'review': 724,\n",
       " 'systems': 725,\n",
       " 'cardiovascular': 726,\n",
       " 'negative': 727,\n",
       " 'skin': 728,\n",
       " 'neurological': 729,\n",
       " 'headaches': 730,\n",
       " 'objective': 731,\n",
       " 'hent': 732,\n",
       " 'right': 733,\n",
       " 'ear': 734,\n",
       " 'tympanic': 735,\n",
       " 'membrane': 736,\n",
       " 'normal': 737,\n",
       " 'nose': 738,\n",
       " 'oropharynx': 739,\n",
       " 'clear': 740,\n",
       " 'eyes': 741,\n",
       " 'conjunctivae': 742,\n",
       " 'eom': 743,\n",
       " 'neck': 744,\n",
       " 'supple': 745,\n",
       " 'rigidity': 746,\n",
       " 'murmur': 747,\n",
       " 'heard': 748,\n",
       " 'lymphadenopathy': 749,\n",
       " 'cervical': 750,\n",
       " 'adenopathy': 751,\n",
       " 'alert': 752,\n",
       " 'warm': 753,\n",
       " 'noted': 754,\n",
       " 'assessment': 755,\n",
       " 'advise': 756,\n",
       " 'ss': 757,\n",
       " 'pchc': 758,\n",
       " 'penobscot': 759,\n",
       " 'community': 760,\n",
       " 'suite': 761,\n",
       " 'medicine': 762,\n",
       " 'mental': 763,\n",
       " 'transmission': 764,\n",
       " 'sheet': 765,\n",
       " 'ext': 766,\n",
       " 're': 767,\n",
       " 'pages': 768,\n",
       " 'cover': 769,\n",
       " 'thank': 770,\n",
       " 'revised': 771,\n",
       " 'imaging': 772,\n",
       " 'mainecare': 773,\n",
       " 'fqhc': 774,\n",
       " 'low': 775,\n",
       " 'jt': 776,\n",
       " 'erin': 777,\n",
       " 'barker': 778,\n",
       " 'joseph': 779,\n",
       " 'ordering': 780,\n",
       " 'intercondylar': 781,\n",
       " 'space': 782,\n",
       " 'acl': 783,\n",
       " 'pcl': 784,\n",
       " 'intact': 785,\n",
       " 'unremarkable': 786,\n",
       " 'marrow': 787,\n",
       " 'signal': 788,\n",
       " 'dictation': 789,\n",
       " 'location': 790,\n",
       " 'mpsynernet': 791,\n",
       " 'reading': 792,\n",
       " 'kasper': 793,\n",
       " 'jared': 794,\n",
       " 'orthopedics': 795,\n",
       " 'sports': 796,\n",
       " 'np': 797,\n",
       " 'thompson': 798,\n",
       " 'mcguire': 799,\n",
       " 'initial': 800,\n",
       " 'evaluation': 801,\n",
       " 'responsible': 802,\n",
       " 'samara': 803,\n",
       " 'shiromani': 804,\n",
       " 'cc': 805,\n",
       " 'present': 806,\n",
       " 'illness': 807,\n",
       " 'persistent': 808,\n",
       " 'patella': 809,\n",
       " 'alta': 810,\n",
       " 'tendonitis': 811,\n",
       " 'asthma': 812,\n",
       " 'fractures': 813,\n",
       " 'changes': 814,\n",
       " 'father': 815,\n",
       " 'htn': 816,\n",
       " 'sister': 817,\n",
       " 'factor': 818,\n",
       " 'v': 819,\n",
       " 'seizures': 820,\n",
       " 'hs': 821,\n",
       " 'risk': 822,\n",
       " 'factors': 823,\n",
       " 'tobacco': 824,\n",
       " 'use': 825,\n",
       " 'never': 826,\n",
       " 'smoker': 827,\n",
       " 'passive': 828,\n",
       " 'smoke': 829,\n",
       " 'exposure': 830,\n",
       " 'alcohol': 831,\n",
       " 'drug': 832,\n",
       " 'caffeine': 833,\n",
       " 'drinks': 834,\n",
       " 'cellular': 835,\n",
       " 'medsupport': 836,\n",
       " 'exercise': 837,\n",
       " 'times': 838,\n",
       " 'field': 839,\n",
       " 'hockey': 840,\n",
       " 'cardio': 841,\n",
       " 'seatbelt': 842,\n",
       " 'sun': 843,\n",
       " 'occasionally': 844,\n",
       " 'fall': 845,\n",
       " 'problems': 846,\n",
       " 'dx': 847,\n",
       " 'critical': 848,\n",
       " 'chemicals': 849,\n",
       " 'zithromax': 850,\n",
       " 'azithromycin': 851,\n",
       " 'adhesive': 852,\n",
       " 'tape': 853,\n",
       " 'ent': 854,\n",
       " 'cv': 855,\n",
       " 'exertion': 856,\n",
       " 'complains': 857,\n",
       " 'gi': 858,\n",
       " 'gu': 859,\n",
       " 'ms': 860,\n",
       " 'denies': 861,\n",
       " 'varicose': 862,\n",
       " 'veins': 863,\n",
       " 'bone': 864,\n",
       " 'deformity': 865,\n",
       " 'derm': 866,\n",
       " 'neuro': 867,\n",
       " 'psych': 868,\n",
       " 'post': 869,\n",
       " 'traumatic': 870,\n",
       " 'stress': 871,\n",
       " 'disorder': 872,\n",
       " 'endo': 873,\n",
       " 'heme': 874,\n",
       " 'vital': 875,\n",
       " 'signs': 876,\n",
       " 'profile': 877,\n",
       " 'inches': 878,\n",
       " 'pounds': 879,\n",
       " 'rate': 880,\n",
       " 'minute': 881,\n",
       " 'sitting': 882,\n",
       " 'arm': 883,\n",
       " 'r': 884,\n",
       " 'intensity': 885,\n",
       " 'aching': 886,\n",
       " 'sharp': 887,\n",
       " 'lungs': 888,\n",
       " 'bilaterally': 889,\n",
       " 'p': 890,\n",
       " 'heart': 891,\n",
       " 'recommendations': 892,\n",
       " 'james': 893,\n",
       " 'greene': 894,\n",
       " 'orders': 895,\n",
       " 'assistant': 896,\n",
       " 'room': 897,\n",
       " 'diagnostic': 898,\n",
       " 'arthroscopy': 899,\n",
       " 'danielle': 900,\n",
       " 'st': 901,\n",
       " 'onge': 902,\n",
       " 'femoral': 903,\n",
       " 'block': 904,\n",
       " 'pac': 905,\n",
       " 'chondromalacia': 906,\n",
       " 'burning': 907,\n",
       " 'entered': 908,\n",
       " 'amy': 909,\n",
       " 'cyr': 910,\n",
       " 'progressing': 911,\n",
       " 'well': 912,\n",
       " 'postoperatively': 913,\n",
       " 'mariah': 914,\n",
       " 'larsen': 915,\n",
       " 'ma': 916,\n",
       " 'screening': 917,\n",
       " 'constitutional': 918,\n",
       " 'developed': 919,\n",
       " 'dtrs': 920,\n",
       " 'omega': 921,\n",
       " 'mg': 922,\n",
       " 'capsule': 923,\n",
       " 'one': 924,\n",
       " 'n': 925,\n",
       " 'counseling': 926,\n",
       " 'educational': 927,\n",
       " 'finley': 928,\n",
       " 'kevin': 929,\n",
       " 'bal': 930,\n",
       " 'item': 931,\n",
       " 'immunization': 932,\n",
       " 'admin': 933,\n",
       " 'ovr': 934,\n",
       " 'reach': 935,\n",
       " 'centers': 936,\n",
       " 'bethel': 937,\n",
       " 'year': 938,\n",
       " 'old': 939,\n",
       " 'started': 940,\n",
       " 'ago': 941,\n",
       " 'slipping': 942,\n",
       " 'ice': 943,\n",
       " 'tools': 944,\n",
       " 'screenings': 945,\n",
       " 'instrument': 946,\n",
       " 'score': 947,\n",
       " 'mdd': 948,\n",
       " 'classification': 949,\n",
       " 'questionnaire': 950,\n",
       " 'testing': 951,\n",
       " 'coronary': 952,\n",
       " 'disease': 953,\n",
       " 'order': 954,\n",
       " 'interpretation': 955,\n",
       " 'result': 956,\n",
       " 'region': 957,\n",
       " 'updated': 958,\n",
       " 'retired': 959,\n",
       " 'river': 960,\n",
       " 'sales': 961,\n",
       " 'manager': 962,\n",
       " 'support': 963,\n",
       " 'currently': 964,\n",
       " 'married': 965,\n",
       " 'smoking': 966,\n",
       " 'usage': 967,\n",
       " 'years': 968,\n",
       " 'pack': 969,\n",
       " 'prior': 970,\n",
       " 'sig': 971,\n",
       " 'desc': 972,\n",
       " 'start': 973,\n",
       " 'refilled': 974,\n",
       " 'elsewhere': 975,\n",
       " 'reconciliation': 976,\n",
       " 'reconciled': 977,\n",
       " 'ingredient': 978,\n",
       " 'reaction': 979,\n",
       " 'comment': 980,\n",
       " 'known': 981,\n",
       " 'system': 982,\n",
       " 'respiratory': 983,\n",
       " 'dyspnea': 984,\n",
       " 'integumentary': 985,\n",
       " 'positive': 986,\n",
       " 'standing': 987,\n",
       " 'dressed': 988,\n",
       " 'shoes': 989,\n",
       " 'pressure': 990,\n",
       " 'oral': 991,\n",
       " 'regular': 992,\n",
       " 'somatic': 993,\n",
       " 'rib': 994,\n",
       " 'dysfunction': 995,\n",
       " 'michael': 996,\n",
       " 'gould': 997,\n",
       " 'county': 998,\n",
       " 'potomac': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'claim',\n",
       " 5: 'type',\n",
       " 6: 'vb',\n",
       " 7: 'accident',\n",
       " 8: 'accidental',\n",
       " 9: 'injury',\n",
       " 10: 'information',\n",
       " 11: 'first',\n",
       " 12: 'name',\n",
       " 13: 'middle',\n",
       " 14: 'last',\n",
       " 15: 'social',\n",
       " 16: 'security',\n",
       " 17: 'number',\n",
       " 18: 'birth',\n",
       " 19: 'date',\n",
       " 20: 'gender',\n",
       " 21: 'language',\n",
       " 22: 'preference',\n",
       " 23: 'address',\n",
       " 24: 'line',\n",
       " 25: 'city',\n",
       " 26: 'postal',\n",
       " 27: 'code',\n",
       " 28: 'country',\n",
       " 29: 'email',\n",
       " 30: 'page',\n",
       " 31: 'radiology',\n",
       " 32: 'report',\n",
       " 33: 'patient',\n",
       " 34: 'mrn',\n",
       " 35: 'accession',\n",
       " 36: 'ref',\n",
       " 37: 'physician',\n",
       " 38: 'unknown',\n",
       " 39: 'study',\n",
       " 40: 'hospital',\n",
       " 41: 'dob',\n",
       " 42: 'technique',\n",
       " 43: 'views',\n",
       " 44: 'left',\n",
       " 45: 'wrist',\n",
       " 46: 'cormarison',\n",
       " 47: 'none',\n",
       " 48: 'availabie',\n",
       " 49: 'comparison',\n",
       " 50: 'available',\n",
       " 51: 'findings',\n",
       " 52: 'impression',\n",
       " 53: 'acute',\n",
       " 54: 'osseous',\n",
       " 55: 'abnormality',\n",
       " 56: 'identified',\n",
       " 57: 'daytime',\n",
       " 58: 'phone',\n",
       " 59: 'event',\n",
       " 60: 'stopped',\n",
       " 61: 'working',\n",
       " 62: 'yes',\n",
       " 63: 'physically',\n",
       " 64: 'work',\n",
       " 65: 'hours',\n",
       " 66: 'worked',\n",
       " 67: 'day',\n",
       " 68: 'scheduled',\n",
       " 69: 'missed',\n",
       " 70: 'returned',\n",
       " 71: 'related',\n",
       " 72: 'time',\n",
       " 73: 'surgery',\n",
       " 74: 'required',\n",
       " 75: 'indicator',\n",
       " 76: 'outpatient',\n",
       " 77: 'medical',\n",
       " 78: 'provider',\n",
       " 79: 'roles',\n",
       " 80: 'treating',\n",
       " 81: 'patrick',\n",
       " 82: 'emerson',\n",
       " 83: 'business',\n",
       " 84: 'telephone',\n",
       " 85: 'fax',\n",
       " 86: 'visit',\n",
       " 87: 'next',\n",
       " 88: 'hospitalization',\n",
       " 89: 'discharge',\n",
       " 90: 'procedure',\n",
       " 91: 'arthiscopic',\n",
       " 92: 'employment',\n",
       " 93: 'employer',\n",
       " 94: 'policy',\n",
       " 95: 'electronic',\n",
       " 96: 'submission',\n",
       " 97: 'identifier',\n",
       " 98: 'electronically',\n",
       " 99: 'signed',\n",
       " 100: 'fraud',\n",
       " 101: 'statements',\n",
       " 102: 'reviewed',\n",
       " 103: 'unum',\n",
       " 104: 'benefits',\n",
       " 105: 'center',\n",
       " 106: 'fmla',\n",
       " 107: 'requests',\n",
       " 108: 'insured',\n",
       " 109: '’',\n",
       " 110: 'signature',\n",
       " 111: 'printed',\n",
       " 112: 'confirmation',\n",
       " 113: 'coverage',\n",
       " 114: 'group',\n",
       " 115: 'customer',\n",
       " 116: 'ee',\n",
       " 117: 'effective',\n",
       " 118: 'employee',\n",
       " 119: 'acc',\n",
       " 120: 'january',\n",
       " 121: 'wellness',\n",
       " 122: 'benefit',\n",
       " 123: 'total',\n",
       " 124: 'monthly',\n",
       " 125: 'premium',\n",
       " 126: 'montly',\n",
       " 127: 'payroll',\n",
       " 128: 'deduction',\n",
       " 129: 'account',\n",
       " 130: 'f',\n",
       " 131: 'exam',\n",
       " 132: 'referring',\n",
       " 133: 'phys',\n",
       " 134: 'stephen',\n",
       " 135: 'gelovich',\n",
       " 136: 'tax',\n",
       " 137: 'mri',\n",
       " 138: 'without',\n",
       " 139: 'contrast',\n",
       " 140: 'results',\n",
       " 141: 'faxed',\n",
       " 142: 'bravo',\n",
       " 143: 'dependent',\n",
       " 144: 'detail',\n",
       " 145: 'zachary',\n",
       " 146: 'jager',\n",
       " 147: 'billed',\n",
       " 148: 'amounts',\n",
       " 149: 'contract',\n",
       " 150: 'adjustment',\n",
       " 151: 'allowed',\n",
       " 152: 'amount',\n",
       " 153: 'covered',\n",
       " 154: 'reason',\n",
       " 155: 'deductible',\n",
       " 156: 'carrier',\n",
       " 157: 'paid',\n",
       " 158: 'responsibility',\n",
       " 159: 'totals',\n",
       " 160: 'appeals',\n",
       " 161: 'rights',\n",
       " 162: 'important',\n",
       " 163: 'appeal',\n",
       " 164: 'languages',\n",
       " 165: 'contact',\n",
       " 166: 'know',\n",
       " 167: 'specialty',\n",
       " 168: 'orthopedic',\n",
       " 169: 'surgeon',\n",
       " 170: 'kari',\n",
       " 171: 'lund',\n",
       " 172: 'orthopedist',\n",
       " 173: 'dan',\n",
       " 174: 'palmer',\n",
       " 175: '&',\n",
       " 176: 'may',\n",
       " 177: 'spouse',\n",
       " 178: 'child',\n",
       " 179: 'black',\n",
       " 180: 'hills',\n",
       " 181: 'pc',\n",
       " 182: 'pmt',\n",
       " 183: 'due',\n",
       " 184: 'statement',\n",
       " 185: 'ins',\n",
       " 186: 'description',\n",
       " 187: 'e',\n",
       " 188: 'm',\n",
       " 189: 'new',\n",
       " 190: 'moderat',\n",
       " 191: 'clo',\n",
       " 192: 'tx',\n",
       " 193: 'phalangealfx',\n",
       " 194: 'finger',\n",
       " 195: 'splint',\n",
       " 196: 'offic',\n",
       " 197: 'cons',\n",
       " 198: 'moderate',\n",
       " 199: 'sever',\n",
       " 200: 'rad',\n",
       " 201: 'mini',\n",
       " 202: 'applic',\n",
       " 203: 'hand',\n",
       " 204: 'lower',\n",
       " 205: 'forearm',\n",
       " 206: 'fiberglass',\n",
       " 207: 'gauntlet',\n",
       " 208: 'cast',\n",
       " 209: 'yrs',\n",
       " 210: 'charge',\n",
       " 211: 'pat',\n",
       " 212: 'adjust',\n",
       " 213: 'current',\n",
       " 214: 'days',\n",
       " 215: 'balance',\n",
       " 216: 'pending',\n",
       " 217: 'message',\n",
       " 218: 'make',\n",
       " 219: 'checks',\n",
       " 220: 'payable',\n",
       " 221: 'billing',\n",
       " 222: 'questions',\n",
       " 223: 'choice',\n",
       " 224: 'health',\n",
       " 225: 'administrators',\n",
       " 226: 'forwarding',\n",
       " 227: 'service',\n",
       " 228: 'requested',\n",
       " 229: 'regional',\n",
       " 230: 'inc',\n",
       " 231: 'participant',\n",
       " 232: 'id',\n",
       " 233: 'original',\n",
       " 234: 'print',\n",
       " 235: 'website',\n",
       " 236: 'individual',\n",
       " 237: 'summary',\n",
       " 238: 'plan',\n",
       " 239: 'status',\n",
       " 240: 'period',\n",
       " 241: 'pocket',\n",
       " 242: 'explanation',\n",
       " 243: 'retain',\n",
       " 244: 'purposes',\n",
       " 245: 'family',\n",
       " 246: 'network',\n",
       " 247: 'karl',\n",
       " 248: 'services',\n",
       " 249: 'modifiers',\n",
       " 250: 'tc',\n",
       " 251: 'rt',\n",
       " 252: 'qualified',\n",
       " 253: 'sign',\n",
       " 254: 'interpreters',\n",
       " 255: 'written',\n",
       " 256: 'jacquelin',\n",
       " 257: 'brainard',\n",
       " 258: 'compliance',\n",
       " 259: 'officer',\n",
       " 260: 'mail',\n",
       " 261: 'department',\n",
       " 262: 'human',\n",
       " 263: 'complaint',\n",
       " 264: 'forms',\n",
       " 265: 'dakota',\n",
       " 266: 'ez',\n",
       " 267: 'ways',\n",
       " 268: 'pay',\n",
       " 269: 'automated',\n",
       " 270: 'attendant',\n",
       " 271: 'payments',\n",
       " 272: 'please',\n",
       " 273: 'call',\n",
       " 274: 'upon',\n",
       " 275: 'receipt',\n",
       " 276: 'improved',\n",
       " 277: 'online',\n",
       " 278: 'experience',\n",
       " 279: '|',\n",
       " 280: 'update',\n",
       " 281: 'info',\n",
       " 282: 'see',\n",
       " 283: 'details',\n",
       " 284: 'back',\n",
       " 285: 'show',\n",
       " 286: 'proc',\n",
       " 287: 'units',\n",
       " 288: 'charges',\n",
       " 289: 'insur',\n",
       " 290: 'thorac',\n",
       " 291: 'spine',\n",
       " 292: 'commercial',\n",
       " 293: 'non',\n",
       " 294: 'ct',\n",
       " 295: 'abd',\n",
       " 296: 'pelv',\n",
       " 297: 'payment',\n",
       " 298: 'chest',\n",
       " 299: 'harges',\n",
       " 300: 'digit',\n",
       " 301: 'today',\n",
       " 302: \"'s\",\n",
       " 303: 'ethnicity',\n",
       " 304: 'hispanic',\n",
       " 305: 'latino',\n",
       " 306: 'preferred',\n",
       " 307: 'english',\n",
       " 308: 'suzanne',\n",
       " 309: 'newsom',\n",
       " 310: 'cnp',\n",
       " 311: '•',\n",
       " 312: 'lethargy',\n",
       " 313: 'cough',\n",
       " 314: 'vitals',\n",
       " 315: 'lbs',\n",
       " 316: 'kg',\n",
       " 317: 'wt',\n",
       " 318: 'temp',\n",
       " 319: 'hr',\n",
       " 320: 'oxygen',\n",
       " 321: 'sat',\n",
       " 322: 'allergies',\n",
       " 323: 'amoxicillin',\n",
       " 324: 'rash',\n",
       " 325: 'possible',\n",
       " 326: 'hives',\n",
       " 327: 'active',\n",
       " 328: 'diagnoses',\n",
       " 329: 'include',\n",
       " 330: 'frontal',\n",
       " 331: 'sinusitis',\n",
       " 332: 'unspecified',\n",
       " 333: 'dizziness',\n",
       " 334: 'giddiness',\n",
       " 335: 'medication',\n",
       " 336: 'list',\n",
       " 337: 'medications',\n",
       " 338: 'taking',\n",
       " 339: 'zyrtec',\n",
       " 340: 'childrens',\n",
       " 341: 'allergy',\n",
       " 342: 'notes',\n",
       " 343: 'tests',\n",
       " 344: 'labs',\n",
       " 345: 'illumigene',\n",
       " 346: 'myco',\n",
       " 347: 'http',\n",
       " 348: 'basic',\n",
       " 349: 'metabolic',\n",
       " 350: 'sodium',\n",
       " 351: 'range',\n",
       " 352: 'potassium',\n",
       " 353: 'chloride',\n",
       " 354: 'glucose',\n",
       " 355: 'bun',\n",
       " 356: 'creatinine',\n",
       " 357: 'calcium',\n",
       " 358: 'crea',\n",
       " 359: 'ratio',\n",
       " 360: 'anion',\n",
       " 361: 'gap',\n",
       " 362: 'calc',\n",
       " 363: 'cbc',\n",
       " 364: 'diff',\n",
       " 365: 'wbc',\n",
       " 366: 'rbc',\n",
       " 367: 'hgb',\n",
       " 368: 'hct',\n",
       " 369: 'mcv',\n",
       " 370: 'fl',\n",
       " 371: 'mch',\n",
       " 372: 'pg',\n",
       " 373: 'mchc',\n",
       " 374: 'mpv',\n",
       " 375: 'platelets',\n",
       " 376: 'neutrophils',\n",
       " 377: 'lymphocytes',\n",
       " 378: 'monocytes',\n",
       " 379: 'conditions',\n",
       " 380: 'problem',\n",
       " 381: 'idiopathic',\n",
       " 382: 'urticaria',\n",
       " 383: 'document',\n",
       " 384: 'wish',\n",
       " 385: 'keep',\n",
       " 386: 'policyholder',\n",
       " 387: 'owner',\n",
       " 388: 'eastside',\n",
       " 389: 'acct',\n",
       " 390: 'jasminder',\n",
       " 391: 'singh',\n",
       " 392: 'dev',\n",
       " 393: 'pa',\n",
       " 394: 'excuse',\n",
       " 395: 'east',\n",
       " 396: 'side',\n",
       " 397: 'april',\n",
       " 398: 'weekly',\n",
       " 399: 'ph',\n",
       " 400: 'mr',\n",
       " 401: 'primary',\n",
       " 402: 'thoracic',\n",
       " 403: 'strain',\n",
       " 404: 'strained',\n",
       " 405: 'following',\n",
       " 406: 'occurs',\n",
       " 407: 'feel',\n",
       " 408: 'weakness',\n",
       " 409: 'arms',\n",
       " 410: 'legs',\n",
       " 411: 'severe',\n",
       " 412: 'increase',\n",
       " 413: 'pain',\n",
       " 414: 'lumbosacral',\n",
       " 415: 'weak',\n",
       " 416: 'becomes',\n",
       " 417: 'follow',\n",
       " 418: 'take',\n",
       " 419: 'directed',\n",
       " 420: 'additional',\n",
       " 421: 'prescriptions',\n",
       " 422: 'prescriber',\n",
       " 423: 'paper',\n",
       " 424: 'prescription',\n",
       " 425: 'given',\n",
       " 426: 'preventative',\n",
       " 427: 'instructions',\n",
       " 428: 'diagnosis',\n",
       " 429: 'knee',\n",
       " 430: 'david',\n",
       " 431: 'bruce',\n",
       " 432: 'identiﬁer',\n",
       " 433: 'june',\n",
       " 434: 'concussion',\n",
       " 435: 'devin',\n",
       " 436: 'conrad',\n",
       " 437: 'september',\n",
       " 438: 'form',\n",
       " 439: 'attending',\n",
       " 440: 'part',\n",
       " 441: 'completed',\n",
       " 442: 'icd',\n",
       " 443: 'unable',\n",
       " 444: 'expected',\n",
       " 445: 'delivery',\n",
       " 446: 'actual',\n",
       " 447: 'vaginal',\n",
       " 448: 'per',\n",
       " 449: 'continued',\n",
       " 450: 'facility',\n",
       " 451: 'state',\n",
       " 452: 'zip',\n",
       " 453: 'performed',\n",
       " 454: 'surgical',\n",
       " 455: 'cpt',\n",
       " 456: 'degree',\n",
       " 457: 'check',\n",
       " 458: 'filing',\n",
       " 459: 'b',\n",
       " 460: 'suffix',\n",
       " 461: 'mi',\n",
       " 462: 'spanish',\n",
       " 463: 'short',\n",
       " 464: 'term',\n",
       " 465: 'disability',\n",
       " 466: 'long',\n",
       " 467: 'life',\n",
       " 468: 'insurance',\n",
       " 469: 'voluntary',\n",
       " 470: 'motor',\n",
       " 471: 'vehicle',\n",
       " 472: 'physicians',\n",
       " 473: 'hospitals',\n",
       " 474: 'considerations',\n",
       " 475: 'male',\n",
       " 476: 'female',\n",
       " 477: 'x',\n",
       " 478: 'member',\n",
       " 479: 'relationship',\n",
       " 480: 'person',\n",
       " 481: 'marital',\n",
       " 482: 'single',\n",
       " 483: 'occ',\n",
       " 484: 'title',\n",
       " 485: 'resinmixer',\n",
       " 486: 'hire',\n",
       " 487: 'termination',\n",
       " 488: 'atw',\n",
       " 489: 'limitations',\n",
       " 490: 'permitted',\n",
       " 491: 'months',\n",
       " 492: 'office',\n",
       " 493: 'crane',\n",
       " 494: 'composites',\n",
       " 495: 'florence',\n",
       " 496: 'earn',\n",
       " 497: 'change',\n",
       " 498: 'leave',\n",
       " 499: 'absence',\n",
       " 500: 'record',\n",
       " 501: 'loaded',\n",
       " 502: 'residence',\n",
       " 503: 'physical',\n",
       " 504: 'access',\n",
       " 505: 'home',\n",
       " 506: 'supervisor',\n",
       " 507: 'coverages',\n",
       " 508: 'product',\n",
       " 509: 'flex',\n",
       " 510: 'funding',\n",
       " 511: 'fully',\n",
       " 512: 'division',\n",
       " 513: 'peg',\n",
       " 514: 'eff',\n",
       " 515: 'earnings',\n",
       " 516: 'hourly',\n",
       " 517: 'mode',\n",
       " 518: 'aso',\n",
       " 519: 'self',\n",
       " 520: 'probable',\n",
       " 521: 'duration',\n",
       " 522: 'condition',\n",
       " 523: 'dates',\n",
       " 524: 'admission',\n",
       " 525: 'treatment',\n",
       " 526: 'seen',\n",
       " 527: 'past',\n",
       " 528: 'anticipated',\n",
       " 529: 'nature',\n",
       " 530: 'estimated',\n",
       " 531: 'treatments',\n",
       " 532: 'job',\n",
       " 533: 'attached',\n",
       " 534: 'checked',\n",
       " 535: 'episodic',\n",
       " 536: 'flare',\n",
       " 537: 'ups',\n",
       " 538: 'practice',\n",
       " 539: 'care',\n",
       " 540: 'including',\n",
       " 541: 'confinement',\n",
       " 542: 'provide',\n",
       " 543: 'advice',\n",
       " 544: 'stop',\n",
       " 545: 'mgmt',\n",
       " 546: 'svc',\n",
       " 547: 'applicable',\n",
       " 548: 'deductions',\n",
       " 549: 'schedule',\n",
       " 550: 'week',\n",
       " 551: 'sick',\n",
       " 552: 'variable',\n",
       " 553: 'sunday',\n",
       " 554: 'monday',\n",
       " 555: 'tuesday',\n",
       " 556: 'wednesday',\n",
       " 557: 'thursday',\n",
       " 558: 'friday',\n",
       " 559: 'saturday',\n",
       " 560: 'hospitalized',\n",
       " 561: 'explain',\n",
       " 562: 'height',\n",
       " 563: 'weight',\n",
       " 564: 'secondary',\n",
       " 565: 'functional',\n",
       " 566: 'capacity',\n",
       " 567: 'restrictions',\n",
       " 568: 'elizabeth',\n",
       " 569: 'edgewood',\n",
       " 570: 'facesheet',\n",
       " 571: 'sex',\n",
       " 572: 'adm',\n",
       " 573: 'demographics',\n",
       " 574: 'ssn',\n",
       " 575: 'reg',\n",
       " 576: 'verified',\n",
       " 577: 'pcp',\n",
       " 578: 'renew',\n",
       " 579: 'admitting',\n",
       " 580: 'larkin',\n",
       " 581: 'john',\n",
       " 582: 'md',\n",
       " 583: 'elective',\n",
       " 584: 'incomplete',\n",
       " 585: 'area',\n",
       " 586: 'edg',\n",
       " 587: 'sc',\n",
       " 588: 'crestview',\n",
       " 589: 'discharged',\n",
       " 590: 'confirmed',\n",
       " 591: 'hose',\n",
       " 592: 'ital',\n",
       " 593: 'class',\n",
       " 594: 'guarantor',\n",
       " 595: 'relation',\n",
       " 596: 'pt',\n",
       " 597: 'seh',\n",
       " 598: 'precert',\n",
       " 599: 'subscriber',\n",
       " 600: 'operative',\n",
       " 601: 'brief',\n",
       " 602: 'op',\n",
       " 603: 'note',\n",
       " 604: 'author',\n",
       " 605: 'j',\n",
       " 606: 'filed',\n",
       " 607: 'editor',\n",
       " 608: 'healthcare',\n",
       " 609: 'body',\n",
       " 610: 'mass',\n",
       " 611: 'index',\n",
       " 612: 'role',\n",
       " 613: 'anesthesia',\n",
       " 614: 'general',\n",
       " 615: 'specimens',\n",
       " 616: 'log',\n",
       " 617: 'blood',\n",
       " 618: 'loss',\n",
       " 619: 'course',\n",
       " 620: 'pacu',\n",
       " 621: 'operation',\n",
       " 622: 'best',\n",
       " 623: 'reached',\n",
       " 624: 'broken',\n",
       " 625: 'big',\n",
       " 626: 'toe',\n",
       " 627: 'foot',\n",
       " 628: 'todd',\n",
       " 629: 'francis',\n",
       " 630: 'podiatrist',\n",
       " 631: 'ryan',\n",
       " 632: 'kish',\n",
       " 633: 'toledo',\n",
       " 634: 'er',\n",
       " 635: 'xray',\n",
       " 636: 'july',\n",
       " 637: 'paramount',\n",
       " 638: 'promedica',\n",
       " 639: 'authorization',\n",
       " 640: 'modifier',\n",
       " 641: 'lt',\n",
       " 642: 'indicates',\n",
       " 643: 'c',\n",
       " 644: 'dpm',\n",
       " 645: 'ems',\n",
       " 646: 'christine',\n",
       " 647: 'nolen',\n",
       " 648: 'waukesha',\n",
       " 649: 'memorial',\n",
       " 650: 'cleaning',\n",
       " 651: 'bandage',\n",
       " 652: 'montano',\n",
       " 653: 'ci',\n",
       " 654: 'web',\n",
       " 655: 'user',\n",
       " 656: 'prohealth',\n",
       " 657: 'umr',\n",
       " 658: 'adjustments',\n",
       " 659: 'patients',\n",
       " 660: 'invoice',\n",
       " 661: 'previous',\n",
       " 662: 'return',\n",
       " 663: 'portion',\n",
       " 664: 'mastercard',\n",
       " 665: 'discover',\n",
       " 666: 'visa',\n",
       " 667: 'american',\n",
       " 668: 'express',\n",
       " 669: 'card',\n",
       " 670: 'enclosed',\n",
       " 671: 'emergency',\n",
       " 672: 'associates',\n",
       " 673: 'using',\n",
       " 674: 'addressee',\n",
       " 675: 'industrial',\n",
       " 676: 'loop',\n",
       " 677: 'dr',\n",
       " 678: 'ps',\n",
       " 679: 'dept',\n",
       " 680: 'ppo',\n",
       " 681: 'adj',\n",
       " 682: 'applied',\n",
       " 683: 'rendered',\n",
       " 684: 'fiserv',\n",
       " 685: 'wi',\n",
       " 686: 'stmt',\n",
       " 687: 'doctor',\n",
       " 688: 'legend',\n",
       " 689: 'd',\n",
       " 690: 'comments',\n",
       " 691: 'souha',\n",
       " 692: 'hakim',\n",
       " 693: 'medexpress',\n",
       " 694: 'codes',\n",
       " 695: 'urgent',\n",
       " 696: 'clairn',\n",
       " 697: 'vijay',\n",
       " 698: 'patel',\n",
       " 699: 'holder',\n",
       " 700: 'qty',\n",
       " 701: 'clinical',\n",
       " 702: 'chief',\n",
       " 703: 'taken',\n",
       " 704: 'bp',\n",
       " 705: 'mmhg',\n",
       " 706: 'pulse',\n",
       " 707: 'bpm',\n",
       " 708: 'resp',\n",
       " 709: 'lb',\n",
       " 710: 'ft',\n",
       " 711: 'bmi',\n",
       " 712: 'meds',\n",
       " 713: 'albuterol',\n",
       " 714: 'sulfate',\n",
       " 715: 'encounter',\n",
       " 716: 'progress',\n",
       " 717: 'subjective',\n",
       " 718: 'history',\n",
       " 719: 'provided',\n",
       " 720: 'dad',\n",
       " 721: 'interpreter',\n",
       " 722: 'used',\n",
       " 723: 'presents',\n",
       " 724: 'review',\n",
       " 725: 'systems',\n",
       " 726: 'cardiovascular',\n",
       " 727: 'negative',\n",
       " 728: 'skin',\n",
       " 729: 'neurological',\n",
       " 730: 'headaches',\n",
       " 731: 'objective',\n",
       " 732: 'hent',\n",
       " 733: 'right',\n",
       " 734: 'ear',\n",
       " 735: 'tympanic',\n",
       " 736: 'membrane',\n",
       " 737: 'normal',\n",
       " 738: 'nose',\n",
       " 739: 'oropharynx',\n",
       " 740: 'clear',\n",
       " 741: 'eyes',\n",
       " 742: 'conjunctivae',\n",
       " 743: 'eom',\n",
       " 744: 'neck',\n",
       " 745: 'supple',\n",
       " 746: 'rigidity',\n",
       " 747: 'murmur',\n",
       " 748: 'heard',\n",
       " 749: 'lymphadenopathy',\n",
       " 750: 'cervical',\n",
       " 751: 'adenopathy',\n",
       " 752: 'alert',\n",
       " 753: 'warm',\n",
       " 754: 'noted',\n",
       " 755: 'assessment',\n",
       " 756: 'advise',\n",
       " 757: 'ss',\n",
       " 758: 'pchc',\n",
       " 759: 'penobscot',\n",
       " 760: 'community',\n",
       " 761: 'suite',\n",
       " 762: 'medicine',\n",
       " 763: 'mental',\n",
       " 764: 'transmission',\n",
       " 765: 'sheet',\n",
       " 766: 'ext',\n",
       " 767: 're',\n",
       " 768: 'pages',\n",
       " 769: 'cover',\n",
       " 770: 'thank',\n",
       " 771: 'revised',\n",
       " 772: 'imaging',\n",
       " 773: 'mainecare',\n",
       " 774: 'fqhc',\n",
       " 775: 'low',\n",
       " 776: 'jt',\n",
       " 777: 'erin',\n",
       " 778: 'barker',\n",
       " 779: 'joseph',\n",
       " 780: 'ordering',\n",
       " 781: 'intercondylar',\n",
       " 782: 'space',\n",
       " 783: 'acl',\n",
       " 784: 'pcl',\n",
       " 785: 'intact',\n",
       " 786: 'unremarkable',\n",
       " 787: 'marrow',\n",
       " 788: 'signal',\n",
       " 789: 'dictation',\n",
       " 790: 'location',\n",
       " 791: 'mpsynernet',\n",
       " 792: 'reading',\n",
       " 793: 'kasper',\n",
       " 794: 'jared',\n",
       " 795: 'orthopedics',\n",
       " 796: 'sports',\n",
       " 797: 'np',\n",
       " 798: 'thompson',\n",
       " 799: 'mcguire',\n",
       " 800: 'initial',\n",
       " 801: 'evaluation',\n",
       " 802: 'responsible',\n",
       " 803: 'samara',\n",
       " 804: 'shiromani',\n",
       " 805: 'cc',\n",
       " 806: 'present',\n",
       " 807: 'illness',\n",
       " 808: 'persistent',\n",
       " 809: 'patella',\n",
       " 810: 'alta',\n",
       " 811: 'tendonitis',\n",
       " 812: 'asthma',\n",
       " 813: 'fractures',\n",
       " 814: 'changes',\n",
       " 815: 'father',\n",
       " 816: 'htn',\n",
       " 817: 'sister',\n",
       " 818: 'factor',\n",
       " 819: 'v',\n",
       " 820: 'seizures',\n",
       " 821: 'hs',\n",
       " 822: 'risk',\n",
       " 823: 'factors',\n",
       " 824: 'tobacco',\n",
       " 825: 'use',\n",
       " 826: 'never',\n",
       " 827: 'smoker',\n",
       " 828: 'passive',\n",
       " 829: 'smoke',\n",
       " 830: 'exposure',\n",
       " 831: 'alcohol',\n",
       " 832: 'drug',\n",
       " 833: 'caffeine',\n",
       " 834: 'drinks',\n",
       " 835: 'cellular',\n",
       " 836: 'medsupport',\n",
       " 837: 'exercise',\n",
       " 838: 'times',\n",
       " 839: 'field',\n",
       " 840: 'hockey',\n",
       " 841: 'cardio',\n",
       " 842: 'seatbelt',\n",
       " 843: 'sun',\n",
       " 844: 'occasionally',\n",
       " 845: 'fall',\n",
       " 846: 'problems',\n",
       " 847: 'dx',\n",
       " 848: 'critical',\n",
       " 849: 'chemicals',\n",
       " 850: 'zithromax',\n",
       " 851: 'azithromycin',\n",
       " 852: 'adhesive',\n",
       " 853: 'tape',\n",
       " 854: 'ent',\n",
       " 855: 'cv',\n",
       " 856: 'exertion',\n",
       " 857: 'complains',\n",
       " 858: 'gi',\n",
       " 859: 'gu',\n",
       " 860: 'ms',\n",
       " 861: 'denies',\n",
       " 862: 'varicose',\n",
       " 863: 'veins',\n",
       " 864: 'bone',\n",
       " 865: 'deformity',\n",
       " 866: 'derm',\n",
       " 867: 'neuro',\n",
       " 868: 'psych',\n",
       " 869: 'post',\n",
       " 870: 'traumatic',\n",
       " 871: 'stress',\n",
       " 872: 'disorder',\n",
       " 873: 'endo',\n",
       " 874: 'heme',\n",
       " 875: 'vital',\n",
       " 876: 'signs',\n",
       " 877: 'profile',\n",
       " 878: 'inches',\n",
       " 879: 'pounds',\n",
       " 880: 'rate',\n",
       " 881: 'minute',\n",
       " 882: 'sitting',\n",
       " 883: 'arm',\n",
       " 884: 'r',\n",
       " 885: 'intensity',\n",
       " 886: 'aching',\n",
       " 887: 'sharp',\n",
       " 888: 'lungs',\n",
       " 889: 'bilaterally',\n",
       " 890: 'p',\n",
       " 891: 'heart',\n",
       " 892: 'recommendations',\n",
       " 893: 'james',\n",
       " 894: 'greene',\n",
       " 895: 'orders',\n",
       " 896: 'assistant',\n",
       " 897: 'room',\n",
       " 898: 'diagnostic',\n",
       " 899: 'arthroscopy',\n",
       " 900: 'danielle',\n",
       " 901: 'st',\n",
       " 902: 'onge',\n",
       " 903: 'femoral',\n",
       " 904: 'block',\n",
       " 905: 'pac',\n",
       " 906: 'chondromalacia',\n",
       " 907: 'burning',\n",
       " 908: 'entered',\n",
       " 909: 'amy',\n",
       " 910: 'cyr',\n",
       " 911: 'progressing',\n",
       " 912: 'well',\n",
       " 913: 'postoperatively',\n",
       " 914: 'mariah',\n",
       " 915: 'larsen',\n",
       " 916: 'ma',\n",
       " 917: 'screening',\n",
       " 918: 'constitutional',\n",
       " 919: 'developed',\n",
       " 920: 'dtrs',\n",
       " 921: 'omega',\n",
       " 922: 'mg',\n",
       " 923: 'capsule',\n",
       " 924: 'one',\n",
       " 925: 'n',\n",
       " 926: 'counseling',\n",
       " 927: 'educational',\n",
       " 928: 'finley',\n",
       " 929: 'kevin',\n",
       " 930: 'bal',\n",
       " 931: 'item',\n",
       " 932: 'immunization',\n",
       " 933: 'admin',\n",
       " 934: 'ovr',\n",
       " 935: 'reach',\n",
       " 936: 'centers',\n",
       " 937: 'bethel',\n",
       " 938: 'year',\n",
       " 939: 'old',\n",
       " 940: 'started',\n",
       " 941: 'ago',\n",
       " 942: 'slipping',\n",
       " 943: 'ice',\n",
       " 944: 'tools',\n",
       " 945: 'screenings',\n",
       " 946: 'instrument',\n",
       " 947: 'score',\n",
       " 948: 'mdd',\n",
       " 949: 'classification',\n",
       " 950: 'questionnaire',\n",
       " 951: 'testing',\n",
       " 952: 'coronary',\n",
       " 953: 'disease',\n",
       " 954: 'order',\n",
       " 955: 'interpretation',\n",
       " 956: 'result',\n",
       " 957: 'region',\n",
       " 958: 'updated',\n",
       " 959: 'retired',\n",
       " 960: 'river',\n",
       " 961: 'sales',\n",
       " 962: 'manager',\n",
       " 963: 'support',\n",
       " 964: 'currently',\n",
       " 965: 'married',\n",
       " 966: 'smoking',\n",
       " 967: 'usage',\n",
       " 968: 'years',\n",
       " 969: 'pack',\n",
       " 970: 'prior',\n",
       " 971: 'sig',\n",
       " 972: 'desc',\n",
       " 973: 'start',\n",
       " 974: 'refilled',\n",
       " 975: 'elsewhere',\n",
       " 976: 'reconciliation',\n",
       " 977: 'reconciled',\n",
       " 978: 'ingredient',\n",
       " 979: 'reaction',\n",
       " 980: 'comment',\n",
       " 981: 'known',\n",
       " 982: 'system',\n",
       " 983: 'respiratory',\n",
       " 984: 'dyspnea',\n",
       " 985: 'integumentary',\n",
       " 986: 'positive',\n",
       " 987: 'standing',\n",
       " 988: 'dressed',\n",
       " 989: 'shoes',\n",
       " 990: 'pressure',\n",
       " 991: 'oral',\n",
       " 992: 'regular',\n",
       " 993: 'somatic',\n",
       " 994: 'rib',\n",
       " 995: 'dysfunction',\n",
       " 996: 'michael',\n",
       " 997: 'gould',\n",
       " 998: 'county',\n",
       " 999: 'potomac',\n",
       " ...}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize hierarichal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load char encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model = load_model(encoder_char_model_file.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 20)           0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             738889      lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 1024)         0           model_2[1][1]                    \n",
      "                                                                 model_2[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1024)         0           model_2[2][1]                    \n",
      "                                                                 model_2[2][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 1024)         0           model_2[3][1]                    \n",
      "                                                                 model_2[3][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 1024)         0           model_2[4][1]                    \n",
      "                                                                 model_2[4][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 1024)         0           model_2[5][1]                    \n",
      "                                                                 model_2[5][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 1024)         0           model_2[6][1]                    \n",
      "                                                                 model_2[6][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 1024)         0           model_2[7][1]                    \n",
      "                                                                 model_2[7][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 1024)         0           model_2[8][1]                    \n",
      "                                                                 model_2[8][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 1024)         0           model_2[9][1]                    \n",
      "                                                                 model_2[9][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 1024)         0           model_2[10][1]                   \n",
      "                                                                 model_2[10][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 1024)         0           model_2[11][1]                   \n",
      "                                                                 model_2[11][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 1024)         0           model_2[12][1]                   \n",
      "                                                                 model_2[12][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 1024)         0           model_2[13][1]                   \n",
      "                                                                 model_2[13][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 1024)         0           model_2[14][1]                   \n",
      "                                                                 model_2[14][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 1024)         0           model_2[15][1]                   \n",
      "                                                                 model_2[15][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_16 (Reshape)            (None, 1, 1024)      0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 1, 1024)      0           concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 1, 1024)      0           concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_19 (Reshape)            (None, 1, 1024)      0           concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_20 (Reshape)            (None, 1, 1024)      0           concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_21 (Reshape)            (None, 1, 1024)      0           concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_22 (Reshape)            (None, 1, 1024)      0           concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_23 (Reshape)            (None, 1, 1024)      0           concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_24 (Reshape)            (None, 1, 1024)      0           concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_25 (Reshape)            (None, 1, 1024)      0           concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_26 (Reshape)            (None, 1, 1024)      0           concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_27 (Reshape)            (None, 1, 1024)      0           concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_28 (Reshape)            (None, 1, 1024)      0           concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_29 (Reshape)            (None, 1, 1024)      0           concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_30 (Reshape)            (None, 1, 1024)      0           concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 15, 1024)     0           reshape_16[0][0]                 \n",
      "                                                                 reshape_17[0][0]                 \n",
      "                                                                 reshape_18[0][0]                 \n",
      "                                                                 reshape_19[0][0]                 \n",
      "                                                                 reshape_20[0][0]                 \n",
      "                                                                 reshape_21[0][0]                 \n",
      "                                                                 reshape_22[0][0]                 \n",
      "                                                                 reshape_23[0][0]                 \n",
      "                                                                 reshape_24[0][0]                 \n",
      "                                                                 reshape_25[0][0]                 \n",
      "                                                                 reshape_26[0][0]                 \n",
      "                                                                 reshape_27[0][0]                 \n",
      "                                                                 reshape_28[0][0]                 \n",
      "                                                                 reshape_29[0][0]                 \n",
      "                                                                 reshape_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) [(None, 15, 512), (N 2623488     concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 15, 1024)     1795072     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 512)          0           bidirectional_3[0][1]            \n",
      "                                                                 bidirectional_3[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 512)          0           bidirectional_3[0][2]            \n",
      "                                                                 bidirectional_3[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_4[0][0]                \n",
      "                                                                 concatenate_39[0][0]             \n",
      "                                                                 concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 15, 15)       0           lstm_6[0][0]                     \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15)       0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 15, 512)      0           activation_2[0][0]               \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 15, 1024)     0           dot_6[0][0]                      \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 15, 1753)     1796825     concatenate_41[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 10,102,050\n",
      "Trainable params: 10,092,249\n",
      "Non-trainable params: 9,801\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_hier_model(encoder_char_model=encoder_char_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_char_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "32000/32000 [==============================] - 380s 12ms/step - loss: 0.7699 - categorical_accuracy: 0.8290 - val_loss: 0.2767 - val_categorical_accuracy: 0.8442\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.84418, saving model to best_hier_model-50.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_6 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_39/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_40/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "32000/32000 [==============================] - 370s 12ms/step - loss: 0.3087 - categorical_accuracy: 0.8705 - val_loss: 0.3155 - val_categorical_accuracy: 0.8601\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.84418 to 0.86008, saving model to best_hier_model-50.hdf5\n",
      "Epoch 3/10\n",
      "32000/32000 [==============================] - 348s 11ms/step - loss: 0.2676 - categorical_accuracy: 0.8713 - val_loss: 0.2348 - val_categorical_accuracy: 0.8670\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.86008 to 0.86702, saving model to best_hier_model-50.hdf5\n",
      "Epoch 4/10\n",
      " 7296/32000 [=====>........................] - ETA: 4:10 - loss: 0.2413 - categorical_accuracy: 0.8769"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-ef072d4aad23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_char_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
