{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aelsalla\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, num_samples, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []   \n",
    "    \n",
    "    #for row in open(file_name, encoding='utf8'):\n",
    "    for row in open(file_name):\n",
    "        if cnt < num_samples :            \n",
    "            input_text = row           \n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len:\n",
    "                cnt += 1                \n",
    "                input_texts.append(input_text)\n",
    "    return input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    \n",
    "    if(len(input_texts) > max_encoder_seq_length):\n",
    "        input_texts = input_texts[:max_encoder_seq_length]\n",
    "    \n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        for t, char in enumerate(input_text[:max_encoder_seq_length]):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "                \n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, max_decoder_seq_length, vocab_to_int, int_to_vocab):\n",
    "    \n",
    "    #print(max_decoder_seq_length)\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', 'â€”' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#']\n",
    "    #special_chars = []\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        \n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            #print('End', sampled_char, 'Len ', len(decoded_sentence), 'Max len ', max_decoder_seq_length)\n",
    "            sampled_char = ''\n",
    "        \n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        \n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "        #decoded_sentence += sampled_char\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    \n",
    "    # Word level spell correct\n",
    "    '''\n",
    "    corrected_decoded_sentence = ''\n",
    "    for w in decoded_sentence.split(' '):\n",
    "        corrected_decoded_sentence += spell(w) + ' '\n",
    "    decoded_sentence = corrected_decoded_sentence\n",
    "    '''\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_spell_correct(decoded_sentence):\n",
    "    corrected_decoded_sentence = ''\n",
    "    special_chars = ['\\\\', '/', '-', 'â€”' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#']\n",
    "    for w in decoded_sentence.split(' '):\n",
    "        if((len(re.findall(r'\\d+', w))==0) and not (w in special_chars)):\n",
    "            corrected_decoded_sentence += spell(w) + ' '\n",
    "        else:\n",
    "            corrected_decoded_sentence += w + ' '\n",
    "    return corrected_decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_lengths = [50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = {}\n",
    "model_file = {}\n",
    "encoder_model_file = {}\n",
    "decoder_model_file = {}\n",
    "model = {}\n",
    "encoder_model = {}\n",
    "decoder_model = {}\n",
    "vocab = {}\n",
    "vocab_to_int = {}\n",
    "int_to_vocab = {}\n",
    "max_sent_len = {}\n",
    "min_sent_len = {}\n",
    "num_decoder_tokens = {}\n",
    "num_encoder_tokens = {}\n",
    "max_encoder_seq_length = {}\n",
    "max_decoder_seq_length = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aelsalla\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in max_sent_lengths:\n",
    "    vocab_file[i] = 'vocab-{}.npz'.format(i)\n",
    "    model_file[i] = 'best_model-{}.hdf5'.format(i)\n",
    "    encoder_model_file[i] = 'encoder_model-{}.hdf5'.format(i)\n",
    "    decoder_model_file[i] = 'decoder_model-{}.hdf5'.format(i)\n",
    "    \n",
    "    vocab = np.load(file=vocab_file[i])\n",
    "    vocab_to_int[i] = vocab['vocab_to_int'].item()\n",
    "    int_to_vocab[i] = vocab['int_to_vocab'].item()\n",
    "    max_sent_len[i] = vocab['max_sent_len']\n",
    "    min_sent_len[i] = vocab['min_sent_len']\n",
    "    input_characters = sorted(list(vocab_to_int))\n",
    "    num_decoder_tokens[i] = num_encoder_tokens[i] = len(input_characters) #int(encoder_model.layers[0].input.shape[2])\n",
    "    max_encoder_seq_length[i] = max_decoder_seq_length[i] = max_sent_len[i] - 1#max([len(txt) for txt in input_texts])\n",
    "    \n",
    "    model[i] = load_model(model_file[i])\n",
    "    encoder_model[i] = load_model(encoder_model_file[i])\n",
    "    decoder_model[i] = load_model(decoder_model_file[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "#tess_correction_data = os.path.join(data_path, 'test_data.txt')\n",
    "#input_texts = load_data(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "OCR_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "#input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)\n",
    "input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len=10000, min_sent_len=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1951\n",
      "Me dieal Provider Roles: Treating  \n",
      " \tMedical Provider Roles: Treating\n",
      "\n",
      "\n",
      "Provider First Name: Christine  \n",
      " \tProvider First Name: Christine\n",
      "\n",
      "\n",
      "Provider Last Name: Nolen, MD  \n",
      " \tProvider Last Name: Nolen, MD\n",
      "\n",
      "\n",
      "Address Line 1 : 7 25 American Avenue  \n",
      " \tAddress Line 1 : 725 American Avenue\n",
      "\n",
      "\n",
      "City. Wâ€™aukesha  \n",
      " \tCity: Waukesha\n",
      "\n",
      "\n",
      "StatefProvinee: â€˜WI  \n",
      " \tState/Province: WI\n",
      "\n",
      "\n",
      "Postal Code: 5 31 88  \n",
      " \tPostal Code: 53188\n",
      "\n",
      "\n",
      "Country\". US  \n",
      " \tCountry:  US\n",
      "\n",
      "\n",
      "Business Telephone: (2 62) 92 8- 1000  \n",
      " \tBusiness Telephone: (262) 928- 1000\n",
      "\n",
      "\n",
      "Date otâ€˜Pirst Visit: 1 2/01f20 17  \n",
      " \tDate of First Visit: 12/01/2017\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell correct before inference\n",
    "'''\n",
    "input_texts_ = []\n",
    "for sent in input_texts:\n",
    "    sent_ = ''\n",
    "    for word in sent.split(' '):\n",
    "        sent_ += spell(word) + ' '\n",
    "    input_texts_.append(sent_)\n",
    "input_texts = input_texts_\n",
    "input_texts_ = []\n",
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentences = []\n",
    "corrected_sentences = []\n",
    "\n",
    "#for seq_index in range(len(input_texts)):\n",
    "results = open('RESULTS.md', 'w')\n",
    "results.write('|OCR sentence|GT sentence|Char decoded sentence|Word decoded sentence|Sentence length (chars)|\\n')\n",
    "results.write('---------------|-----------|----------------|----------------|----------------|\\n')\n",
    "     \n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    #print(input_text)\n",
    "    # Find the input length range to choose the proper model to use\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "    \n",
    "    \n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "    \n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    print('GT sentence:', target_text.strip())\n",
    "    print('Char Decoded sentence:', decoded_sentence)   \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    results.write(' | ' + input_text + ' | ' + target_text.strip() + ' | ' + decoded_sentence + ' | ' + corrected_sentence + ' | ' + str(len_range) + ' | \\n')\n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    corrected_sentences.append(corrected_sentence)\n",
    "results.close()    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Unum Life Insurance Company of America 2211\n",
      "Char Decoded sentence: Umage Ordent Insurance Policy  \n",
      "Word Decoded sentence: Unum Life Insurance Company of America 2211 \n",
      "\n",
      "\n",
      "Input sentence: Congress Street Portland, Maine 04122\n",
      "Char Decoded sentence: Contracter Store Permiea, Maxidu04122RM\n",
      "Word Decoded sentence: Congress Street Portlands Maine 04122 \n",
      "\n",
      "\n",
      "Input sentence: APPLICATION FOR GROUP CRITICAL LLNESS INSURANCE\n",
      "Char Decoded sentence: APTLICT OF REPOITIONER'S PAIAL ERANT  NCURAPCE\n",
      "Word Decoded sentence: APPLICATION FOR GROUP CRITICAL ILLNESS INSURANCE \n",
      "\n",
      "\n",
      "Input sentence: I Evidence of Insurability\n",
      "Char Decoded sentence: Individual Insurance\n",
      "Word Decoded sentence: I Evidence of Insurability \n",
      "\n",
      "\n",
      "Input sentence: \n",
      "Char Decoded sentence: Surgery\n",
      "Word Decoded sentence: a \n",
      "\n",
      "\n",
      "Input sentence: Application Type: @ New Enrollee Change to\n",
      "Char Decoded sentence: Applicatic Type:Note E Complete Medication\n",
      "Word Decoded sentence: Application Type a New Enrollee Change to \n",
      "\n",
      "\n",
      "Input sentence: Existing Coverage  Reinstatement  Internal\n",
      "Char Decoded sentence: Ethning Coverage Tyme Pernorating Injurnerat Instr\n",
      "Word Decoded sentence: Existing Coverage a Reinstatement a Internal \n",
      "\n",
      "\n",
      "Input sentence: Replacement  Late Applicant  Rehire SECTION 1:\n",
      "Char Decoded sentence: Repartment Leate plate Report Medical Center1\n",
      "Word Decoded sentence: Replacement a Late Applicant a Rehire SECTION 1: \n",
      "\n",
      "\n",
      "Input sentence: Employee(Applicant) Information  Always\n",
      "Char Decoded sentence: Employee(Apon Inf)rmation  Alfer legore\n",
      "Word Decoded sentence: Employee(Applicant) Information a Always \n",
      "\n",
      "\n",
      "Input sentence: Complete Employee Name(First, Middle, Last)\n",
      "Char Decoded sentence: Complete Employee Name(mail, homps,n La)h\n",
      "Word Decoded sentence: Complete Employee Name(First, Middle Last \n",
      "\n",
      "\n",
      "Input sentence: Social Security Number Nikolas J Jones\n",
      "Char Decoded sentence: Social Security Number to be Reached Do\n",
      "Word Decoded sentence: Social Security Number Nikolas J Jones \n",
      "\n",
      "\n",
      "Input sentence: 123 - 456 - 7890 Home Address(Street/ PO Box)\n",
      "Char Decoded sentence: 123-456-7890Alder Date of(Birst /Contres)e I123\n",
      "Word Decoded sentence: 123 - 456 - 7890 Home Address(Street/ PO Box \n",
      "\n",
      "\n",
      "Input sentence: Gender 1634 Stewert St  F  M City Date of Birth\n",
      "Char Decoded sentence: Gender 1634tiab  t e  ta tin st pate of Birth\n",
      "Word Decoded sentence: Gender 1634 Stewart St a F a M City Date of Birth \n",
      "\n",
      "\n",
      "Input sentence: (mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip\n",
      "Char Decoded sentence: (mm/dyy/ Stat) Zip Stat06Z/p15 / 1991 \n",
      "Word Decoded sentence: mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip \n",
      "\n",
      "\n",
      "Input sentence: Code Home Phone # Washington 98101 854-555-1212\n",
      "Char Decoded sentence: Cholde Hore Phon#   Work 98101854-555-1212\n",
      "Word Decoded sentence: Code Home Phone # Washington 98101 854-555-1212 \n",
      "\n",
      "\n",
      "Input sentence: Are you Actively at Work? Employee ID / Payroll #\n",
      "Char Decoded sentence: Ard Outcorked to Work mmE pay ent Wor/?\n",
      "Word Decoded sentence: Are you Actively at Work Employee ID / Payroll # \n",
      "\n",
      "\n",
      "Input sentence:  Yes  No55624 a.Are you a U.S.Citizen or\n",
      "Char Decoded sentence: Is Pry o55624hi.d  Yes o. .ad Fat\n",
      "Word Decoded sentence: a Yes a No55624 aware you a U.S.Citizen or \n",
      "\n",
      "\n",
      "Input sentence: Canadian Citizen working in the U.S.? b.Are you\n",
      "Char Decoded sentence: Cardinarn Complete wiviey working.w.en .equired\n",
      "Word Decoded sentence: Canadian Citizen working in the U.S.? bare you \n",
      "\n",
      "\n",
      "Input sentence: legally authorized to work in  Yes  No(If No\n",
      "Char Decoded sentence: Relacian thithor dity od Not knee IC(or M N\n",
      "Word Decoded sentence: legally authorized to work in a Yes a Motif No \n",
      "\n",
      "\n",
      "Input sentence: reply to part b) the U.S.?  Yes  No Employer\n",
      "Char Decoded sentence: Nepartment pate)la am.u.t hame of  No Eamplo\n",
      "Word Decoded sentence: reply to part by the U.S.? a Yes a No Employer \n",
      "\n",
      "\n",
      "Input sentence: Name Group Number Date of Hire(mm/ dd / yyyy)\n",
      "Char Decoded sentence: Name of Group Name mmddyy( /oder/t mo)erate \n",
      "Word Decoded sentence: Name Group Number Date of Hire(mm/ dd / yyyy) \n",
      "\n",
      "\n",
      "Input sentence: Facebook 11 - 555566 11 / 30 / 2016 Occupation\n",
      "Char Decoded sentence: Faciborod11-55556611 /30 /g2016\n",
      "Word Decoded sentence: Facebook 11 - 555566 11 / 30 / 2016 Occupation \n",
      "\n",
      "\n",
      "Input sentence: Eligibility Class Software Engineer 7 Scheduled\n",
      "Char Decoded sentence: Elinible Claim SelfInsured holder F7red hnally\n",
      "Word Decoded sentence: Eligibility Class Software Engineer 7 Scheduled \n",
      "\n",
      "\n",
      "Input sentence: Number of Work Hours per Week Work Phone # 35\n",
      "Char Decoded sentence: Numer Work for Hour specient Work Pon riv#3\n",
      "Word Decoded sentence: Number of Work Hours per Week Work Phone # 35 \n",
      "\n",
      "\n",
      "Input sentence: 854-555-6622 SECTION 2: Spouse Information \n",
      "Char Decoded sentence: 854-555-6622 Insured 2:formation \n",
      "Word Decoded sentence: 854-555-6622 SECTION 2: Spouse Information a \n",
      "\n",
      "\n",
      "Input sentence: Complete Only if applying for Spouse coverage Name\n",
      "Char Decoded sentence: Complete Onlly Information for Spouse coverage Name\n",
      "Word Decoded sentence: Complete Only if applying for Spouse coverage Name \n",
      "\n",
      "\n",
      "Input sentence: (First, Middle, Last) Social Security Number\n",
      "Char Decoded sentence: (irst ,ide Mad, Sore) Surgery Number\n",
      "Word Decoded sentence: First Middle Last Social Security Number \n",
      "\n",
      "\n",
      "Input sentence: Gender Date of Birth(mm / dd / yyyy) Does the\n",
      "Char Decoded sentence: Gender Date of Birth(mm/ddy/ mo)erate \n",
      "Word Decoded sentence: Gender Date of Birth(mm / dd / yyyy) Does the \n",
      "\n",
      "\n",
      "Input sentence: 1019 - 07 - AZ 1\n",
      "Char Decoded sentence: 1019-07- 1 \n",
      "Word Decoded sentence: 1019 - 07 - AZ 1 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = ['Unum Life Insurance Company of America 2211',               \n",
    "               'Congress Street Portland, Maine 04122',\n",
    "               'APPLICATION FOR GROUP CRITICAL LLNESS INSURANCE',\n",
    "               'I Evidence of Insurability',\n",
    "               '',\n",
    "               'Application Type: @ New Enrollee Change to',\n",
    "               'Existing Coverage  Reinstatement  Internal',\n",
    "               'Replacement  Late Applicant  Rehire SECTION 1:',\n",
    "               'Employee(Applicant) Information  Always',\n",
    "               'Complete Employee Name(First, Middle, Last)',\n",
    "               'Social Security Number Nikolas J Jones',\n",
    "               '123 - 456 - 7890 Home Address(Street/ PO Box)',\n",
    "               'Gender 1634 Stewert St  F  M City Date of Birth',\n",
    "               '(mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip',\n",
    "               'Code Home Phone # Washington 98101 854-555-1212',\n",
    "               'Are you Actively at Work? Employee ID / Payroll #',\n",
    "               ' Yes  No55624 a.Are you a U.S.Citizen or',\n",
    "               'Canadian Citizen working in the U.S.? b.Are you',\n",
    "               'legally authorized to work in  Yes  No(If No',\n",
    "               'reply to part b) the U.S.?  Yes  No Employer',\n",
    "               'Name Group Number Date of Hire(mm/ dd / yyyy)',\n",
    "               'Facebook 11 - 555566 11 / 30 / 2016 Occupation',\n",
    "               'Eligibility Class Software Engineer 7 Scheduled',\n",
    "               'Number of Work Hours per Week Work Phone # 35',\n",
    "               '854-555-6622 SECTION 2: Spouse Information ',\n",
    "               'Complete Only if applying for Spouse coverage Name',\n",
    "               '(First, Middle, Last) Social Security Number',\n",
    "               'Gender Date of Birth(mm / dd / yyyy) Does the',\n",
    "               '1019 - 07 - AZ 1']\n",
    "               \n",
    "for input_text in input_texts:\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    pre_corrected_sentence = word_spell_correct(input_text)\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "\n",
    "\n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "\n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    #print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    #print('Spell Decoded sentence:', pre_corrected_sentence) \n",
    "    print('Char Decoded sentence:', decoded_sentence)   \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(gt_texts, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_word_correction = calculate_WER(gt_texts, corrected_sentences)\n",
    "print('WER_spell_word_correction |TEST= ', WER_spell_word_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(gt_texts, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
