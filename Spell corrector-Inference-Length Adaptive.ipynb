{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, num_samples, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []   \n",
    "    \n",
    "    #for row in open(file_name, encoding='utf8'):\n",
    "    for row in open(file_name):\n",
    "        if cnt < num_samples :            \n",
    "            input_text = row           \n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len:\n",
    "                cnt += 1                \n",
    "                input_texts.append(input_text)\n",
    "    return input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    \n",
    "    if(len(input_texts) > max_encoder_seq_length):\n",
    "        input_texts = input_texts[:max_encoder_seq_length]\n",
    "    \n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        for t, char in enumerate(input_text[:max_encoder_seq_length]):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "                \n",
    "    return encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, max_decoder_seq_length, vocab_to_int, int_to_vocab):\n",
    "    \n",
    "    #print(max_decoder_seq_length)\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#']\n",
    "    #special_chars = []\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        \n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        \n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            #print('End', sampled_char, 'Len ', len(decoded_sentence), 'Max len ', max_decoder_seq_length)\n",
    "            sampled_char = ''\n",
    "        \n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        \n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "        #decoded_sentence += sampled_char\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    \n",
    "    # Word level spell correct\n",
    "    '''\n",
    "    corrected_decoded_sentence = ''\n",
    "    for w in decoded_sentence.split(' '):\n",
    "        corrected_decoded_sentence += spell(w) + ' '\n",
    "    decoded_sentence = corrected_decoded_sentence\n",
    "    '''\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_spell_correct(decoded_sentence):\n",
    "    corrected_decoded_sentence = ''\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$', '#']\n",
    "    for w in decoded_sentence.split(' '):\n",
    "        if((len(re.findall(r'\\d+', w))==0) and not (w in special_chars)):\n",
    "            corrected_decoded_sentence += spell(w) + ' '\n",
    "        else:\n",
    "            corrected_decoded_sentence += w + ' '\n",
    "    return corrected_decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence, vocab):\n",
    "    s = ''\n",
    "    prev_char = ''\n",
    "    for c in sentence.strip():\n",
    "        if c not in vocab or (c == ' ' and prev_char == ' '):\n",
    "            s += ''\n",
    "        else:\n",
    "            s += c\n",
    "        prev_char = c\n",
    "            \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_lengths = [50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = {}\n",
    "model_file = {}\n",
    "encoder_model_file = {}\n",
    "decoder_model_file = {}\n",
    "model = {}\n",
    "encoder_model = {}\n",
    "decoder_model = {}\n",
    "vocab = {}\n",
    "vocab_to_int = {}\n",
    "int_to_vocab = {}\n",
    "max_sent_len = {}\n",
    "min_sent_len = {}\n",
    "num_decoder_tokens = {}\n",
    "num_encoder_tokens = {}\n",
    "max_encoder_seq_length = {}\n",
    "max_decoder_seq_length = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in max_sent_lengths:\n",
    "    vocab_file[i] = 'vocab-{}.npz'.format(i)\n",
    "    model_file[i] = 'best_model-{}.hdf5'.format(i)\n",
    "    encoder_model_file[i] = 'encoder_model-{}.hdf5'.format(i)\n",
    "    decoder_model_file[i] = 'decoder_model-{}.hdf5'.format(i)\n",
    "    \n",
    "    vocab = np.load(file=vocab_file[i])\n",
    "    vocab_to_int[i] = vocab['vocab_to_int'].item()\n",
    "    int_to_vocab[i] = vocab['int_to_vocab'].item()\n",
    "    max_sent_len[i] = vocab['max_sent_len']\n",
    "    min_sent_len[i] = vocab['min_sent_len']\n",
    "    input_characters = sorted(list(vocab_to_int))\n",
    "    num_decoder_tokens[i] = num_encoder_tokens[i] = len(input_characters) #int(encoder_model.layers[0].input.shape[2])\n",
    "    max_encoder_seq_length[i] = max_decoder_seq_length[i] = max_sent_len[i] - 1#max([len(txt) for txt in input_texts])\n",
    "    \n",
    "    model[i] = load_model(model_file[i])\n",
    "    encoder_model[i] = load_model(encoder_model_file[i])\n",
    "    decoder_model[i] = load_model(decoder_model_file[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "#tess_correction_data = os.path.join(data_path, 'test_data.txt')\n",
    "#input_texts = load_data(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "OCR_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "#input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)\n",
    "input_texts, target_texts, gt_texts = load_data_with_gt(OCR_data, num_samples, max_sent_len=10000, min_sent_len=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1951\n",
      "Me dieal Provider Roles: Treating  \n",
      " \tMedical Provider Roles: Treating\n",
      "\n",
      "\n",
      "Provider First Name: Christine  \n",
      " \tProvider First Name: Christine\n",
      "\n",
      "\n",
      "Provider Last Name: Nolen, MD  \n",
      " \tProvider Last Name: Nolen, MD\n",
      "\n",
      "\n",
      "Address Line 1 : 7 25 American Avenue  \n",
      " \tAddress Line 1 : 725 American Avenue\n",
      "\n",
      "\n",
      "City. W’aukesha  \n",
      " \tCity: Waukesha\n",
      "\n",
      "\n",
      "StatefProvinee: ‘WI  \n",
      " \tState/Province: WI\n",
      "\n",
      "\n",
      "Postal Code: 5 31 88  \n",
      " \tPostal Code: 53188\n",
      "\n",
      "\n",
      "Country\". US  \n",
      " \tCountry:  US\n",
      "\n",
      "\n",
      "Business Telephone: (2 62) 92 8- 1000  \n",
      " \tBusiness Telephone: (262) 928- 1000\n",
      "\n",
      "\n",
      "Date ot‘Pirst Visit: 1 2/01f20 17  \n",
      " \tDate of First Visit: 12/01/2017\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_texts_ = []\\nfor sent in input_texts:\\n    sent_ = ''\\n    for word in sent.split(' '):\\n        sent_ += spell(word) + ' '\\n    input_texts_.append(sent_)\\ninput_texts = input_texts_\\ninput_texts_ = []\\n# Sample data\\nprint(len(input_texts))\\nfor i in range(10):\\n    print(input_texts[i], '\\n', target_texts[i])\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spell correct before inference\n",
    "'''\n",
    "input_texts_ = []\n",
    "for sent in input_texts:\n",
    "    sent_ = ''\n",
    "    for word in sent.split(' '):\n",
    "        sent_ += spell(word) + ' '\n",
    "    input_texts_.append(sent_)\n",
    "input_texts = input_texts_\n",
    "input_texts_ = []\n",
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Lenght =  50\n",
      "Input sentence: Me dieal Provider Roles: Treating\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Char Decoded sentence: Medical Provider Roles:Treating\n",
      "Word Decoded sentence: Me deal Provider Roles Treating \n",
      "-Lenght =  50\n",
      "Input sentence: Provider First Name: Christine\n",
      "GT sentence: Provider First Name: Christine\n",
      "Char Decoded sentence: Provider First Name: Christine\n",
      "Word Decoded sentence: Provider First Name Christine \n",
      "-Lenght =  50\n",
      "Input sentence: Provider Last Name: Nolen, MD\n",
      "GT sentence: Provider Last Name: Nolen, MD\n",
      "Char Decoded sentence: Provider Last Name: Norle, MD\n",
      "Word Decoded sentence: Provider Last Name Dolens MD \n",
      "-Lenght =  50\n",
      "Input sentence: Address Line 1 : 7 25 American Avenue\n",
      "GT sentence: Address Line 1 : 725 American Avenue\n",
      "Char Decoded sentence: Address Line 1:7A25ent Admentine Ave\n",
      "Word Decoded sentence: Address Line 1 : 7 25 American Avenue \n",
      "-Lenght =  50\n",
      "Input sentence: City. W’aukesha\n",
      "GT sentence: City: Waukesha\n",
      "Char Decoded sentence: City.States\n",
      "Word Decoded sentence: City Waukesha \n",
      "-Lenght =  50\n",
      "Input sentence: StatefProvinee: ‘WI\n",
      "GT sentence: State/Province: WI\n",
      "Char Decoded sentence: StateProvine:En\n",
      "Word Decoded sentence: StatefProvinee: DWI \n",
      "-Lenght =  50\n",
      "Input sentence: Postal Code: 5 31 88\n",
      "GT sentence: Postal Code: 53188\n",
      "Char Decoded sentence: Postal Code: 5 3188\n",
      "Word Decoded sentence: Postal Codes 5 31 88 \n",
      "-Lenght =  50\n",
      "Input sentence: Country\". US\n",
      "GT sentence: Country:  US\n",
      "Char Decoded sentence: Country\".S\n",
      "Word Decoded sentence: Country US \n",
      "-Lenght =  50\n",
      "Input sentence: Business Telephone: (2 62) 92 8- 1000\n",
      "GT sentence: Business Telephone: (262) 928- 1000\n",
      "Char Decoded sentence: Business Telephone: (262)928-1000\n",
      "Word Decoded sentence: Business Telephone (2 62) 92 8- 1000 \n",
      "-Lenght =  50\n",
      "Input sentence: Date ot‘Pirst Visit: 1 2/01f20 17\n",
      "GT sentence: Date of First Visit: 12/01/2017\n",
      "Char Decoded sentence: Date to Pisit Visit: 12/012017q\n",
      "Word Decoded sentence: Date ot‘Pirst Visit 1 2/01f20 17 \n",
      "-Lenght =  50\n",
      "Input sentence: Medical Protitler Information — Hospitalization\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Char Decoded sentence: Medical Provider Information —Pospitaliz Dation\n",
      "Word Decoded sentence: Medical Profiteer Information — Hospitalization \n",
      "-Lenght =  50\n",
      "Input sentence: Hospital Name: W'aukesha Memorial Hospital\n",
      "GT sentence: Hospital Name: Waukesha Memorial Hospital\n",
      "Char Decoded sentence: ospital Name: Faumesta Memprisal orstall m\n",
      "Word Decoded sentence: Hospital Name Waukesha Memorial Hospital \n",
      "-Lenght =  50\n",
      "Input sentence: Address Line 1 : 7\" 25 Arnerie an Drive\n",
      "GT sentence: Address Line 1 : 725 American Drive\n",
      "Char Decoded sentence: Address Line 1:7\" 25 Araine an Drive\n",
      "Word Decoded sentence: Address Line 1 : 7\" 25 Arteries an Drive \n",
      "-Lenght =  50\n",
      "Input sentence: City. ‘Waukesha\n",
      "GT sentence: City: Waukesha\n",
      "Char Decoded sentence: City.Stauk ualse\n",
      "Word Decoded sentence: City Waukesha \n",
      "-Lenght =  50\n",
      "Input sentence: StatefProﬁnoe: W'I\n",
      "GT sentence: State/Province: WI\n",
      "Char Decoded sentence: StateProvinc:\n",
      "Word Decoded sentence: StatefProﬁnoe: Wei \n",
      "-Lenght =  50\n",
      "Input sentence: Postal Code: 5 31 88\n",
      "GT sentence: Postal Code: 53188\n",
      "Char Decoded sentence: Postal Code: 5 3188\n",
      "Word Decoded sentence: Postal Codes 5 31 88 \n",
      "-Lenght =  50\n",
      "Input sentence: Country. US\n",
      "GT sentence: Country: US\n",
      "Char Decoded sentence: Country.UuS\n",
      "Word Decoded sentence: Country US \n",
      "-Lenght =  50\n",
      "Input sentence: Claim Type: VB Accident - Accidental Injury\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Char Decoded sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Word Decoded sentence: Claim Type VB Accident - Accidental Injury \n",
      "-Lenght =  100\n",
      "Input sentence: Who The Reporled ETEIII Happened To: ErrployeefPolicyholders Child\n",
      "GT sentence: Who The Reported Event Happened To: Employee/Policyholder's child\n",
      "Char Decoded sentence: Who The Reported ETTHEIIT Happened :o EmployeePolicyholder Child\n",
      "Word Decoded sentence: Who The Reported ETEIII Happened To ErrployeefPolicyholders Child \n",
      "-Lenght =  50\n",
      "Input sentence: Policyhold El':\"0“1l€l' In form ariorl\n",
      "GT sentence: Policyholder/Owner Information\n",
      "Char Decoded sentence: Policyholder :\"0s1ring Information\n",
      "Word Decoded sentence: Policyholder El':\"0“1l€l' In form prior \n",
      "-Lenght =  50\n",
      "Input sentence: First Name:\n",
      "GT sentence: First Name:\n",
      "Char Decoded sentence: First Name:\n",
      "Word Decoded sentence: First Name \n",
      "-Lenght =  50\n",
      "Input sentence: Middle Narmflnitial:\n",
      "GT sentence: Middle Name/Initial:\n",
      "Char Decoded sentence: Middle NameInitial:\n",
      "Word Decoded sentence: Middle Narmflnitial: \n",
      "-Lenght =  50\n",
      "Input sentence: Last Name:\n",
      "GT sentence: Last Name:\n",
      "Char Decoded sentence: Last Name:\n",
      "Word Decoded sentence: Last Name \n",
      "-Lenght =  50\n",
      "Input sentence: Social 8 ecurity Number:\n",
      "GT sentence: Social Security Number:\n",
      "Char Decoded sentence: Social 8 ecurity Numbe:\n",
      "Word Decoded sentence: Social 8 security Number \n",
      "-Lenght =  50\n",
      "Input sentence: Birth Date:\n",
      "GT sentence: Birth Date:\n",
      "Char Decoded sentence: Birth Date:\n",
      "Word Decoded sentence: Birth Date \n",
      "-Lenght =  50\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "Char Decoded sentence: Gender:\n",
      "Word Decoded sentence: Gender \n",
      "-Lenght =  50\n",
      "Input sentence: Language Preference:\n",
      "GT sentence: Language Preference:\n",
      "Char Decoded sentence: Language Preference:\n",
      "Word Decoded sentence: Language Preference \n",
      "-Lenght =  50\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Char Decoded sentence: Address Line 1:\n",
      "Word Decoded sentence: Address Line 1: \n",
      "-Lenght =  50\n",
      "Input sentence: CW-\n",
      "GT sentence: City:\n",
      "Char Decoded sentence: Co-CA\n",
      "Word Decoded sentence: CWC \n",
      "-Lenght =  50\n",
      "Input sentence: StatefProvince :\n",
      "GT sentence: State/Province:\n",
      "Char Decoded sentence: StateProvince:\n",
      "Word Decoded sentence: StatefProvince : \n",
      "-Lenght =  50\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "Char Decoded sentence: Postal Code:\n",
      "Word Decoded sentence: Postal Codes \n",
      "-Lenght =  50\n",
      "Input sentence: Country\n",
      "GT sentence: Country:\n",
      "Char Decoded sentence: Country\n",
      "Word Decoded sentence: Country \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ec4367d43179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(max_decoder_seq_length[len_range])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#print(max_decoder_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mcorrected_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_spell_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-Lenght = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ba4192e10587>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq, encoder_model, decoder_model, num_decoder_tokens, max_decoder_seq_length, vocab_to_int, int_to_vocab)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(target_seq)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         output_tokens, attention, h, c  = decoder_model.predict(\n\u001b[0;32m---> 24\u001b[0;31m             [target_seq, encoder_outputs] + states_value)\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print(attention.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mattention_density\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoded_sentences = []\n",
    "corrected_sentences = []\n",
    "\n",
    "#for seq_index in range(len(input_texts)):\n",
    "results = open('RESULTS.md', 'w')\n",
    "results.write('|OCR sentence|GT sentence|Char decoded sentence|Word decoded sentence|Sentence length (chars)|\\n')\n",
    "results.write('---------------|-----------|----------------|----------------|----------------|\\n')\n",
    "     \n",
    "\n",
    "for i, input_text in enumerate(input_texts):\n",
    "    #print(input_text)\n",
    "    # Find the input length range to choose the proper model to use\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    \n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "    \n",
    "    \n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "    \n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    print('GT sentence:', target_text.strip())\n",
    "    print('Char Decoded sentence:', decoded_sentence)   \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    results.write(' | ' + input_text + ' | ' + target_text.strip() + ' | ' + decoded_sentence + ' | ' + corrected_sentence + ' | ' + str(len_range) + ' | \\n')\n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    corrected_sentences.append(corrected_sentence)\n",
    "results.close()    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: SUBJECTIVE: This is a S-year-old +@W his left great toe with the handleh lacration.\n",
      "Word Decoded sentence: SUBJECTIVES This is a S-year-old New his left great toe with the handled laceration \n",
      "\n",
      "\n",
      "Input sentence: Thera was no handlebarthe lacration.\n",
      "Word Decoded sentence: Thera was no handlebarthe laceration \n",
      "\n",
      "\n",
      "Input sentence: Patiet last tet is needing this for school at this\n",
      "Word Decoded sentence: Patient last tet is needing this for school at this \n",
      "\n",
      "\n",
      "Input sentence: OBJECTIVE : The temp is 99.8, the f tha blood pressure 99/64, O2 sat 94 8/10 at this time.\n",
      "Word Decoded sentence: OBJECTIVE : The temp is 99.8, the f tha blood pressure 99/64, O2 sat 94 8/10 at this time \n",
      "\n",
      "\n",
      "Input sentence: Left great toe the dorsl surface, extending ta th active hemorrhage at this time.\n",
      "Word Decoded sentence: Left great toe the dorsal surface extending ta th active hemorrhage at this time \n",
      "\n",
      "\n",
      "Input sentence: Th anaathetized with a cotton ball sat Left this in place for 20 minutes.\n",
      "Word Decoded sentence: Th anaathetized with a cotton ball sat Left this in place for 20 minutes \n",
      "\n",
      "\n",
      "Input sentence: with Betadine again and injected th he tolerated very well.\n",
      "Word Decoded sentence: with Betadine again and injected th he tolerated very well \n",
      "\n",
      "\n",
      "Input sentence: The wound sutures. Antibiotic eintment and g\n",
      "Word Decoded sentence: The wound sutures Antibiotic eintment and g \n",
      "\n",
      "\n",
      "Input sentence: Patient tolerated very well. Pat\n",
      "Word Decoded sentence: Patient tolerated very well Pat \n",
      "\n",
      "\n",
      "Input sentence: IMPRESSION: Lacration te left grs\n",
      "Word Decoded sentence: IMPRESSION Laceration te left grs \n",
      "\n",
      "\n",
      "Input sentence: PLAN: Patent is to do dressing ch advised as far as checking tha waurn it with soap and water.\n",
      "Word Decoded sentence: PLAN Patent is to do dressing ch advised as far as checking tha warn it with soap and water \n",
      "\n",
      "\n",
      "Input sentence: Sutures oy have any problems prior te that tim ona teaspoon three times a day rer Ibuprofan far pain, discomfort. Cg\n",
      "Word Decoded sentence: Sutures oy have any problems prior te that tim ona teaspoon three times a day rer Ibuprofan far pain discomfort Cg \n",
      "\n",
      "\n",
      "Input sentence: hite male who accidently dropped a bike onto ar end hitting the left great toe,causing a guard to the end of the bike, which caused anus shot is more that three years ago and time.\n",
      "Word Decoded sentence: Hite male who accidently dropped a bike onto ar end hitting the left great toe,causing a guard to the end of the bike which caused anus shot is more that three years ago and time \n",
      "\n",
      "\n",
      "Input sentence: nlse ef 105 and regular, resprations 286,% on room air.\n",
      "Word Decoded sentence: nlse ef 105 and regular respirations 286,% on room air \n",
      "\n",
      "\n",
      "Input sentence: Patient rates hia pain at — there i15 noted a 3-om laceration across a lateral aspect of tha toe.\n",
      "Word Decoded sentence: Patient rates hia pain at — there i15 noted a 3-om laceration across a lateral aspect of tha toe \n",
      "\n",
      "\n",
      "Input sentence: There is no e toa ir cleansed with Betadine.\n",
      "Word Decoded sentence: There is no e toa ir cleansed with Betadine. \n",
      "\n",
      "\n",
      "Input sentence: It is then urated with 5 cu of 2% Hylocaine plain.\n",
      "Word Decoded sentence: It is then urated with 5 cu of 2% Hylocaine plain \n",
      "\n",
      "\n",
      "Input sentence: We then cleansed ae toa with 3 cc of 2% Xylacaina plain\n",
      "Word Decoded sentence: We then cleansed ae toa with 3 cc of 2% Xylacaina plain \n",
      "\n",
      "\n",
      "Input sentence: which was then clesed with five 5-0 Prolene ressure dressing was then applied to the tos\n",
      "Word Decoded sentence: which was then closed with five 5-0 Prolene ressure dressing was then applied to the tos \n",
      "\n",
      "\n",
      "Input sentence: paient is given DPT 0.5 ee intramucular (IM).at toe.\n",
      "Word Decoded sentence: patient is given DPT 0.5 ee intramuscular (IM).at toe \n",
      "\n",
      "\n",
      "Input sentence: Kefylex 250 mg per 5 ml, the next seven days.\n",
      "Word Decoded sentence: Keflex 250 mg per 5 ml the next seven days \n",
      "\n",
      "\n",
      "Input sentence: He may use Tylenol or 11 if any problems.\n",
      "Word Decoded sentence: He may use Tylenol or 11 if any problems \n",
      "\n",
      "\n",
      "Input sentence: Unum Life Insurance Company of America 2211\n",
      "Word Decoded sentence: Unum Life Insurance Company of America 2211 \n",
      "\n",
      "\n",
      "Input sentence: Congress Street Portland, Maine 04122\n",
      "Word Decoded sentence: Congress Street Portland Maine 04122 \n",
      "\n",
      "\n",
      "Input sentence: APPLICATION FOR GROUP CRITICAL LLNESS INSURANCE\n",
      "Word Decoded sentence: APPLICATION FOR GROUP CRITICAL ILLNESS INSURANCE \n",
      "\n",
      "\n",
      "Input sentence: I Evidence of Insurability\n",
      "Word Decoded sentence: I Evidence of Insurability \n",
      "\n",
      "\n",
      "Input sentence: \n",
      "Word Decoded sentence: a \n",
      "\n",
      "\n",
      "Input sentence: Application Type: @ New Enrollee Change to\n",
      "Word Decoded sentence: Application Type a New Enrollee Change to \n",
      "\n",
      "\n",
      "Input sentence: Existing Coverage Reinstatement Internal\n",
      "Word Decoded sentence: Existing Coverage Reinstatement Internal \n",
      "\n",
      "\n",
      "Input sentence: Replacement Late Applicant Rehire SECTION 1:\n",
      "Word Decoded sentence: Replacement Late Applicant Rehire SECTION 1: \n",
      "\n",
      "\n",
      "Input sentence: Employee(Applicant) Information Always\n",
      "Word Decoded sentence: Employee(Applicant) Information Always \n",
      "\n",
      "\n",
      "Input sentence: Complete Employee Name(First, Middle, Last)\n",
      "Word Decoded sentence: Complete Employee Name(First, Middle Last \n",
      "\n",
      "\n",
      "Input sentence: Social Security Number Nikolas J Jones\n",
      "Word Decoded sentence: Social Security Number Nikolas J Jones \n",
      "\n",
      "\n",
      "Input sentence: 123 - 456 - 7890 Home Address(Street/ PO Box)\n",
      "Word Decoded sentence: 123 - 456 - 7890 Home Address(Street/ PO Box \n",
      "\n",
      "\n",
      "Input sentence: Gender 1634 Stewert St F M City Date of Birth\n",
      "Word Decoded sentence: Gender 1634 Stewer St F M City Date of Birth \n",
      "\n",
      "\n",
      "Input sentence: (mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip\n",
      "Word Decoded sentence: mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip \n",
      "\n",
      "\n",
      "Input sentence: Code Home Phone # Washington 98101 854-555-1212\n",
      "Word Decoded sentence: Code Home Phone # Washington 98101 854-555-1212 \n",
      "\n",
      "\n",
      "Input sentence: Are you Actively at Work? Employee ID / Payroll #\n",
      "Word Decoded sentence: Are you Actively at Work Employee ID / Payroll # \n",
      "\n",
      "\n",
      "Input sentence: Yes No55624 a.Are you a U.S.Citizen or\n",
      "Word Decoded sentence: Yes No55624 aware you a U.S.Citizen or \n",
      "\n",
      "\n",
      "Input sentence: Canadian Citizen working in the U.S.? b.Are you\n",
      "Word Decoded sentence: Canadian Citizen working in the U.S.? bare you \n",
      "\n",
      "\n",
      "Input sentence: legally authorized to work in Yes No(If No\n",
      "Word Decoded sentence: legally authorized to work in Yes Notify No \n",
      "\n",
      "\n",
      "Input sentence: reply to part b) the U.S.? Yes No Employer\n",
      "Word Decoded sentence: reply to part by the U.S.? Yes No Employer \n",
      "\n",
      "\n",
      "Input sentence: Name Group Number Date of Hire(mm/ dd / yyyy)\n",
      "Word Decoded sentence: Name Group Number Date of Hire(mm/ dd / yyyy) \n",
      "\n",
      "\n",
      "Input sentence: Facebook 11 - 555566 11 / 30 / 2016 Occupation\n",
      "Word Decoded sentence: Facebook 11 - 555566 11 / 30 / 2016 Occupation \n",
      "\n",
      "\n",
      "Input sentence: Eligibility Class Software Engineer 7 Scheduled\n",
      "Word Decoded sentence: Eligibility Class Software Engineer 7 Scheduled \n",
      "\n",
      "\n",
      "Input sentence: Number of Work Hours per Week Work Phone # 35\n",
      "Word Decoded sentence: Number of Work Hours per Week Work Phone # 35 \n",
      "\n",
      "\n",
      "Input sentence: 854-555-6622 SECTION 2: Spouse Information\n",
      "Word Decoded sentence: 854-555-6622 SECTION 2: Spouse Information \n",
      "\n",
      "\n",
      "Input sentence: Complete Only if applying for Spouse coverage Name\n",
      "Word Decoded sentence: Complete Only if applying for Spouse coverage Name \n",
      "\n",
      "\n",
      "Input sentence: (First, Middle, Last) Social Security Number\n",
      "Word Decoded sentence: First Middle Last Social Security Number \n",
      "\n",
      "\n",
      "Input sentence: Gender Date of Birth(mm / dd / yyyy) Does the\n",
      "Word Decoded sentence: Gender Date of Birth(mm / dd / yyyy) Does the \n",
      "\n",
      "\n",
      "Input sentence: 1019 - 07 - AZ 1\n",
      "Word Decoded sentence: 1019 - 07 - AZ 1 \n",
      "\n",
      "\n",
      "Input sentence: if claint is for a child, please state your relationship 10 the child\n",
      "Word Decoded sentence: if clint is for a child please state your relationship 10 the child \n",
      "\n",
      "\n",
      "Input sentence: date of accident 3d _ time of accident ram. 0 p.m.\n",
      "Word Decoded sentence: date of accident 3d a time of accident rams 0 pm \n",
      "\n",
      "\n",
      "Input sentence: have you slopped working? (of yes [1 no if yes, what was the last day that you worked? (mm/ddryy)_| —3 | —{% cnslamegs bil =\n",
      "Word Decoded sentence: have you slopped working of yes [1 no if yes what was the last day that you worked (mm/ddryy)_| —3 a —{% cnslamegs Bil a \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = ['SUBJECTIVE: This is a S-year-old +@W his left great toe with the handleh lacration.',\n",
    "               'Thera was no handlebarthe lacration.',\n",
    "               'Patiet last tet is needing this for school at this',\n",
    "               'OBJECTIVE : The temp is 99.8, the f tha blood pressure 99/64, O2 sat 94 8/10 at this time.',\n",
    "               'Left great toe the dorsl surface, extending ta th active hemorrhage at this time.',\n",
    "               'Th anaathetized with a cotton ball sat Left this in place for 20 minutes.',\n",
    "               'with Betadine again and injected th he tolerated very well.',\n",
    "               'The wound sutures. Antibiotic eintment and g',\n",
    "               'Patient tolerated very well. Pat',\n",
    "               'IMPRESSION: Lacration te left grs',\n",
    "               'PLAN: Patent is to do dressing ch advised as far as checking tha waurn it with soap and water.',\n",
    "               'Sutures oy have any problems prior te that tim ona teaspoon three times a day rer Ibuprofan far pain, discomfort. Cg',\n",
    "               'hite male who accidently dropped a bike onto ar end hitting the left great toe,'\n",
    "               'causing a guard to the end of the bike, which caused anus shot is more that three years ago and time.',\n",
    "               'nlse ef 105 and regular, resprations 286,% on room air.',\n",
    "               'Patient rates hia pain at — there i15 noted a 3-om laceration across a lateral aspect of tha toe.',\n",
    "               'There is no e toa ir cleansed with Betadine.',\n",
    "               'It is then urated with 5 cu of 2% Hylocaine plain.',\n",
    "               'We then cleansed ae toa with 3 cc of 2% Xylacaina plain',\n",
    "               'which was then clesed with five 5-0 Prolene ressure dressing was then applied to the tos',\n",
    "               'paient is given DPT 0.5 ee intramucular (IM).at toe.',\n",
    "               'Kefylex 250 mg per 5 ml, the next seven days.',\n",
    "               'He may use Tylenol or 11 if any problems.',\n",
    "               'Unum Life Insurance Company of America 2211',               \n",
    "               'Congress Street Portland, Maine 04122',\n",
    "               'APPLICATION FOR GROUP CRITICAL LLNESS INSURANCE',\n",
    "               'I Evidence of Insurability',\n",
    "               '',\n",
    "               'Application Type: @ New Enrollee Change to',\n",
    "               'Existing Coverage  Reinstatement  Internal',\n",
    "               'Replacement  Late Applicant  Rehire SECTION 1:',\n",
    "               'Employee(Applicant) Information  Always',\n",
    "               'Complete Employee Name(First, Middle, Last)',\n",
    "               'Social Security Number Nikolas J Jones',\n",
    "               '123 - 456 - 7890 Home Address(Street/ PO Box)',\n",
    "               'Gender 1634 Stewert St  F  M City Date of Birth',\n",
    "               '(mm / dd / yyyy) Seattle 06 / 15 / 1991 State Zip',\n",
    "               'Code Home Phone # Washington 98101 854-555-1212',\n",
    "               'Are you Actively at Work? Employee ID / Payroll #',\n",
    "               ' Yes  No55624 a.Are you a U.S.Citizen or',\n",
    "               'Canadian Citizen working in the U.S.? b.Are you',\n",
    "               'legally authorized to work in  Yes  No(If No',\n",
    "               'reply to part b) the U.S.?  Yes  No Employer',\n",
    "               'Name Group Number Date of Hire(mm/ dd / yyyy)',\n",
    "               'Facebook 11 - 555566 11 / 30 / 2016 Occupation',\n",
    "               'Eligibility Class Software Engineer 7 Scheduled',\n",
    "               'Number of Work Hours per Week Work Phone # 35',\n",
    "               '854-555-6622 SECTION 2: Spouse Information ',\n",
    "               'Complete Only if applying for Spouse coverage Name',\n",
    "               '(First, Middle, Last) Social Security Number',\n",
    "               'Gender Date of Birth(mm / dd / yyyy) Does the',\n",
    "               '1019 - 07 - AZ 1',\n",
    "              'if claint is for a child, please state your relationship 10 the child',\n",
    "              'date of accident 3d _ time of accident ram. 0 p.m.',\n",
    "              'have you slopped working? (of yes [1 no if yes, what was the last day that you worked? (mm/ddryy)_| —3 | —{% cnslamegs bil =']\n",
    "               \n",
    "for input_text in input_texts:\n",
    "    len_range = max_sent_lengths[-1] # Take the longest range\n",
    "    for length in max_sent_lengths:\n",
    "        if(len(input_text) < length):\n",
    "            len_range = length\n",
    "            break\n",
    "    #print(len_range)\n",
    "    pre_corrected_sentence = word_spell_correct(input_text)\n",
    "    input_text = clean_up_sentence(input_text, vocab_to_int[len_range])\n",
    "    encoder_input_data = vectorize_data(input_texts=[input_text], max_encoder_seq_length=max_encoder_seq_length[len_range], num_encoder_tokens=num_encoder_tokens[len_range], vocab_to_int=vocab_to_int[len_range])\n",
    "\n",
    "\n",
    "\n",
    "    target_text = gt_texts[i]\n",
    "\n",
    "    input_seq = encoder_input_data\n",
    "    #print(input_seq.shape)\n",
    "    #print(max_decoder_seq_length[len_range])\n",
    "    #print(max_decoder_seq_length)\n",
    "\n",
    "    decoded_sentence,_  = decode_sequence(input_seq, encoder_model[len_range], decoder_model[len_range], num_decoder_tokens[len_range],  max_decoder_seq_length[len_range], vocab_to_int[len_range], int_to_vocab[len_range])\n",
    "    corrected_sentence = word_spell_correct(input_text)\n",
    "    #print('-Lenght = ', len_range)\n",
    "    print('Input sentence:', input_text)\n",
    "    #print('Spell Decoded sentence:', pre_corrected_sentence) \n",
    "    #print('Char Decoded sentence:', decoded_sentence)   \n",
    "    print('Word Decoded sentence:', corrected_sentence) \n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(gt_texts, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_word_correction = calculate_WER(gt_texts, corrected_sentences)\n",
    "print('WER_spell_word_correction |TEST= ', WER_spell_word_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(gt_texts, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
