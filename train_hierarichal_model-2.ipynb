{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import spell\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index].strip() + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1].strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_hier_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data = np.zeros(\n",
    "    (len(input_texts), max_words_seq_length, max_chars_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    decoder_word_input_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length),\n",
    "        dtype='float32')\n",
    "    \n",
    "    decoder_word_target_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length, num_word_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[i, j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)# word_tokenize removes the \\t and \\n, we need them to start and end a sequence\n",
    "        words_lst.insert(0, '\\t')\n",
    "        words_lst.append('\\n')        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            processed_word = process_word(word)\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[i, j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[i, j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "    return encoder_char_input_data, decoder_word_input_data, decoder_word_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_words_seq_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "       \n",
    "        \n",
    "        #orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(word_tokenize(decoded_sentence)) > max_words_seq_len):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        '''\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, max_words_seq_len))\n",
    "        if i < max_words_seq_len:\n",
    "            target_seq[0, i] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "        #if i > 48:\n",
    "        #    i = 0\n",
    "        \n",
    "\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(encoder_char_model, max_words_seq_len, max_char_seq_len, num_word_tokens, num_char_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    \n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "\n",
    "    \n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(max_words_seq_len, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_words, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\\nhier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_, encoder_char_model, _ = build_char_model(num_encoder_tokens=28, latent_dim=256)\n",
    "hier_model = build_hier_model(encoder_char_model=encoder_char_model, max_words_seq_len=40, max_char_seq_len=20, num_word_tokens=1000, num_char_tokens=28, latent_dim=256)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = Input(shape=(50,))\\nx = Reshape((1,50))(x)\\nprint(x)\\nl = []\\nl.append(x)\\nl.append(x)\\nprint(l)\\ny = Concatenate(axis=-2)(l)\\nprint(y)\\nz = Reshape((-1,2,50))(y)\\nprint(z)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "x = Input(shape=(50,))\n",
    "x = Reshape((1,50))(x)\n",
    "print(x)\n",
    "l = []\n",
    "l.append(x)\n",
    "l.append(x)\n",
    "print(l)\n",
    "y = Concatenate(axis=-2)(l)\n",
    "print(y)\n",
    "z = Reshape((-1,2,50))(y)\n",
    "print(z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        #processed_word = word.lower()\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []\n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in (input_texts_OCR+target_texts_OCR):\n",
    "    words = word_tokenize(text)\n",
    "    #words_per_sents_lengths.append(len(words))\n",
    "    for word in words:\n",
    "        chars_per_words_lengths.append(len(word))\n",
    "\n",
    "# Words in sent should be on target only        \n",
    "for text in target_texts_OCR:\n",
    "    words = word_tokenize(text)\n",
    "    words_per_sents_lengths.append(len(words))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c4798dbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8c4799fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4000\n",
      "Number of unique input tokens: 91\n",
      "Number of unique output tokens: 91\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '#': 67,\n",
       " '$': 80,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 48,\n",
       " '/': 29,\n",
       " '0': 54,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 57,\n",
       " '4': 56,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 70,\n",
       " '8': 61,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '=': 89,\n",
       " '?': 60,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 46,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 52,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 50,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 45,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 49,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 51,\n",
       " 'X': 79,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '^': 86,\n",
       " '_': 90,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 59,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 58,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 62,\n",
       " 'y': 10,\n",
       " 'z': 63,\n",
       " '|': 82,\n",
       " '’': 66,\n",
       " '•': 84,\n",
       " '●': 87,\n",
       " 'ﬁ': 88}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'R',\n",
       " 46: 'E',\n",
       " 47: 'Y',\n",
       " 48: '.',\n",
       " 49: 'U',\n",
       " 50: 'K',\n",
       " 51: 'W',\n",
       " 52: 'H',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: '4',\n",
       " 57: '3',\n",
       " 58: 'q',\n",
       " 59: 'k',\n",
       " 60: '?',\n",
       " 61: '8',\n",
       " 62: 'x',\n",
       " 63: 'z',\n",
       " 64: '(',\n",
       " 65: ')',\n",
       " 66: '’',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: 'X',\n",
       " 80: '$',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '•',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '●',\n",
       " 88: 'ﬁ',\n",
       " 89: '=',\n",
       " 90: '_'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize char data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_char_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 91)     8281        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  712704      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 91)     8281        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1236992     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 91)     93275       concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,059,533\n",
      "Trainable params: 2,042,971\n",
      "Non-trainable params: 16,562\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_char_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 19s 6ms/step - loss: 3.4438 - categorical_accuracy: 0.1277 - val_loss: 2.9185 - val_categorical_accuracy: 0.1991\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.19906, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 2.2993 - categorical_accuracy: 0.3271 - val_loss: 2.2006 - val_categorical_accuracy: 0.3697\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.19906 to 0.36974, saving model to best_model_char-50.hdf5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 1.4210 - categorical_accuracy: 0.5819 - val_loss: 1.5870 - val_categorical_accuracy: 0.5295\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.36974 to 0.52948, saving model to best_model_char-50.hdf5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.7419 - categorical_accuracy: 0.7688 - val_loss: 1.0764 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.52948 to 0.66674, saving model to best_model_char-50.hdf5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.3660 - categorical_accuracy: 0.8658 - val_loss: 0.6269 - val_categorical_accuracy: 0.7875\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.66674 to 0.78749, saving model to best_model_char-50.hdf5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.1877 - categorical_accuracy: 0.9087 - val_loss: 0.4190 - val_categorical_accuracy: 0.8439\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.78749 to 0.84387, saving model to best_model_char-50.hdf5\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.1126 - categorical_accuracy: 0.9282 - val_loss: 0.3337 - val_categorical_accuracy: 0.8639\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.84387 to 0.86387, saving model to best_model_char-50.hdf5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0730 - categorical_accuracy: 0.9390 - val_loss: 0.3306 - val_categorical_accuracy: 0.8629\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.86387\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0606 - categorical_accuracy: 0.9415 - val_loss: 0.2845 - val_categorical_accuracy: 0.8768\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.86387 to 0.87681, saving model to best_model_char-50.hdf5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 18s 6ms/step - loss: 0.0392 - categorical_accuracy: 0.9476 - val_loss: 0.2532 - val_categorical_accuracy: 0.8871\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.87681 to 0.88711, saving model to best_model_char-50.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8b46fdfd30>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_char-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_4:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'input_5:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model_file = 'encoder_char_model-{}.hdf5'\n",
    "decoder_char_model_file = 'decoder_char_model-{}.hdf5'\n",
    "encoder_model.save('encoder_char_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_char_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Hierarichal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_seq_len=15\n",
    "max_chars_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len, max_chars_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_chars_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ' ': 1,\n",
       " '\\t': 2,\n",
       " '\\n': 3,\n",
       " 'Claim': 4,\n",
       " 'Type': 5,\n",
       " 'VB': 6,\n",
       " 'Accident': 7,\n",
       " 'Accidental': 8,\n",
       " 'Injury': 9,\n",
       " 'Information': 10,\n",
       " 'First': 11,\n",
       " 'Name': 12,\n",
       " 'Middle': 13,\n",
       " 'Last': 14,\n",
       " 'Social': 15,\n",
       " 'Security': 16,\n",
       " 'Number': 17,\n",
       " 'Birth': 18,\n",
       " 'Date': 19,\n",
       " 'Gender': 20,\n",
       " 'Language': 21,\n",
       " 'Preference': 22,\n",
       " 'Address': 23,\n",
       " 'Line': 24,\n",
       " 'City': 25,\n",
       " 'Postal': 26,\n",
       " 'Code': 27,\n",
       " 'Country': 28,\n",
       " 'Best': 29,\n",
       " 'Phone': 30,\n",
       " 'to': 31,\n",
       " 'be': 32,\n",
       " 'Reached': 33,\n",
       " 'During': 34,\n",
       " 'the': 35,\n",
       " 'Day': 36,\n",
       " 'Email': 37,\n",
       " 'Page': 38,\n",
       " 'of': 39,\n",
       " 'RADIOLOGY': 40,\n",
       " 'REPORT': 41,\n",
       " 'Patient': 42,\n",
       " 'MRN': 43,\n",
       " 'Accession': 44,\n",
       " 'No': 45,\n",
       " 'Ref': 46,\n",
       " 'Physician': 47,\n",
       " 'UNKNOWN': 48,\n",
       " 'Study': 49,\n",
       " 'Hospital': 50,\n",
       " 'DOB': 51,\n",
       " 'Technique': 52,\n",
       " 'views': 53,\n",
       " 'left': 54,\n",
       " 'wrist': 55,\n",
       " 'Cormarison': 56,\n",
       " 'None': 57,\n",
       " 'availabie': 58,\n",
       " 'Comparison': 59,\n",
       " 'available': 60,\n",
       " 'FINDINGS': 61,\n",
       " 'IMPRESSION': 62,\n",
       " 'acute': 63,\n",
       " 'osseous': 64,\n",
       " 'abnormality': 65,\n",
       " 'identified': 66,\n",
       " 'Daytime': 67,\n",
       " 'Event': 68,\n",
       " 'Stopped': 69,\n",
       " 'Working': 70,\n",
       " 'Yes': 71,\n",
       " 'Physically': 72,\n",
       " 'at': 73,\n",
       " 'Work': 74,\n",
       " 'Hours': 75,\n",
       " 'Worked': 76,\n",
       " 'on': 77,\n",
       " 'Scheduled': 78,\n",
       " 'Missed': 79,\n",
       " 'Returned': 80,\n",
       " 'Related': 81,\n",
       " 'Time': 82,\n",
       " 'Diagnosis': 83,\n",
       " 'Arthiscopic': 84,\n",
       " 'surgery': 85,\n",
       " 'Surgery': 86,\n",
       " 'Is': 87,\n",
       " 'Required': 88,\n",
       " 'Indicator': 89,\n",
       " 'Outpatient': 90,\n",
       " 'Medical': 91,\n",
       " 'Provider': 92,\n",
       " 'Roles': 93,\n",
       " 'Treating': 94,\n",
       " 'Patrick': 95,\n",
       " 'Emerson': 96,\n",
       " 'Business': 97,\n",
       " 'Telephone': 98,\n",
       " 'Fax': 99,\n",
       " 'Visit': 100,\n",
       " 'Next': 101,\n",
       " 'Hospitalization': 102,\n",
       " 'Discharge': 103,\n",
       " 'Procedure': 104,\n",
       " 'Left': 105,\n",
       " 'arthiscopic': 106,\n",
       " 'Employment': 107,\n",
       " 'Employer': 108,\n",
       " 'Policy': 109,\n",
       " 'Electronic': 110,\n",
       " 'Submission': 111,\n",
       " 'Identifier': 112,\n",
       " 'Electronically': 113,\n",
       " 'Signed': 114,\n",
       " 'Fraud': 115,\n",
       " 'Statements': 116,\n",
       " 'Reviewed': 117,\n",
       " 'and': 118,\n",
       " 'unum': 119,\n",
       " 'The': 120,\n",
       " 'Benefits': 121,\n",
       " 'Center': 122,\n",
       " 'Not': 123,\n",
       " 'for': 124,\n",
       " 'FMLA': 125,\n",
       " 'Requests': 126,\n",
       " 'Insured': 127,\n",
       " '’': 128,\n",
       " 's': 129,\n",
       " 'Signature': 130,\n",
       " 'Printed': 131,\n",
       " 'Unum': 132,\n",
       " 'Confirmation': 133,\n",
       " 'Coverage': 134,\n",
       " 'Group': 135,\n",
       " 'Customer': 136,\n",
       " 'EE': 137,\n",
       " 'Effective': 138,\n",
       " 'Employee': 139,\n",
       " 'Acc': 140,\n",
       " 'January': 141,\n",
       " 'Wellness': 142,\n",
       " 'Benefit': 143,\n",
       " 'Total': 144,\n",
       " 'Monthly': 145,\n",
       " 'Premium': 146,\n",
       " 'Montly': 147,\n",
       " 'Payroll': 148,\n",
       " 'Deduction': 149,\n",
       " 'Account': 150,\n",
       " 'F': 151,\n",
       " 'Exam': 152,\n",
       " 'Referring': 153,\n",
       " 'Phys': 154,\n",
       " 'STEPHEN': 155,\n",
       " 'GELOVICH': 156,\n",
       " 'Tax': 157,\n",
       " 'MRI': 158,\n",
       " 'OF': 159,\n",
       " 'THE': 160,\n",
       " 'LEFT': 161,\n",
       " 'WRIST': 162,\n",
       " 'WITHOUT': 163,\n",
       " 'CONTRAST': 164,\n",
       " 'COMPARISON': 165,\n",
       " 'These': 166,\n",
       " 'results': 167,\n",
       " 'were': 168,\n",
       " 'faxed': 169,\n",
       " 'Gelovich': 170,\n",
       " 'signed': 171,\n",
       " 'by': 172,\n",
       " 'Stephen': 173,\n",
       " 'Bravo': 174,\n",
       " 'Dependent': 175,\n",
       " 'Detail': 176,\n",
       " 'Zachary': 177,\n",
       " 'Jager': 178,\n",
       " 'Billed': 179,\n",
       " 'Amounts': 180,\n",
       " 'Contract': 181,\n",
       " 'Adjustment': 182,\n",
       " 'Allowed': 183,\n",
       " 'Amount': 184,\n",
       " 'Covered': 185,\n",
       " 'Reason': 186,\n",
       " 'Deductible': 187,\n",
       " 'Other': 188,\n",
       " 'Carrier': 189,\n",
       " 'Paid': 190,\n",
       " 'Responsibility': 191,\n",
       " 'Totals': 192,\n",
       " 'For': 193,\n",
       " 'Appeals': 194,\n",
       " 'Rights': 195,\n",
       " 'Important': 196,\n",
       " 'about': 197,\n",
       " 'Your': 198,\n",
       " 'Appeal': 199,\n",
       " 'All': 200,\n",
       " 'Languages': 201,\n",
       " 'Contact': 202,\n",
       " 'Did': 203,\n",
       " 'You': 204,\n",
       " 'Know': 205,\n",
       " 'Specialty': 206,\n",
       " 'Orthopedic': 207,\n",
       " 'Surgeon': 208,\n",
       " 'Unknown': 209,\n",
       " 'Kari': 210,\n",
       " 'Lund': 211,\n",
       " 'Orthopedist': 212,\n",
       " 'Dan': 213,\n",
       " 'Palmer': 214,\n",
       " 'On': 215,\n",
       " '&': 216,\n",
       " 'May': 217,\n",
       " 'Spouse': 218,\n",
       " 'Child': 219,\n",
       " 'BLACK': 220,\n",
       " 'HILLS': 221,\n",
       " 'ORTHOPEDIC': 222,\n",
       " 'CENTER': 223,\n",
       " 'PC': 224,\n",
       " 'LAST': 225,\n",
       " 'PMT': 226,\n",
       " 'AMOUNT': 227,\n",
       " 'DUE': 228,\n",
       " 'DATE': 229,\n",
       " 'PAGE': 230,\n",
       " 'STATEMENT': 231,\n",
       " 'Ins': 232,\n",
       " 'Description': 233,\n",
       " 'E': 234,\n",
       " 'm': 235,\n",
       " 'New': 236,\n",
       " 'Moderat': 237,\n",
       " 'S': 238,\n",
       " 'Clo': 239,\n",
       " 'Tx': 240,\n",
       " 'Phalangealfx': 241,\n",
       " 'Finger': 242,\n",
       " 'Splint': 243,\n",
       " 'Offic': 244,\n",
       " 'Cons': 245,\n",
       " 'Moderate': 246,\n",
       " 'Sever': 247,\n",
       " 'Rad': 248,\n",
       " 'Mini': 249,\n",
       " 'Views': 250,\n",
       " 'Applic': 251,\n",
       " 'Hand': 252,\n",
       " 'Lower': 253,\n",
       " 'Forearm': 254,\n",
       " 'Fiberglass': 255,\n",
       " 'gauntlet': 256,\n",
       " 'Cast': 257,\n",
       " 'Yrs': 258,\n",
       " 'Charge': 259,\n",
       " 'Pmt': 260,\n",
       " 'Pat': 261,\n",
       " 'Adjust': 262,\n",
       " 'Current': 263,\n",
       " 'Days': 264,\n",
       " 'Balance': 265,\n",
       " 'Pending': 266,\n",
       " 'Now': 267,\n",
       " 'Due': 268,\n",
       " 'Message': 269,\n",
       " 'Make': 270,\n",
       " 'Checks': 271,\n",
       " 'Payable': 272,\n",
       " 'To': 273,\n",
       " 'Statement': 274,\n",
       " 'Billing': 275,\n",
       " 'Questions': 276,\n",
       " 'Choice': 277,\n",
       " 'Health': 278,\n",
       " 'Administrators': 279,\n",
       " 'Forwarding': 280,\n",
       " 'Service': 281,\n",
       " 'Requested': 282,\n",
       " 'REGIONAL': 283,\n",
       " 'HEALTH': 284,\n",
       " 'INC': 285,\n",
       " 'Participant': 286,\n",
       " 'ID': 287,\n",
       " 'Original': 288,\n",
       " 'Print': 289,\n",
       " 'Website': 290,\n",
       " 'DEA': 291,\n",
       " 'Individual': 292,\n",
       " 'Summary': 293,\n",
       " 'By': 294,\n",
       " 'Plan': 295,\n",
       " 'Status': 296,\n",
       " 'Period': 297,\n",
       " 'DEDUCTIBLE': 298,\n",
       " 'OUT': 299,\n",
       " 'POCKET': 300,\n",
       " 'Regional': 301,\n",
       " 'Inc': 302,\n",
       " 'EXPLANATION': 303,\n",
       " 'BENEFITS': 304,\n",
       " 'Retain': 305,\n",
       " 'Purposes': 306,\n",
       " 'Sign': 307,\n",
       " 'up': 308,\n",
       " 'paperless': 309,\n",
       " 'Family': 310,\n",
       " 'Out': 311,\n",
       " 'Network': 312,\n",
       " 'Karl': 313,\n",
       " 'Services': 314,\n",
       " 'exam': 315,\n",
       " 'hand': 316,\n",
       " 'Modifiers': 317,\n",
       " 'TC': 318,\n",
       " 'RT': 319,\n",
       " 'This': 320,\n",
       " 'Qualified': 321,\n",
       " 'sign': 322,\n",
       " 'language': 323,\n",
       " 'interpreters': 324,\n",
       " 'written': 325,\n",
       " 'in': 326,\n",
       " 'other': 327,\n",
       " 'languages': 328,\n",
       " 'Jacquelin': 329,\n",
       " 'Brainard': 330,\n",
       " 'Compliance': 331,\n",
       " 'Officer': 332,\n",
       " 'or': 333,\n",
       " 'mail': 334,\n",
       " 'phone': 335,\n",
       " 'Department': 336,\n",
       " 'Human': 337,\n",
       " 'Complaint': 338,\n",
       " 'forms': 339,\n",
       " 'are': 340,\n",
       " 'DAKOTA': 341,\n",
       " 'EZ': 342,\n",
       " 'Ways': 343,\n",
       " 'Pay': 344,\n",
       " 'Automated': 345,\n",
       " 'Attendant': 346,\n",
       " 'hours': 347,\n",
       " 'a': 348,\n",
       " 'day': 349,\n",
       " 'Payments': 350,\n",
       " 'Please': 351,\n",
       " 'Call': 352,\n",
       " 'Upon': 353,\n",
       " 'Receipt': 354,\n",
       " 'Improved': 355,\n",
       " 'Online': 356,\n",
       " 'Experience': 357,\n",
       " '|': 358,\n",
       " 'Update': 359,\n",
       " 'Info': 360,\n",
       " 'About': 361,\n",
       " 'See': 362,\n",
       " 'Details': 363,\n",
       " 'Back': 364,\n",
       " 'ACCOUNT': 365,\n",
       " 'NO': 366,\n",
       " 'SHOW': 367,\n",
       " 'PAID': 368,\n",
       " 'HERE': 369,\n",
       " 'MAKE': 370,\n",
       " 'CHECKS': 371,\n",
       " 'TO': 372,\n",
       " 'PROC': 373,\n",
       " 'CODE': 374,\n",
       " 'UNITS': 375,\n",
       " 'DETAILS': 376,\n",
       " 'SERVICES': 377,\n",
       " 'CHARGES': 378,\n",
       " 'INSUR': 379,\n",
       " 'PENDING': 380,\n",
       " 'PATIENT': 381,\n",
       " 'BALANCE': 382,\n",
       " 'EXAM': 383,\n",
       " 'HAND': 384,\n",
       " 'THORAC': 385,\n",
       " 'SPINE': 386,\n",
       " 'COMMERCIAL': 387,\n",
       " 'NON': 388,\n",
       " 'ALLOWED': 389,\n",
       " 'CT': 390,\n",
       " 'ABD': 391,\n",
       " 'PELV': 392,\n",
       " 'PAYMENT': 393,\n",
       " 'CHEST': 394,\n",
       " 'VIEWS': 395,\n",
       " 'HARGES': 396,\n",
       " 'Over': 397,\n",
       " 'DIGIT': 398,\n",
       " 'Of': 399,\n",
       " 'Today': 400,\n",
       " \"'s\": 401,\n",
       " 'Ethnicity': 402,\n",
       " 'Hispanic': 403,\n",
       " 'Latino': 404,\n",
       " 'Preferred': 405,\n",
       " 'English': 406,\n",
       " 'visit': 407,\n",
       " 'with': 408,\n",
       " 'Suzanne': 409,\n",
       " 'Newsom': 410,\n",
       " 'CNP': 411,\n",
       " '•': 412,\n",
       " 'Lethargy': 413,\n",
       " 'cough': 414,\n",
       " 'Vitals': 415,\n",
       " 'lbs': 416,\n",
       " 'kg': 417,\n",
       " 'Wt': 418,\n",
       " 'Temp': 419,\n",
       " 'HR': 420,\n",
       " 'Oxygen': 421,\n",
       " 'sat': 422,\n",
       " 'Allergies': 423,\n",
       " 'Amoxicillin': 424,\n",
       " 'rash': 425,\n",
       " 'possible': 426,\n",
       " 'hives': 427,\n",
       " 'Active': 428,\n",
       " 'Diagnoses': 429,\n",
       " 'Include': 430,\n",
       " 'Acute': 431,\n",
       " 'frontal': 432,\n",
       " 'sinusitis': 433,\n",
       " 'unspecified': 434,\n",
       " 'Dizziness': 435,\n",
       " 'giddiness': 436,\n",
       " 'Medication': 437,\n",
       " 'List': 438,\n",
       " 'medications': 439,\n",
       " 'you': 440,\n",
       " 'Taking': 441,\n",
       " 'Zyrtec': 442,\n",
       " 'Childrens': 443,\n",
       " 'Allergy': 444,\n",
       " 'Notes': 445,\n",
       " 'Tests': 446,\n",
       " 'Labs': 447,\n",
       " 'Illumigene': 448,\n",
       " 'MYCO': 449,\n",
       " 'http': 450,\n",
       " 'BASIC': 451,\n",
       " 'METABOLIC': 452,\n",
       " 'SODIUM': 453,\n",
       " 'Range': 454,\n",
       " 'POTASSIUM': 455,\n",
       " 'CHLORIDE': 456,\n",
       " 'GLUCOSE': 457,\n",
       " 'BUN': 458,\n",
       " 'CREATININE': 459,\n",
       " 'CALCIUM': 460,\n",
       " 'CREA': 461,\n",
       " 'RATIO': 462,\n",
       " 'Ratio': 463,\n",
       " 'ANION': 464,\n",
       " 'GAP': 465,\n",
       " 'Calc': 466,\n",
       " 'CBC': 467,\n",
       " 'DIFF': 468,\n",
       " 'WBC': 469,\n",
       " 'RBC': 470,\n",
       " 'HGB': 471,\n",
       " 'HCT': 472,\n",
       " 'MCV': 473,\n",
       " 'fL': 474,\n",
       " 'MCH': 475,\n",
       " 'pg': 476,\n",
       " 'MCHC': 477,\n",
       " 'MPV': 478,\n",
       " 'PLATELETS': 479,\n",
       " 'NEUTROPHILS': 480,\n",
       " 'LYMPHOCYTES': 481,\n",
       " 'MONOCYTES': 482,\n",
       " 'Conditions': 483,\n",
       " 'Problem': 484,\n",
       " 'Idiopathic': 485,\n",
       " 'urticaria': 486,\n",
       " 'document': 487,\n",
       " 'wish': 488,\n",
       " 'keep': 489,\n",
       " 'Policyholder': 490,\n",
       " 'Owner': 491,\n",
       " 'Eastside': 492,\n",
       " 'Acct': 493,\n",
       " 'Jasminder': 494,\n",
       " 'Singh': 495,\n",
       " 'Dev': 496,\n",
       " 'PA': 497,\n",
       " 'EXCUSE': 498,\n",
       " 'east': 499,\n",
       " 'side': 500,\n",
       " 'medical': 501,\n",
       " 'center': 502,\n",
       " 'April': 503,\n",
       " 'Weekly': 504,\n",
       " 'MEDICAL': 505,\n",
       " 'Ph': 506,\n",
       " 'MR': 507,\n",
       " 'Primary': 508,\n",
       " 'Thoracic': 509,\n",
       " 'Strain': 510,\n",
       " 'have': 511,\n",
       " 'strained': 512,\n",
       " 'your': 513,\n",
       " 'thoracic': 514,\n",
       " 'spine': 515,\n",
       " 'IF': 516,\n",
       " 'ANY': 517,\n",
       " 'FOLLOWING': 518,\n",
       " 'OCCURS': 519,\n",
       " 'feel': 520,\n",
       " 'weakness': 521,\n",
       " 'arms': 522,\n",
       " 'legs': 523,\n",
       " 'severe': 524,\n",
       " 'increase': 525,\n",
       " 'pain': 526,\n",
       " 'Lumbosacral': 527,\n",
       " 'weak': 528,\n",
       " 'becomes': 529,\n",
       " 'more': 530,\n",
       " 'Follow': 531,\n",
       " 'Up': 532,\n",
       " 'What': 533,\n",
       " 'Do': 534,\n",
       " 'Take': 535,\n",
       " 'all': 536,\n",
       " 'as': 537,\n",
       " 'directed': 538,\n",
       " 'Additional': 539,\n",
       " 'Prescriptions': 540,\n",
       " 'Written': 541,\n",
       " 'Prescriber': 542,\n",
       " 'Paper': 543,\n",
       " 'Prescription': 544,\n",
       " 'given': 545,\n",
       " 'patient': 546,\n",
       " 'Preventative': 547,\n",
       " 'Instructions': 548,\n",
       " 'knee': 549,\n",
       " 'injury': 550,\n",
       " 'David': 551,\n",
       " 'Bruce': 552,\n",
       " 'Identiﬁer': 553,\n",
       " 'June': 554,\n",
       " 'Explanation': 555,\n",
       " 'Gap': 556,\n",
       " 'no': 557,\n",
       " 'concussion': 558,\n",
       " 'Assistant': 559,\n",
       " 'devin': 560,\n",
       " 'conrad': 561,\n",
       " 'September': 562,\n",
       " 'ACCIDENT': 563,\n",
       " 'CLAIM': 564,\n",
       " 'FORM': 565,\n",
       " 'ATTENDING': 566,\n",
       " 'PHYSICIAN': 567,\n",
       " 'PLEASE': 568,\n",
       " 'PRINT': 569,\n",
       " 'PART': 570,\n",
       " 'I': 571,\n",
       " 'BE': 572,\n",
       " 'COMPLETED': 573,\n",
       " 'BY': 574,\n",
       " 'ICD': 575,\n",
       " 'first': 576,\n",
       " 'unable': 577,\n",
       " 'work': 578,\n",
       " 'Expected': 579,\n",
       " 'Delivery': 580,\n",
       " 'Actual': 581,\n",
       " 'Unable': 582,\n",
       " 'Vaginal': 583,\n",
       " 'per': 584,\n",
       " 'Continued': 585,\n",
       " 'Facility': 586,\n",
       " 'State': 587,\n",
       " 'Zip': 588,\n",
       " 'Performed': 589,\n",
       " 'Surgical': 590,\n",
       " 'CPT': 591,\n",
       " 'Attending': 592,\n",
       " 'Degree': 593,\n",
       " 'A': 594,\n",
       " 'check': 595,\n",
       " 'type': 596,\n",
       " 'claim': 597,\n",
       " 'filing': 598,\n",
       " 'B': 599,\n",
       " 'Suffix': 600,\n",
       " 'MI': 601,\n",
       " 'Spanish': 602,\n",
       " 'Short': 603,\n",
       " 'Term': 604,\n",
       " 'Disability': 605,\n",
       " 'Long': 606,\n",
       " 'Life': 607,\n",
       " 'Insurance': 608,\n",
       " 'Voluntary': 609,\n",
       " 'Was': 610,\n",
       " 'this': 611,\n",
       " 'motor': 612,\n",
       " 'vehicle': 613,\n",
       " 'accident': 614,\n",
       " 'Physicians': 615,\n",
       " 'Hospitals': 616,\n",
       " 'Considerations': 617,\n",
       " 'Folder': 618,\n",
       " 'Contents': 619,\n",
       " 'Claimant': 620,\n",
       " 'Unauthorized': 621,\n",
       " 'access': 622,\n",
       " 'is': 623,\n",
       " 'strictly': 624,\n",
       " 'probihited': 625,\n",
       " 'Male': 626,\n",
       " 'Reach': 627,\n",
       " 'Return': 628,\n",
       " 'Sprained': 629,\n",
       " 'Ankle': 630,\n",
       " 'Practitioner': 631,\n",
       " 'Monica': 632,\n",
       " 'Shaffer': 633,\n",
       " 'Johnson': 634,\n",
       " 'Ave': 635,\n",
       " 'Bridgeport': 636,\n",
       " 'WV': 637,\n",
       " 'US': 638,\n",
       " 'Accountability': 639,\n",
       " 'Act': 640,\n",
       " 'HIPAA': 641,\n",
       " 'Privacy': 642,\n",
       " 'Rule': 643,\n",
       " 'banks': 644,\n",
       " 'governmental': 645,\n",
       " 'entities': 646,\n",
       " 'communicable': 647,\n",
       " 'disease': 648,\n",
       " 'CL': 649,\n",
       " 'GREGORY': 650,\n",
       " 'we': 651,\n",
       " 'can': 652,\n",
       " 'assist': 653,\n",
       " 'prohibited': 654,\n",
       " 'otherwise': 655,\n",
       " 'permitted': 656,\n",
       " 'law': 657,\n",
       " 'EMS': 658,\n",
       " 'Christopher': 659,\n",
       " 'Bartruff': 660,\n",
       " 'health': 661,\n",
       " 'park': 662,\n",
       " 'blvd': 663,\n",
       " 'Naples': 664,\n",
       " 'FL': 665,\n",
       " 'NCH': 666,\n",
       " 'Emergency': 667,\n",
       " 'Healthcare': 668,\n",
       " 'System': 669,\n",
       " 'Napies': 670,\n",
       " 'North': 671,\n",
       " 'Collier': 672,\n",
       " 'Northeast': 673,\n",
       " 'ED': 674,\n",
       " 'Chief': 675,\n",
       " 'ICD=CC': 676,\n",
       " 'Numbers': 677,\n",
       " 'Feeling': 678,\n",
       " 'Suicidal': 679,\n",
       " 'Help': 680,\n",
       " 'With': 681,\n",
       " 'Where': 682,\n",
       " 'When': 683,\n",
       " 'Comments': 684,\n",
       " 'Dear': 685,\n",
       " 'However': 686,\n",
       " 'Estimado': 687,\n",
       " 'Paciente': 688,\n",
       " 'has': 689,\n",
       " 'been': 690,\n",
       " 'these': 691,\n",
       " 'instructions': 692,\n",
       " 'HOWEVER': 693,\n",
       " 'Education': 694,\n",
       " 'Materials': 695,\n",
       " 'Peds': 696,\n",
       " 'Upper': 697,\n",
       " 'Extremity': 698,\n",
       " 'Contusion': 699,\n",
       " 'Home': 700,\n",
       " 'care': 701,\n",
       " 'any': 702,\n",
       " 'Special': 703,\n",
       " 'note': 704,\n",
       " 'parents': 705,\n",
       " 'seek': 706,\n",
       " 'advice': 707,\n",
       " 'Bruising': 708,\n",
       " 'that': 709,\n",
       " 'gets': 710,\n",
       " 'worse': 711,\n",
       " 'Numbness': 712,\n",
       " 'tingling': 713,\n",
       " 'injured': 714,\n",
       " 'arm': 715,\n",
       " 'Trauma': 716,\n",
       " 'Watch': 717,\n",
       " 'following': 718,\n",
       " 'symptoms': 719,\n",
       " 'Headache': 720,\n",
       " 'Nausea': 721,\n",
       " 'vomiting': 722,\n",
       " 'Sensitivity': 723,\n",
       " 'light': 724,\n",
       " 'noise': 725,\n",
       " 'Unusual': 726,\n",
       " 'sleepiness': 727,\n",
       " 'grogginess': 728,\n",
       " 'Trouble': 729,\n",
       " 'falling': 730,\n",
       " 'asleep': 731,\n",
       " 'Personality': 732,\n",
       " 'changes': 733,\n",
       " 'Vision': 734,\n",
       " 'Memory': 735,\n",
       " 'loss': 736,\n",
       " 'Confusion': 737,\n",
       " 'walking': 738,\n",
       " 'clumsiness': 739,\n",
       " 'Loss': 740,\n",
       " 'consciousness': 741,\n",
       " 'even': 742,\n",
       " 'short': 743,\n",
       " 'time': 744,\n",
       " 'Inability': 745,\n",
       " 'awakened': 746,\n",
       " 'Stiff': 747,\n",
       " 'neck': 748,\n",
       " 'Weakness': 749,\n",
       " 'numbness': 750,\n",
       " 'part': 751,\n",
       " 'body': 752,\n",
       " 'Seizures': 753,\n",
       " 'General': 754,\n",
       " 'Avoid': 755,\n",
       " 'lifting': 756,\n",
       " 'strenuous': 757,\n",
       " 'activities': 758,\n",
       " 'Pain': 759,\n",
       " 'doesn': 760,\n",
       " 't': 761,\n",
       " 'get': 762,\n",
       " 'better': 763,\n",
       " 'worsens': 764,\n",
       " 'increased': 765,\n",
       " 'swelling': 766,\n",
       " 'bruising': 767,\n",
       " 'Sick': 768,\n",
       " 'appearance': 769,\n",
       " 'behaviors': 770,\n",
       " 'worry': 771,\n",
       " 'Excuse': 772,\n",
       " 'From': 773,\n",
       " 'School': 774,\n",
       " 'excuse': 775,\n",
       " 'from': 776,\n",
       " 'until': 777,\n",
       " 'Caregiver': 778,\n",
       " 'Document': 779,\n",
       " 'Released': 780,\n",
       " 'reminders': 781,\n",
       " 'regarding': 782,\n",
       " 'prescriptions': 783,\n",
       " 'provided': 784,\n",
       " 'leaflets': 785,\n",
       " 'COLLIER': 786,\n",
       " 'COUNTRY': 787,\n",
       " 'DIGITECH': 788,\n",
       " 'COMPUTER': 789,\n",
       " 'BILLING': 790,\n",
       " 'ON': 791,\n",
       " 'BEHALF': 792,\n",
       " 'BEDFORD': 793,\n",
       " 'RD': 794,\n",
       " 'BLDG': 795,\n",
       " 'FLOOR': 796,\n",
       " 'CHAPPAQUA': 797,\n",
       " 'NY': 798,\n",
       " 'VISIT': 799,\n",
       " 'HTTPS': 800,\n",
       " 'PAY': 801,\n",
       " 'THIS': 802,\n",
       " 'INVOICE': 803,\n",
       " 'COUNTY': 804,\n",
       " 'NAPLES': 805,\n",
       " 'N': 806,\n",
       " 'NORTH': 807,\n",
       " 'HOSPITAL': 808,\n",
       " 'PARK': 809,\n",
       " 'BLVD': 810,\n",
       " 'BAKER': 811,\n",
       " 'MARCO': 812,\n",
       " 'HEALTHCARE': 813,\n",
       " 'NORTHEAST': 814,\n",
       " 'EMERGENCY': 815,\n",
       " 'DEPARTMENT': 816,\n",
       " 'Lockbox': 817,\n",
       " 'Processing': 818,\n",
       " 'Atlanta': 819,\n",
       " 'GA': 820,\n",
       " 'SYSTEM': 821,\n",
       " 'FOR': 822,\n",
       " 'AADC': 823,\n",
       " 'RE': 824,\n",
       " 'thirty': 825,\n",
       " 'days': 826,\n",
       " 'receipt': 827,\n",
       " 'letter': 828,\n",
       " 'Accounting': 829,\n",
       " 'SOUTHWEST': 830,\n",
       " 'FLORIDA': 831,\n",
       " 'MANAGEMENT': 832,\n",
       " 'CINCINNATI': 833,\n",
       " 'OH': 834,\n",
       " 'PHONE': 835,\n",
       " 'TH_AR_LTR': 836,\n",
       " 'SERVICE': 837,\n",
       " 'NUMBER': 838,\n",
       " 'JASON': 839,\n",
       " 'SANTANA': 840,\n",
       " 'Sincerely': 841,\n",
       " 'PLANTATION': 842,\n",
       " 'Allstate': 843,\n",
       " 'Youre': 844,\n",
       " 'good': 845,\n",
       " 'hands': 846,\n",
       " 'Florida': 847,\n",
       " 'PIP': 848,\n",
       " 'Central': 849,\n",
       " 'CLINTON': 850,\n",
       " 'IA': 851,\n",
       " 'February': 852,\n",
       " 'INSURED': 853,\n",
       " 'LOSS': 854,\n",
       " 'FAX': 855,\n",
       " 'OFFICE': 856,\n",
       " 'HOURS': 857,\n",
       " 'VYPHAPHONE': 858,\n",
       " 'INTHALANGSY': 859,\n",
       " 'EXT': 860,\n",
       " \"'re\": 861,\n",
       " 'Jacksonville': 862,\n",
       " 'DALLAS': 863,\n",
       " 'TX': 864,\n",
       " 'Some': 865,\n",
       " 'specifics': 866,\n",
       " 'request': 867,\n",
       " 'sincerely': 868,\n",
       " 'CAITLEN': 869,\n",
       " 'CARROLL': 870,\n",
       " 'Ext': 871,\n",
       " 'date': 872,\n",
       " 'name': 873,\n",
       " 'Cellular': 874,\n",
       " 'MedSupport': 875,\n",
       " 'policy': 876,\n",
       " 'policies': 877,\n",
       " 'Female': 878,\n",
       " 'Domestic': 879,\n",
       " 'Partner': 880,\n",
       " 'information': 881,\n",
       " 'Condition': 882,\n",
       " 'am': 883,\n",
       " 'pm': 884,\n",
       " 'Confinement': 885,\n",
       " 'Dates': 886,\n",
       " 'Teri': 887,\n",
       " 'Willochell': 888,\n",
       " 'phycian': 889,\n",
       " 'considerations': 890,\n",
       " 'each': 891,\n",
       " 'such': 892,\n",
       " 'violation': 893,\n",
       " 'number': 894,\n",
       " 'indicated': 895,\n",
       " 'above': 896,\n",
       " 'My': 897,\n",
       " 'Member': 898,\n",
       " 'Relationship': 899,\n",
       " 'person': 900,\n",
       " 'granting': 901,\n",
       " 'authority': 902,\n",
       " 'birth': 903,\n",
       " 'Including': 904,\n",
       " 'L': 905,\n",
       " 'lesser': 906,\n",
       " 'If': 907,\n",
       " 'yes': 908,\n",
       " 'please': 909,\n",
       " 'provide': 910,\n",
       " 'tha': 911,\n",
       " 'diagnosis': 912,\n",
       " 'Treatment': 913,\n",
       " 'advise': 914,\n",
       " 'stop': 915,\n",
       " 'working': 916,\n",
       " 'what': 917,\n",
       " 'Willochel': 918,\n",
       " 'Specially': 919,\n",
       " 'internal': 920,\n",
       " 'medicine': 921,\n",
       " 'MD': 922,\n",
       " 'MedExpress': 923,\n",
       " 'Urgent': 924,\n",
       " 'Care': 925,\n",
       " 'Route': 926,\n",
       " 'HIPPA': 927,\n",
       " 'Socia': 928,\n",
       " 'Location': 929,\n",
       " 'Norwin': 930,\n",
       " 'Huntingdon': 931,\n",
       " 'Holder': 932,\n",
       " 'Sex': 933,\n",
       " 'COMP': 934,\n",
       " 'Clinical': 935,\n",
       " 'Report': 936,\n",
       " 'BP': 937,\n",
       " 'mmHg': 938,\n",
       " 'PULSE': 939,\n",
       " 'bpm': 940,\n",
       " 'RESP': 941,\n",
       " 'TEMP': 942,\n",
       " 'WEIGTH': 943,\n",
       " 'ft': 944,\n",
       " 'BMI': 945,\n",
       " 'LMP': 946,\n",
       " 'PMP': 947,\n",
       " 'SAT': 948,\n",
       " 'CONTUSION': 949,\n",
       " 'Meds': 950,\n",
       " 'ACTIVE': 951,\n",
       " 'acetaminophen': 952,\n",
       " 'albuterol': 953,\n",
       " 'bulk': 954,\n",
       " 'Dilantin': 955,\n",
       " 'gabapentin': 956,\n",
       " 'Humalog': 957,\n",
       " 'Lamictal': 958,\n",
       " 'Lyrica': 959,\n",
       " 'Neurontin': 960,\n",
       " 'valacyclovir': 961,\n",
       " 'Procedures': 962,\n",
       " 'FOOT': 963,\n",
       " 'MIN': 964,\n",
       " 'THREE': 965,\n",
       " 'ESTAB': 966,\n",
       " 'URGENT': 967,\n",
       " 'CARE': 968,\n",
       " 'Pocket': 969,\n",
       " 'Met': 970,\n",
       " 'Estimated': 971,\n",
       " 'charges': 972,\n",
       " 'MSO': 973,\n",
       " 'LLC': 974,\n",
       " 'Med': 975,\n",
       " 'Express': 976,\n",
       " 'UC': 977,\n",
       " 'HUNINGTON': 978,\n",
       " 'Merchant': 979,\n",
       " 'Transaction': 980,\n",
       " 'PURCHASE': 981,\n",
       " 'Approval': 982,\n",
       " 'code': 983,\n",
       " 'Record': 984,\n",
       " 'Visa': 985,\n",
       " 'Trace': 986,\n",
       " 'reference': 987,\n",
       " 'Cardholder': 988,\n",
       " 'identifier': 989,\n",
       " 'Application': 990,\n",
       " 'label': 991,\n",
       " 'TVR': 992,\n",
       " 'AID': 993,\n",
       " 'Subtotal': 994,\n",
       " 'Sales': 995,\n",
       " 'customer': 996,\n",
       " 'copy': 997,\n",
       " 'send': 998,\n",
       " 'payments': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'Claim',\n",
       " 5: 'Type',\n",
       " 6: 'VB',\n",
       " 7: 'Accident',\n",
       " 8: 'Accidental',\n",
       " 9: 'Injury',\n",
       " 10: 'Information',\n",
       " 11: 'First',\n",
       " 12: 'Name',\n",
       " 13: 'Middle',\n",
       " 14: 'Last',\n",
       " 15: 'Social',\n",
       " 16: 'Security',\n",
       " 17: 'Number',\n",
       " 18: 'Birth',\n",
       " 19: 'Date',\n",
       " 20: 'Gender',\n",
       " 21: 'Language',\n",
       " 22: 'Preference',\n",
       " 23: 'Address',\n",
       " 24: 'Line',\n",
       " 25: 'City',\n",
       " 26: 'Postal',\n",
       " 27: 'Code',\n",
       " 28: 'Country',\n",
       " 29: 'Best',\n",
       " 30: 'Phone',\n",
       " 31: 'to',\n",
       " 32: 'be',\n",
       " 33: 'Reached',\n",
       " 34: 'During',\n",
       " 35: 'the',\n",
       " 36: 'Day',\n",
       " 37: 'Email',\n",
       " 38: 'Page',\n",
       " 39: 'of',\n",
       " 40: 'RADIOLOGY',\n",
       " 41: 'REPORT',\n",
       " 42: 'Patient',\n",
       " 43: 'MRN',\n",
       " 44: 'Accession',\n",
       " 45: 'No',\n",
       " 46: 'Ref',\n",
       " 47: 'Physician',\n",
       " 48: 'UNKNOWN',\n",
       " 49: 'Study',\n",
       " 50: 'Hospital',\n",
       " 51: 'DOB',\n",
       " 52: 'Technique',\n",
       " 53: 'views',\n",
       " 54: 'left',\n",
       " 55: 'wrist',\n",
       " 56: 'Cormarison',\n",
       " 57: 'None',\n",
       " 58: 'availabie',\n",
       " 59: 'Comparison',\n",
       " 60: 'available',\n",
       " 61: 'FINDINGS',\n",
       " 62: 'IMPRESSION',\n",
       " 63: 'acute',\n",
       " 64: 'osseous',\n",
       " 65: 'abnormality',\n",
       " 66: 'identified',\n",
       " 67: 'Daytime',\n",
       " 68: 'Event',\n",
       " 69: 'Stopped',\n",
       " 70: 'Working',\n",
       " 71: 'Yes',\n",
       " 72: 'Physically',\n",
       " 73: 'at',\n",
       " 74: 'Work',\n",
       " 75: 'Hours',\n",
       " 76: 'Worked',\n",
       " 77: 'on',\n",
       " 78: 'Scheduled',\n",
       " 79: 'Missed',\n",
       " 80: 'Returned',\n",
       " 81: 'Related',\n",
       " 82: 'Time',\n",
       " 83: 'Diagnosis',\n",
       " 84: 'Arthiscopic',\n",
       " 85: 'surgery',\n",
       " 86: 'Surgery',\n",
       " 87: 'Is',\n",
       " 88: 'Required',\n",
       " 89: 'Indicator',\n",
       " 90: 'Outpatient',\n",
       " 91: 'Medical',\n",
       " 92: 'Provider',\n",
       " 93: 'Roles',\n",
       " 94: 'Treating',\n",
       " 95: 'Patrick',\n",
       " 96: 'Emerson',\n",
       " 97: 'Business',\n",
       " 98: 'Telephone',\n",
       " 99: 'Fax',\n",
       " 100: 'Visit',\n",
       " 101: 'Next',\n",
       " 102: 'Hospitalization',\n",
       " 103: 'Discharge',\n",
       " 104: 'Procedure',\n",
       " 105: 'Left',\n",
       " 106: 'arthiscopic',\n",
       " 107: 'Employment',\n",
       " 108: 'Employer',\n",
       " 109: 'Policy',\n",
       " 110: 'Electronic',\n",
       " 111: 'Submission',\n",
       " 112: 'Identifier',\n",
       " 113: 'Electronically',\n",
       " 114: 'Signed',\n",
       " 115: 'Fraud',\n",
       " 116: 'Statements',\n",
       " 117: 'Reviewed',\n",
       " 118: 'and',\n",
       " 119: 'unum',\n",
       " 120: 'The',\n",
       " 121: 'Benefits',\n",
       " 122: 'Center',\n",
       " 123: 'Not',\n",
       " 124: 'for',\n",
       " 125: 'FMLA',\n",
       " 126: 'Requests',\n",
       " 127: 'Insured',\n",
       " 128: '’',\n",
       " 129: 's',\n",
       " 130: 'Signature',\n",
       " 131: 'Printed',\n",
       " 132: 'Unum',\n",
       " 133: 'Confirmation',\n",
       " 134: 'Coverage',\n",
       " 135: 'Group',\n",
       " 136: 'Customer',\n",
       " 137: 'EE',\n",
       " 138: 'Effective',\n",
       " 139: 'Employee',\n",
       " 140: 'Acc',\n",
       " 141: 'January',\n",
       " 142: 'Wellness',\n",
       " 143: 'Benefit',\n",
       " 144: 'Total',\n",
       " 145: 'Monthly',\n",
       " 146: 'Premium',\n",
       " 147: 'Montly',\n",
       " 148: 'Payroll',\n",
       " 149: 'Deduction',\n",
       " 150: 'Account',\n",
       " 151: 'F',\n",
       " 152: 'Exam',\n",
       " 153: 'Referring',\n",
       " 154: 'Phys',\n",
       " 155: 'STEPHEN',\n",
       " 156: 'GELOVICH',\n",
       " 157: 'Tax',\n",
       " 158: 'MRI',\n",
       " 159: 'OF',\n",
       " 160: 'THE',\n",
       " 161: 'LEFT',\n",
       " 162: 'WRIST',\n",
       " 163: 'WITHOUT',\n",
       " 164: 'CONTRAST',\n",
       " 165: 'COMPARISON',\n",
       " 166: 'These',\n",
       " 167: 'results',\n",
       " 168: 'were',\n",
       " 169: 'faxed',\n",
       " 170: 'Gelovich',\n",
       " 171: 'signed',\n",
       " 172: 'by',\n",
       " 173: 'Stephen',\n",
       " 174: 'Bravo',\n",
       " 175: 'Dependent',\n",
       " 176: 'Detail',\n",
       " 177: 'Zachary',\n",
       " 178: 'Jager',\n",
       " 179: 'Billed',\n",
       " 180: 'Amounts',\n",
       " 181: 'Contract',\n",
       " 182: 'Adjustment',\n",
       " 183: 'Allowed',\n",
       " 184: 'Amount',\n",
       " 185: 'Covered',\n",
       " 186: 'Reason',\n",
       " 187: 'Deductible',\n",
       " 188: 'Other',\n",
       " 189: 'Carrier',\n",
       " 190: 'Paid',\n",
       " 191: 'Responsibility',\n",
       " 192: 'Totals',\n",
       " 193: 'For',\n",
       " 194: 'Appeals',\n",
       " 195: 'Rights',\n",
       " 196: 'Important',\n",
       " 197: 'about',\n",
       " 198: 'Your',\n",
       " 199: 'Appeal',\n",
       " 200: 'All',\n",
       " 201: 'Languages',\n",
       " 202: 'Contact',\n",
       " 203: 'Did',\n",
       " 204: 'You',\n",
       " 205: 'Know',\n",
       " 206: 'Specialty',\n",
       " 207: 'Orthopedic',\n",
       " 208: 'Surgeon',\n",
       " 209: 'Unknown',\n",
       " 210: 'Kari',\n",
       " 211: 'Lund',\n",
       " 212: 'Orthopedist',\n",
       " 213: 'Dan',\n",
       " 214: 'Palmer',\n",
       " 215: 'On',\n",
       " 216: '&',\n",
       " 217: 'May',\n",
       " 218: 'Spouse',\n",
       " 219: 'Child',\n",
       " 220: 'BLACK',\n",
       " 221: 'HILLS',\n",
       " 222: 'ORTHOPEDIC',\n",
       " 223: 'CENTER',\n",
       " 224: 'PC',\n",
       " 225: 'LAST',\n",
       " 226: 'PMT',\n",
       " 227: 'AMOUNT',\n",
       " 228: 'DUE',\n",
       " 229: 'DATE',\n",
       " 230: 'PAGE',\n",
       " 231: 'STATEMENT',\n",
       " 232: 'Ins',\n",
       " 233: 'Description',\n",
       " 234: 'E',\n",
       " 235: 'm',\n",
       " 236: 'New',\n",
       " 237: 'Moderat',\n",
       " 238: 'S',\n",
       " 239: 'Clo',\n",
       " 240: 'Tx',\n",
       " 241: 'Phalangealfx',\n",
       " 242: 'Finger',\n",
       " 243: 'Splint',\n",
       " 244: 'Offic',\n",
       " 245: 'Cons',\n",
       " 246: 'Moderate',\n",
       " 247: 'Sever',\n",
       " 248: 'Rad',\n",
       " 249: 'Mini',\n",
       " 250: 'Views',\n",
       " 251: 'Applic',\n",
       " 252: 'Hand',\n",
       " 253: 'Lower',\n",
       " 254: 'Forearm',\n",
       " 255: 'Fiberglass',\n",
       " 256: 'gauntlet',\n",
       " 257: 'Cast',\n",
       " 258: 'Yrs',\n",
       " 259: 'Charge',\n",
       " 260: 'Pmt',\n",
       " 261: 'Pat',\n",
       " 262: 'Adjust',\n",
       " 263: 'Current',\n",
       " 264: 'Days',\n",
       " 265: 'Balance',\n",
       " 266: 'Pending',\n",
       " 267: 'Now',\n",
       " 268: 'Due',\n",
       " 269: 'Message',\n",
       " 270: 'Make',\n",
       " 271: 'Checks',\n",
       " 272: 'Payable',\n",
       " 273: 'To',\n",
       " 274: 'Statement',\n",
       " 275: 'Billing',\n",
       " 276: 'Questions',\n",
       " 277: 'Choice',\n",
       " 278: 'Health',\n",
       " 279: 'Administrators',\n",
       " 280: 'Forwarding',\n",
       " 281: 'Service',\n",
       " 282: 'Requested',\n",
       " 283: 'REGIONAL',\n",
       " 284: 'HEALTH',\n",
       " 285: 'INC',\n",
       " 286: 'Participant',\n",
       " 287: 'ID',\n",
       " 288: 'Original',\n",
       " 289: 'Print',\n",
       " 290: 'Website',\n",
       " 291: 'DEA',\n",
       " 292: 'Individual',\n",
       " 293: 'Summary',\n",
       " 294: 'By',\n",
       " 295: 'Plan',\n",
       " 296: 'Status',\n",
       " 297: 'Period',\n",
       " 298: 'DEDUCTIBLE',\n",
       " 299: 'OUT',\n",
       " 300: 'POCKET',\n",
       " 301: 'Regional',\n",
       " 302: 'Inc',\n",
       " 303: 'EXPLANATION',\n",
       " 304: 'BENEFITS',\n",
       " 305: 'Retain',\n",
       " 306: 'Purposes',\n",
       " 307: 'Sign',\n",
       " 308: 'up',\n",
       " 309: 'paperless',\n",
       " 310: 'Family',\n",
       " 311: 'Out',\n",
       " 312: 'Network',\n",
       " 313: 'Karl',\n",
       " 314: 'Services',\n",
       " 315: 'exam',\n",
       " 316: 'hand',\n",
       " 317: 'Modifiers',\n",
       " 318: 'TC',\n",
       " 319: 'RT',\n",
       " 320: 'This',\n",
       " 321: 'Qualified',\n",
       " 322: 'sign',\n",
       " 323: 'language',\n",
       " 324: 'interpreters',\n",
       " 325: 'written',\n",
       " 326: 'in',\n",
       " 327: 'other',\n",
       " 328: 'languages',\n",
       " 329: 'Jacquelin',\n",
       " 330: 'Brainard',\n",
       " 331: 'Compliance',\n",
       " 332: 'Officer',\n",
       " 333: 'or',\n",
       " 334: 'mail',\n",
       " 335: 'phone',\n",
       " 336: 'Department',\n",
       " 337: 'Human',\n",
       " 338: 'Complaint',\n",
       " 339: 'forms',\n",
       " 340: 'are',\n",
       " 341: 'DAKOTA',\n",
       " 342: 'EZ',\n",
       " 343: 'Ways',\n",
       " 344: 'Pay',\n",
       " 345: 'Automated',\n",
       " 346: 'Attendant',\n",
       " 347: 'hours',\n",
       " 348: 'a',\n",
       " 349: 'day',\n",
       " 350: 'Payments',\n",
       " 351: 'Please',\n",
       " 352: 'Call',\n",
       " 353: 'Upon',\n",
       " 354: 'Receipt',\n",
       " 355: 'Improved',\n",
       " 356: 'Online',\n",
       " 357: 'Experience',\n",
       " 358: '|',\n",
       " 359: 'Update',\n",
       " 360: 'Info',\n",
       " 361: 'About',\n",
       " 362: 'See',\n",
       " 363: 'Details',\n",
       " 364: 'Back',\n",
       " 365: 'ACCOUNT',\n",
       " 366: 'NO',\n",
       " 367: 'SHOW',\n",
       " 368: 'PAID',\n",
       " 369: 'HERE',\n",
       " 370: 'MAKE',\n",
       " 371: 'CHECKS',\n",
       " 372: 'TO',\n",
       " 373: 'PROC',\n",
       " 374: 'CODE',\n",
       " 375: 'UNITS',\n",
       " 376: 'DETAILS',\n",
       " 377: 'SERVICES',\n",
       " 378: 'CHARGES',\n",
       " 379: 'INSUR',\n",
       " 380: 'PENDING',\n",
       " 381: 'PATIENT',\n",
       " 382: 'BALANCE',\n",
       " 383: 'EXAM',\n",
       " 384: 'HAND',\n",
       " 385: 'THORAC',\n",
       " 386: 'SPINE',\n",
       " 387: 'COMMERCIAL',\n",
       " 388: 'NON',\n",
       " 389: 'ALLOWED',\n",
       " 390: 'CT',\n",
       " 391: 'ABD',\n",
       " 392: 'PELV',\n",
       " 393: 'PAYMENT',\n",
       " 394: 'CHEST',\n",
       " 395: 'VIEWS',\n",
       " 396: 'HARGES',\n",
       " 397: 'Over',\n",
       " 398: 'DIGIT',\n",
       " 399: 'Of',\n",
       " 400: 'Today',\n",
       " 401: \"'s\",\n",
       " 402: 'Ethnicity',\n",
       " 403: 'Hispanic',\n",
       " 404: 'Latino',\n",
       " 405: 'Preferred',\n",
       " 406: 'English',\n",
       " 407: 'visit',\n",
       " 408: 'with',\n",
       " 409: 'Suzanne',\n",
       " 410: 'Newsom',\n",
       " 411: 'CNP',\n",
       " 412: '•',\n",
       " 413: 'Lethargy',\n",
       " 414: 'cough',\n",
       " 415: 'Vitals',\n",
       " 416: 'lbs',\n",
       " 417: 'kg',\n",
       " 418: 'Wt',\n",
       " 419: 'Temp',\n",
       " 420: 'HR',\n",
       " 421: 'Oxygen',\n",
       " 422: 'sat',\n",
       " 423: 'Allergies',\n",
       " 424: 'Amoxicillin',\n",
       " 425: 'rash',\n",
       " 426: 'possible',\n",
       " 427: 'hives',\n",
       " 428: 'Active',\n",
       " 429: 'Diagnoses',\n",
       " 430: 'Include',\n",
       " 431: 'Acute',\n",
       " 432: 'frontal',\n",
       " 433: 'sinusitis',\n",
       " 434: 'unspecified',\n",
       " 435: 'Dizziness',\n",
       " 436: 'giddiness',\n",
       " 437: 'Medication',\n",
       " 438: 'List',\n",
       " 439: 'medications',\n",
       " 440: 'you',\n",
       " 441: 'Taking',\n",
       " 442: 'Zyrtec',\n",
       " 443: 'Childrens',\n",
       " 444: 'Allergy',\n",
       " 445: 'Notes',\n",
       " 446: 'Tests',\n",
       " 447: 'Labs',\n",
       " 448: 'Illumigene',\n",
       " 449: 'MYCO',\n",
       " 450: 'http',\n",
       " 451: 'BASIC',\n",
       " 452: 'METABOLIC',\n",
       " 453: 'SODIUM',\n",
       " 454: 'Range',\n",
       " 455: 'POTASSIUM',\n",
       " 456: 'CHLORIDE',\n",
       " 457: 'GLUCOSE',\n",
       " 458: 'BUN',\n",
       " 459: 'CREATININE',\n",
       " 460: 'CALCIUM',\n",
       " 461: 'CREA',\n",
       " 462: 'RATIO',\n",
       " 463: 'Ratio',\n",
       " 464: 'ANION',\n",
       " 465: 'GAP',\n",
       " 466: 'Calc',\n",
       " 467: 'CBC',\n",
       " 468: 'DIFF',\n",
       " 469: 'WBC',\n",
       " 470: 'RBC',\n",
       " 471: 'HGB',\n",
       " 472: 'HCT',\n",
       " 473: 'MCV',\n",
       " 474: 'fL',\n",
       " 475: 'MCH',\n",
       " 476: 'pg',\n",
       " 477: 'MCHC',\n",
       " 478: 'MPV',\n",
       " 479: 'PLATELETS',\n",
       " 480: 'NEUTROPHILS',\n",
       " 481: 'LYMPHOCYTES',\n",
       " 482: 'MONOCYTES',\n",
       " 483: 'Conditions',\n",
       " 484: 'Problem',\n",
       " 485: 'Idiopathic',\n",
       " 486: 'urticaria',\n",
       " 487: 'document',\n",
       " 488: 'wish',\n",
       " 489: 'keep',\n",
       " 490: 'Policyholder',\n",
       " 491: 'Owner',\n",
       " 492: 'Eastside',\n",
       " 493: 'Acct',\n",
       " 494: 'Jasminder',\n",
       " 495: 'Singh',\n",
       " 496: 'Dev',\n",
       " 497: 'PA',\n",
       " 498: 'EXCUSE',\n",
       " 499: 'east',\n",
       " 500: 'side',\n",
       " 501: 'medical',\n",
       " 502: 'center',\n",
       " 503: 'April',\n",
       " 504: 'Weekly',\n",
       " 505: 'MEDICAL',\n",
       " 506: 'Ph',\n",
       " 507: 'MR',\n",
       " 508: 'Primary',\n",
       " 509: 'Thoracic',\n",
       " 510: 'Strain',\n",
       " 511: 'have',\n",
       " 512: 'strained',\n",
       " 513: 'your',\n",
       " 514: 'thoracic',\n",
       " 515: 'spine',\n",
       " 516: 'IF',\n",
       " 517: 'ANY',\n",
       " 518: 'FOLLOWING',\n",
       " 519: 'OCCURS',\n",
       " 520: 'feel',\n",
       " 521: 'weakness',\n",
       " 522: 'arms',\n",
       " 523: 'legs',\n",
       " 524: 'severe',\n",
       " 525: 'increase',\n",
       " 526: 'pain',\n",
       " 527: 'Lumbosacral',\n",
       " 528: 'weak',\n",
       " 529: 'becomes',\n",
       " 530: 'more',\n",
       " 531: 'Follow',\n",
       " 532: 'Up',\n",
       " 533: 'What',\n",
       " 534: 'Do',\n",
       " 535: 'Take',\n",
       " 536: 'all',\n",
       " 537: 'as',\n",
       " 538: 'directed',\n",
       " 539: 'Additional',\n",
       " 540: 'Prescriptions',\n",
       " 541: 'Written',\n",
       " 542: 'Prescriber',\n",
       " 543: 'Paper',\n",
       " 544: 'Prescription',\n",
       " 545: 'given',\n",
       " 546: 'patient',\n",
       " 547: 'Preventative',\n",
       " 548: 'Instructions',\n",
       " 549: 'knee',\n",
       " 550: 'injury',\n",
       " 551: 'David',\n",
       " 552: 'Bruce',\n",
       " 553: 'Identiﬁer',\n",
       " 554: 'June',\n",
       " 555: 'Explanation',\n",
       " 556: 'Gap',\n",
       " 557: 'no',\n",
       " 558: 'concussion',\n",
       " 559: 'Assistant',\n",
       " 560: 'devin',\n",
       " 561: 'conrad',\n",
       " 562: 'September',\n",
       " 563: 'ACCIDENT',\n",
       " 564: 'CLAIM',\n",
       " 565: 'FORM',\n",
       " 566: 'ATTENDING',\n",
       " 567: 'PHYSICIAN',\n",
       " 568: 'PLEASE',\n",
       " 569: 'PRINT',\n",
       " 570: 'PART',\n",
       " 571: 'I',\n",
       " 572: 'BE',\n",
       " 573: 'COMPLETED',\n",
       " 574: 'BY',\n",
       " 575: 'ICD',\n",
       " 576: 'first',\n",
       " 577: 'unable',\n",
       " 578: 'work',\n",
       " 579: 'Expected',\n",
       " 580: 'Delivery',\n",
       " 581: 'Actual',\n",
       " 582: 'Unable',\n",
       " 583: 'Vaginal',\n",
       " 584: 'per',\n",
       " 585: 'Continued',\n",
       " 586: 'Facility',\n",
       " 587: 'State',\n",
       " 588: 'Zip',\n",
       " 589: 'Performed',\n",
       " 590: 'Surgical',\n",
       " 591: 'CPT',\n",
       " 592: 'Attending',\n",
       " 593: 'Degree',\n",
       " 594: 'A',\n",
       " 595: 'check',\n",
       " 596: 'type',\n",
       " 597: 'claim',\n",
       " 598: 'filing',\n",
       " 599: 'B',\n",
       " 600: 'Suffix',\n",
       " 601: 'MI',\n",
       " 602: 'Spanish',\n",
       " 603: 'Short',\n",
       " 604: 'Term',\n",
       " 605: 'Disability',\n",
       " 606: 'Long',\n",
       " 607: 'Life',\n",
       " 608: 'Insurance',\n",
       " 609: 'Voluntary',\n",
       " 610: 'Was',\n",
       " 611: 'this',\n",
       " 612: 'motor',\n",
       " 613: 'vehicle',\n",
       " 614: 'accident',\n",
       " 615: 'Physicians',\n",
       " 616: 'Hospitals',\n",
       " 617: 'Considerations',\n",
       " 618: 'Folder',\n",
       " 619: 'Contents',\n",
       " 620: 'Claimant',\n",
       " 621: 'Unauthorized',\n",
       " 622: 'access',\n",
       " 623: 'is',\n",
       " 624: 'strictly',\n",
       " 625: 'probihited',\n",
       " 626: 'Male',\n",
       " 627: 'Reach',\n",
       " 628: 'Return',\n",
       " 629: 'Sprained',\n",
       " 630: 'Ankle',\n",
       " 631: 'Practitioner',\n",
       " 632: 'Monica',\n",
       " 633: 'Shaffer',\n",
       " 634: 'Johnson',\n",
       " 635: 'Ave',\n",
       " 636: 'Bridgeport',\n",
       " 637: 'WV',\n",
       " 638: 'US',\n",
       " 639: 'Accountability',\n",
       " 640: 'Act',\n",
       " 641: 'HIPAA',\n",
       " 642: 'Privacy',\n",
       " 643: 'Rule',\n",
       " 644: 'banks',\n",
       " 645: 'governmental',\n",
       " 646: 'entities',\n",
       " 647: 'communicable',\n",
       " 648: 'disease',\n",
       " 649: 'CL',\n",
       " 650: 'GREGORY',\n",
       " 651: 'we',\n",
       " 652: 'can',\n",
       " 653: 'assist',\n",
       " 654: 'prohibited',\n",
       " 655: 'otherwise',\n",
       " 656: 'permitted',\n",
       " 657: 'law',\n",
       " 658: 'EMS',\n",
       " 659: 'Christopher',\n",
       " 660: 'Bartruff',\n",
       " 661: 'health',\n",
       " 662: 'park',\n",
       " 663: 'blvd',\n",
       " 664: 'Naples',\n",
       " 665: 'FL',\n",
       " 666: 'NCH',\n",
       " 667: 'Emergency',\n",
       " 668: 'Healthcare',\n",
       " 669: 'System',\n",
       " 670: 'Napies',\n",
       " 671: 'North',\n",
       " 672: 'Collier',\n",
       " 673: 'Northeast',\n",
       " 674: 'ED',\n",
       " 675: 'Chief',\n",
       " 676: 'ICD=CC',\n",
       " 677: 'Numbers',\n",
       " 678: 'Feeling',\n",
       " 679: 'Suicidal',\n",
       " 680: 'Help',\n",
       " 681: 'With',\n",
       " 682: 'Where',\n",
       " 683: 'When',\n",
       " 684: 'Comments',\n",
       " 685: 'Dear',\n",
       " 686: 'However',\n",
       " 687: 'Estimado',\n",
       " 688: 'Paciente',\n",
       " 689: 'has',\n",
       " 690: 'been',\n",
       " 691: 'these',\n",
       " 692: 'instructions',\n",
       " 693: 'HOWEVER',\n",
       " 694: 'Education',\n",
       " 695: 'Materials',\n",
       " 696: 'Peds',\n",
       " 697: 'Upper',\n",
       " 698: 'Extremity',\n",
       " 699: 'Contusion',\n",
       " 700: 'Home',\n",
       " 701: 'care',\n",
       " 702: 'any',\n",
       " 703: 'Special',\n",
       " 704: 'note',\n",
       " 705: 'parents',\n",
       " 706: 'seek',\n",
       " 707: 'advice',\n",
       " 708: 'Bruising',\n",
       " 709: 'that',\n",
       " 710: 'gets',\n",
       " 711: 'worse',\n",
       " 712: 'Numbness',\n",
       " 713: 'tingling',\n",
       " 714: 'injured',\n",
       " 715: 'arm',\n",
       " 716: 'Trauma',\n",
       " 717: 'Watch',\n",
       " 718: 'following',\n",
       " 719: 'symptoms',\n",
       " 720: 'Headache',\n",
       " 721: 'Nausea',\n",
       " 722: 'vomiting',\n",
       " 723: 'Sensitivity',\n",
       " 724: 'light',\n",
       " 725: 'noise',\n",
       " 726: 'Unusual',\n",
       " 727: 'sleepiness',\n",
       " 728: 'grogginess',\n",
       " 729: 'Trouble',\n",
       " 730: 'falling',\n",
       " 731: 'asleep',\n",
       " 732: 'Personality',\n",
       " 733: 'changes',\n",
       " 734: 'Vision',\n",
       " 735: 'Memory',\n",
       " 736: 'loss',\n",
       " 737: 'Confusion',\n",
       " 738: 'walking',\n",
       " 739: 'clumsiness',\n",
       " 740: 'Loss',\n",
       " 741: 'consciousness',\n",
       " 742: 'even',\n",
       " 743: 'short',\n",
       " 744: 'time',\n",
       " 745: 'Inability',\n",
       " 746: 'awakened',\n",
       " 747: 'Stiff',\n",
       " 748: 'neck',\n",
       " 749: 'Weakness',\n",
       " 750: 'numbness',\n",
       " 751: 'part',\n",
       " 752: 'body',\n",
       " 753: 'Seizures',\n",
       " 754: 'General',\n",
       " 755: 'Avoid',\n",
       " 756: 'lifting',\n",
       " 757: 'strenuous',\n",
       " 758: 'activities',\n",
       " 759: 'Pain',\n",
       " 760: 'doesn',\n",
       " 761: 't',\n",
       " 762: 'get',\n",
       " 763: 'better',\n",
       " 764: 'worsens',\n",
       " 765: 'increased',\n",
       " 766: 'swelling',\n",
       " 767: 'bruising',\n",
       " 768: 'Sick',\n",
       " 769: 'appearance',\n",
       " 770: 'behaviors',\n",
       " 771: 'worry',\n",
       " 772: 'Excuse',\n",
       " 773: 'From',\n",
       " 774: 'School',\n",
       " 775: 'excuse',\n",
       " 776: 'from',\n",
       " 777: 'until',\n",
       " 778: 'Caregiver',\n",
       " 779: 'Document',\n",
       " 780: 'Released',\n",
       " 781: 'reminders',\n",
       " 782: 'regarding',\n",
       " 783: 'prescriptions',\n",
       " 784: 'provided',\n",
       " 785: 'leaflets',\n",
       " 786: 'COLLIER',\n",
       " 787: 'COUNTRY',\n",
       " 788: 'DIGITECH',\n",
       " 789: 'COMPUTER',\n",
       " 790: 'BILLING',\n",
       " 791: 'ON',\n",
       " 792: 'BEHALF',\n",
       " 793: 'BEDFORD',\n",
       " 794: 'RD',\n",
       " 795: 'BLDG',\n",
       " 796: 'FLOOR',\n",
       " 797: 'CHAPPAQUA',\n",
       " 798: 'NY',\n",
       " 799: 'VISIT',\n",
       " 800: 'HTTPS',\n",
       " 801: 'PAY',\n",
       " 802: 'THIS',\n",
       " 803: 'INVOICE',\n",
       " 804: 'COUNTY',\n",
       " 805: 'NAPLES',\n",
       " 806: 'N',\n",
       " 807: 'NORTH',\n",
       " 808: 'HOSPITAL',\n",
       " 809: 'PARK',\n",
       " 810: 'BLVD',\n",
       " 811: 'BAKER',\n",
       " 812: 'MARCO',\n",
       " 813: 'HEALTHCARE',\n",
       " 814: 'NORTHEAST',\n",
       " 815: 'EMERGENCY',\n",
       " 816: 'DEPARTMENT',\n",
       " 817: 'Lockbox',\n",
       " 818: 'Processing',\n",
       " 819: 'Atlanta',\n",
       " 820: 'GA',\n",
       " 821: 'SYSTEM',\n",
       " 822: 'FOR',\n",
       " 823: 'AADC',\n",
       " 824: 'RE',\n",
       " 825: 'thirty',\n",
       " 826: 'days',\n",
       " 827: 'receipt',\n",
       " 828: 'letter',\n",
       " 829: 'Accounting',\n",
       " 830: 'SOUTHWEST',\n",
       " 831: 'FLORIDA',\n",
       " 832: 'MANAGEMENT',\n",
       " 833: 'CINCINNATI',\n",
       " 834: 'OH',\n",
       " 835: 'PHONE',\n",
       " 836: 'TH_AR_LTR',\n",
       " 837: 'SERVICE',\n",
       " 838: 'NUMBER',\n",
       " 839: 'JASON',\n",
       " 840: 'SANTANA',\n",
       " 841: 'Sincerely',\n",
       " 842: 'PLANTATION',\n",
       " 843: 'Allstate',\n",
       " 844: 'Youre',\n",
       " 845: 'good',\n",
       " 846: 'hands',\n",
       " 847: 'Florida',\n",
       " 848: 'PIP',\n",
       " 849: 'Central',\n",
       " 850: 'CLINTON',\n",
       " 851: 'IA',\n",
       " 852: 'February',\n",
       " 853: 'INSURED',\n",
       " 854: 'LOSS',\n",
       " 855: 'FAX',\n",
       " 856: 'OFFICE',\n",
       " 857: 'HOURS',\n",
       " 858: 'VYPHAPHONE',\n",
       " 859: 'INTHALANGSY',\n",
       " 860: 'EXT',\n",
       " 861: \"'re\",\n",
       " 862: 'Jacksonville',\n",
       " 863: 'DALLAS',\n",
       " 864: 'TX',\n",
       " 865: 'Some',\n",
       " 866: 'specifics',\n",
       " 867: 'request',\n",
       " 868: 'sincerely',\n",
       " 869: 'CAITLEN',\n",
       " 870: 'CARROLL',\n",
       " 871: 'Ext',\n",
       " 872: 'date',\n",
       " 873: 'name',\n",
       " 874: 'Cellular',\n",
       " 875: 'MedSupport',\n",
       " 876: 'policy',\n",
       " 877: 'policies',\n",
       " 878: 'Female',\n",
       " 879: 'Domestic',\n",
       " 880: 'Partner',\n",
       " 881: 'information',\n",
       " 882: 'Condition',\n",
       " 883: 'am',\n",
       " 884: 'pm',\n",
       " 885: 'Confinement',\n",
       " 886: 'Dates',\n",
       " 887: 'Teri',\n",
       " 888: 'Willochell',\n",
       " 889: 'phycian',\n",
       " 890: 'considerations',\n",
       " 891: 'each',\n",
       " 892: 'such',\n",
       " 893: 'violation',\n",
       " 894: 'number',\n",
       " 895: 'indicated',\n",
       " 896: 'above',\n",
       " 897: 'My',\n",
       " 898: 'Member',\n",
       " 899: 'Relationship',\n",
       " 900: 'person',\n",
       " 901: 'granting',\n",
       " 902: 'authority',\n",
       " 903: 'birth',\n",
       " 904: 'Including',\n",
       " 905: 'L',\n",
       " 906: 'lesser',\n",
       " 907: 'If',\n",
       " 908: 'yes',\n",
       " 909: 'please',\n",
       " 910: 'provide',\n",
       " 911: 'tha',\n",
       " 912: 'diagnosis',\n",
       " 913: 'Treatment',\n",
       " 914: 'advise',\n",
       " 915: 'stop',\n",
       " 916: 'working',\n",
       " 917: 'what',\n",
       " 918: 'Willochel',\n",
       " 919: 'Specially',\n",
       " 920: 'internal',\n",
       " 921: 'medicine',\n",
       " 922: 'MD',\n",
       " 923: 'MedExpress',\n",
       " 924: 'Urgent',\n",
       " 925: 'Care',\n",
       " 926: 'Route',\n",
       " 927: 'HIPPA',\n",
       " 928: 'Socia',\n",
       " 929: 'Location',\n",
       " 930: 'Norwin',\n",
       " 931: 'Huntingdon',\n",
       " 932: 'Holder',\n",
       " 933: 'Sex',\n",
       " 934: 'COMP',\n",
       " 935: 'Clinical',\n",
       " 936: 'Report',\n",
       " 937: 'BP',\n",
       " 938: 'mmHg',\n",
       " 939: 'PULSE',\n",
       " 940: 'bpm',\n",
       " 941: 'RESP',\n",
       " 942: 'TEMP',\n",
       " 943: 'WEIGTH',\n",
       " 944: 'ft',\n",
       " 945: 'BMI',\n",
       " 946: 'LMP',\n",
       " 947: 'PMP',\n",
       " 948: 'SAT',\n",
       " 949: 'CONTUSION',\n",
       " 950: 'Meds',\n",
       " 951: 'ACTIVE',\n",
       " 952: 'acetaminophen',\n",
       " 953: 'albuterol',\n",
       " 954: 'bulk',\n",
       " 955: 'Dilantin',\n",
       " 956: 'gabapentin',\n",
       " 957: 'Humalog',\n",
       " 958: 'Lamictal',\n",
       " 959: 'Lyrica',\n",
       " 960: 'Neurontin',\n",
       " 961: 'valacyclovir',\n",
       " 962: 'Procedures',\n",
       " 963: 'FOOT',\n",
       " 964: 'MIN',\n",
       " 965: 'THREE',\n",
       " 966: 'ESTAB',\n",
       " 967: 'URGENT',\n",
       " 968: 'CARE',\n",
       " 969: 'Pocket',\n",
       " 970: 'Met',\n",
       " 971: 'Estimated',\n",
       " 972: 'charges',\n",
       " 973: 'MSO',\n",
       " 974: 'LLC',\n",
       " 975: 'Med',\n",
       " 976: 'Express',\n",
       " 977: 'UC',\n",
       " 978: 'HUNINGTON',\n",
       " 979: 'Merchant',\n",
       " 980: 'Transaction',\n",
       " 981: 'PURCHASE',\n",
       " 982: 'Approval',\n",
       " 983: 'code',\n",
       " 984: 'Record',\n",
       " 985: 'Visa',\n",
       " 986: 'Trace',\n",
       " 987: 'reference',\n",
       " 988: 'Cardholder',\n",
       " 989: 'identifier',\n",
       " 990: 'Application',\n",
       " 991: 'label',\n",
       " 992: 'TVR',\n",
       " 993: 'AID',\n",
       " 994: 'Subtotal',\n",
       " 995: 'Sales',\n",
       " 996: 'customer',\n",
       " 997: 'copy',\n",
       " 998: 'send',\n",
       " 999: 'payments',\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize hierarichal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load char encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_char_model = load_model(encoder_char_model_file.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             720985      lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           model_2[1][1]                    \n",
      "                                                                 model_2[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           model_2[2][1]                    \n",
      "                                                                 model_2[2][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1024)         0           model_2[3][1]                    \n",
      "                                                                 model_2[3][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1024)         0           model_2[4][1]                    \n",
      "                                                                 model_2[4][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1024)         0           model_2[5][1]                    \n",
      "                                                                 model_2[5][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           model_2[6][1]                    \n",
      "                                                                 model_2[6][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1024)         0           model_2[7][1]                    \n",
      "                                                                 model_2[7][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1024)         0           model_2[8][1]                    \n",
      "                                                                 model_2[8][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1024)         0           model_2[9][1]                    \n",
      "                                                                 model_2[9][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1024)         0           model_2[10][1]                   \n",
      "                                                                 model_2[10][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1024)         0           model_2[11][1]                   \n",
      "                                                                 model_2[11][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1024)         0           model_2[12][1]                   \n",
      "                                                                 model_2[12][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1024)         0           model_2[13][1]                   \n",
      "                                                                 model_2[13][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1024)         0           model_2[14][1]                   \n",
      "                                                                 model_2[14][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1024)         0           model_2[15][1]                   \n",
      "                                                                 model_2[15][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1024)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1024)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1024)      0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1024)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1024)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1024)      0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 1024)      0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 1024)      0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 1024)      0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 1024)      0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1, 1024)      0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 1024)      0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 1024)      0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 1024)      0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 1, 1024)      0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 15, 1024)     0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 2623488     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1031168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 concatenate_20[0][0]             \n",
      "                                                                 concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[0][0]                     \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[0][0]               \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 15, 1024)     0           dot_4[0][0]                      \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1007)     1032175     concatenate_22[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,555,592\n",
      "Trainable params: 8,547,311\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_hier_model(encoder_char_model=encoder_char_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 15, 20)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 20)           0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             720985      lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1024)         0           model_2[1][1]                    \n",
      "                                                                 model_2[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           model_2[2][1]                    \n",
      "                                                                 model_2[2][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1024)         0           model_2[3][1]                    \n",
      "                                                                 model_2[3][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1024)         0           model_2[4][1]                    \n",
      "                                                                 model_2[4][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1024)         0           model_2[5][1]                    \n",
      "                                                                 model_2[5][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           model_2[6][1]                    \n",
      "                                                                 model_2[6][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1024)         0           model_2[7][1]                    \n",
      "                                                                 model_2[7][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1024)         0           model_2[8][1]                    \n",
      "                                                                 model_2[8][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 1024)         0           model_2[9][1]                    \n",
      "                                                                 model_2[9][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1024)         0           model_2[10][1]                   \n",
      "                                                                 model_2[10][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 1024)         0           model_2[11][1]                   \n",
      "                                                                 model_2[11][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 1024)         0           model_2[12][1]                   \n",
      "                                                                 model_2[12][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1024)         0           model_2[13][1]                   \n",
      "                                                                 model_2[13][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 1024)         0           model_2[14][1]                   \n",
      "                                                                 model_2[14][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 1024)         0           model_2[15][1]                   \n",
      "                                                                 model_2[15][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1024)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1024)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1024)      0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1024)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1024)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1024)      0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 1024)      0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 1024)      0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1, 1024)      0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1, 1024)      0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1, 1024)      0           concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 1024)      0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 1024)      0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 1024)      0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 1, 1024)      0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 15, 1024)     0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_9[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 15, 512), (N 2623488     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 512)          0           bidirectional_2[0][1]            \n",
      "                                                                 bidirectional_2[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 512)          0           bidirectional_2[0][2]            \n",
      "                                                                 bidirectional_2[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 3,344,473\n",
      "Trainable params: 3,336,192\n",
      "Non-trainable params: 8,281\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 15, 1024)     1031168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 15, 512), (N 3147776     embedding_3[0][0]                \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 15, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 15, 15)       0           lstm_4[1][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15)       0           dot_3[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 15, 512)      0           activation_1[1][0]               \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 15, 1024)     0           dot_4[1][0]                      \n",
      "                                                                 lstm_4[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15, 1007)     1032175     concatenate_22[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 5,211,119\n",
      "Trainable params: 5,211,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Hierarichal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 51s 16ms/step - loss: 2.8400 - categorical_accuracy: 0.3765 - val_loss: 2.6050 - val_categorical_accuracy: 0.4613\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.46128, saving model to best_hier_model-15-20.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_20/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_21/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.9666 - categorical_accuracy: 0.6493 - val_loss: 1.8155 - val_categorical_accuracy: 0.5268\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.46128 to 0.52679, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.6091 - categorical_accuracy: 0.7135 - val_loss: 1.5928 - val_categorical_accuracy: 0.5761\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.52679 to 0.57606, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.4505 - categorical_accuracy: 0.7409 - val_loss: 1.4528 - val_categorical_accuracy: 0.5843\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.57606 to 0.58428, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.3601 - categorical_accuracy: 0.7491 - val_loss: 1.3495 - val_categorical_accuracy: 0.6066\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.58428 to 0.60665, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.2856 - categorical_accuracy: 0.7756 - val_loss: 1.2785 - val_categorical_accuracy: 0.6114\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.60665 to 0.61142, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.2454 - categorical_accuracy: 0.7668 - val_loss: 1.1758 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.61142 to 0.63004, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.2150 - categorical_accuracy: 0.7727 - val_loss: 1.1811 - val_categorical_accuracy: 0.6396\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.63004 to 0.63959, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.1927 - categorical_accuracy: 0.7782 - val_loss: 1.1125 - val_categorical_accuracy: 0.6445\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.63959 to 0.64450, saving model to best_hier_model-15-20.hdf5\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 33s 10ms/step - loss: 0.1966 - categorical_accuracy: 0.7746 - val_loss: 1.0982 - val_categorical_accuracy: 0.6590\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.64450 to 0.65897, saving model to best_hier_model-15-20.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c47c83da0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10 \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_chars_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t Medical Provider Roles UNK Treating \\n UNK UNK UNK UNK UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_gt_sequence(decoder_word_input_data[idx:idx+1], int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mredicla', 'Provide', 'rRole', ':', 'T', 'raetnqg']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(input_texts[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tMedical Provider Roles: Treating\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t', 'Medical', 'Provider', 'Roles', ':', 'Treating', '\\n']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lst =  word_tokenize(target_texts[idx])\n",
    "words_lst.insert(0, '\\t')\n",
    "words_lst.append('\\n')\n",
    "words_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Medical Provider Roles UNK Treating \\n UNK UNK UNK UNK UNK UNK UNK UNK UNK '"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_gt_sequence(np.argmax(decoder_word_target_data[idx:idx+1], axis=-1), int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Medical Provider Roles UNK UNK \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict([encoder_char_input_data[idx:idx+1], decoder_word_input_data[idx-1:idx]])\n",
    "y.shape\n",
    "#sampled_token_index = np.argmax(y[0, -1, :])\n",
    "d = np.argmax(y, axis=-1)\n",
    "d.shape\n",
    "decode_gt_sequence(d, int2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Provider Roles UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "input_seq = encoder_char_input_data[idx:idx+1]\n",
    "decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Claim Tmype: VeB ccidemnt - Accideynal Injur\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Decoded sentence: Claim Type UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Policyhloer/Owner Information\n",
      "GT sentence: Policyholder/Owner Information\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Fisr tName:\n",
      "GT sentence: First Name:\n",
      "Decoded sentence: First Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Midle Name/Iniitl:\n",
      "GT sentence: Middle Name/Initial:\n",
      "Decoded sentence: Middle UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: aLst Name:\n",
      "GT sentence: Last Name:\n",
      "Decoded sentence: Last Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Social Security Number:\n",
      "GT sentence: Social Security Number:\n",
      "Decoded sentence: Social Security Number UNK  \n",
      "-\n",
      "Input sentence: Birth Date\n",
      "GT sentence: Birth Date:\n",
      "Decoded sentence: Birth Date UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "Decoded sentence: Gender UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: LanguaePrefmerence:\n",
      "GT sentence: Language Preference:\n",
      "Decoded sentence: Language Preference UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Addnress Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: City\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Satt/Province:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: cCountryd:\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Best hone Numbie to  ReachedDruingthe Day:\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "Decoded sentence: Best Phone Number to be Reached During the Day UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: EmailA drebss:\n",
      "GT sentence: Email Address:\n",
      "Decoded sentence: Email Address UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Page 1 of \n",
      "GT sentence: Page 1 of 1\n",
      "Decoded sentence: Page UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: RADOLOGY\n",
      "GT sentence: RADIOLOGY\n",
      "Decoded sentence: RADIOLOGY  \n",
      "-\n",
      "Input sentence: REPORT\n",
      "GT sentence: REPORT\n",
      "Decoded sentence: RADIOLOGY  \n",
      "-\n",
      "Input sentence: www.rays.net\n",
      "GT sentence: www.rays.net\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: atient MRN Accession No. Ref. Physician\n",
      "GT sentence: Patient MRN Accession No. Ref. Physician\n",
      "Decoded sentence: Patient MRN UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: UNKNOWN\n",
      "GT sentence: UNKNOWN\n",
      "Decoded sentence: Gender UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Study \n",
      "GT sentence: Study\n",
      "Decoded sentence: Peds  \n",
      "-\n",
      "Input sentence: Study Date:\n",
      "GT sentence: Study Date:\n",
      "Decoded sentence: Signed Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Hospital Code: 206 \n",
      "GT sentence: Hospital Code: 2026\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: DoOB\n",
      "\n",
      "GT sentence: DOB:\n",
      "Decoded sentence: DOB UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 43F.\n",
      "GT sentence: 43F.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Technzique: 3 mviews pleft wrisjt\n",
      "GT sentence: Technique: 3 views left wrist\n",
      "Decoded sentence: Technique UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Cormarison: None availabie\n",
      "GT sentence: Cormarison: None availabie\n",
      "Decoded sentence: Comparison UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Coparison: None availabl\n",
      "GT sentence: Comparison: None available\n",
      "Decoded sentence: Comparison UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: FINDINGS:\n",
      "GT sentence: FINDINGS:\n",
      "Decoded sentence: FINDINGS UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: IMRPESSImON:\n",
      "GT sentence: IMPRESSION:\n",
      "Decoded sentence: Employer UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: 2. No acute osseous adbnormmalityp idenitfied.\n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Daytime Phonex:\n",
      "GT sentence: Daytime Phone:\n",
      "Decoded sentence: Daytime Phone UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Clpaim vent Informatio\n",
      "GT sentence: Claim Event Information\n",
      "Decoded sentence: Claim Event Information  \n",
      "-\n",
      "Input sentence: Sopped Working?: eYs\n",
      "GT sentence: Stopped Working?: Yes\n",
      "Decoded sentence: Stopped Working UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date Last Physically at Work:\n",
      "GT sentence: Date Last Physically at Work:\n",
      "Decoded sentence: Date Last Physically at Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Hwours oWred no Last Daya: 8\n",
      "GT sentence: Hours Worked on Last Day: 8\n",
      "Decoded sentence: Hours Worked on Last Day UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: aHnours Scehduled toh Work on Lst Da:y\n",
      "GT sentence: Hours Scheduled to Work on Last Day:\n",
      "Decoded sentence: Hours Scheduled to Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date Fdirst Missed Work\n",
      "GT sentence: Date First Missed Work:\n",
      "Decoded sentence: Date First Visit  \n",
      "-\n",
      "Input sentence: Returnedto Wrok?: No\n",
      "GT sentence: Returned to Work?: No\n",
      "Decoded sentence: Returned to Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Accidentu Work Related: Yes\n",
      "GT sentence: Accident Work Related: Yes\n",
      "Decoded sentence: Accident Work UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Time of Accident:\n",
      "GT sentence: Time of Accident:\n",
      "Decoded sentence: Time of Accident UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Accident ate:\n",
      "GT sentence: Accident Date:\n",
      "Decoded sentence: Accident Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Diaagnosis Cdoe: Arvthiscopic srugery left wrist.\n",
      "GT sentence: Diagnosis Code: Arthiscopic surgery left wrist.\n",
      "Decoded sentence: Diagnosis Code UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Suregry Informaitn\n",
      "GT sentence: Surgery Information\n",
      "Decoded sentence: Surgery Information  \n",
      "-\n",
      "Input sentence: IsSugrer yRequiresd: mYeks\n",
      "GT sentence: Is Surgery Required: Yes\n",
      "Decoded sentence: Preferred Language UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Sdurgery Dnate:\n",
      "GT sentence: Surgery Date:\n",
      "Decoded sentence: Signed Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Inpatient/Outpatient Indicator: Outptaient\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Medica lProvider Ifnoration - Physiician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Medical Provider Information UNK  \n",
      "-\n",
      "Input sentence: Mredicla Provide rRole:T raetnqg\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Decoded sentence: Medical Provider Roles UNK UNK  \n",
      "-\n",
      "Input sentence: Provider First Name: Patrikc\n",
      "GT sentence: Provider First Name: Patrick\n",
      "Decoded sentence: Provider First Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: roviedr Last Name: Emerson\n",
      "GT sentence: Provider Last Name: Emerson\n",
      "Decoded sentence: Provider Last Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Address Line 1 :\n",
      "GT sentence: Address Line 1 :\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: City:\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: SttaeP/rovince:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Posal Code\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Counbtry:b\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Business Telteiphone:\n",
      "GT sentence: Business Telephone:\n",
      "Decoded sentence: Business Telephone UNK UNK UNK  \n",
      "-\n",
      "Input sentence: BusyineF ax\n",
      "GT sentence: Business Fax\n",
      "Decoded sentence: Business Fax  \n",
      "-\n",
      "Input sentence: Date of First Visi:\n",
      "GT sentence: Date of First Visit:\n",
      "Decoded sentence: Date of First Visit UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Next Visit:\n",
      "GT sentence: Date of Next Visit:\n",
      "Decoded sentence: Date of Accident UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Medica Provdier Informavtion - Hosiptalization\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Decoded sentence: Medical Provider Information UNK  \n",
      "-\n",
      "Input sentence: Hospital jNae:\n",
      "GT sentence: Hospital Name:\n",
      "Decoded sentence: Hospital Name  \n",
      "-\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Address Line UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: iCt:y\n",
      "GT sentence: City:\n",
      "Decoded sentence: City UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Sate/Poivnce:\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: PxostalC ode\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Postal Code UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: oCuntry\n",
      "\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Country UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Visit/Admission:\n",
      "GT sentence: Date of Visit/Admission:\n",
      "Decoded sentence: Date of Accident UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Date of Dischge:\n",
      "GT sentence: Date of Discharge:\n",
      "Decoded sentence: Date of Accident UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Procedure: Left wrt arthiscopic surgery\n",
      "GT sentence: Procedure: Left wrist arthiscopic surgery\n",
      "Decoded sentence: Ethnicity UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: EmployvmentInfrmation\n",
      "GT sentence: Employment Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Employer aNme:\n",
      "GT sentence: Employer Name:\n",
      "Decoded sentence: Employer Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Policy umber:\n",
      "GT sentence: Policy Number:\n",
      "Decoded sentence: Policy Number UNK UNK  \n",
      "-\n",
      "Input sentence: Elctronic Submisskion\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Electronic Submission  \n",
      "-\n",
      "Input sentence: Claitm fEkven Iddenjtifir:\n",
      "GT sentence: Claim Event Identifier:\n",
      "Decoded sentence: Claim Event Identifier  \n",
      "-\n",
      "Input sentence: Submission Date:\n",
      "GT sentence: Submission Date:\n",
      "Decoded sentence: Submission Date UNK UNK  \n",
      "-\n",
      "Input sentence: Electrocially iSgnede Indicator Yes\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "Decoded sentence: Electronically Signed Indicator UNK  \n",
      "-\n",
      "Input sentence: FraudStatements Reuviewed and Electronically\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Fraud Statements Reviewed and Electronically  \n",
      "-\n",
      "Input sentence: Signed Date:\n",
      "GT sentence: Signed Date:\n",
      "Decoded sentence: Signed Date UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: unum\n",
      "GT sentence: unum\n",
      "Decoded sentence: unum  \n",
      "-\n",
      "Input sentence: The Beenfits Centr\n",
      "GT sentence: The Benefits Center\n",
      "Decoded sentence: The Benefits Center  \n",
      "-\n",
      "Input sentence: (Not for mFMLA Requests)\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Electroncally Signed 02/282018n\n",
      "GT sentence: Electronically Signed 02/28/2018\n",
      "Decoded sentence: Qualified interpreters  \n",
      "-\n",
      "Input sentence: Insured’ Signature Dte Siged\n",
      "GT sentence: Insured’s Signature Date Signed\n",
      "Decoded sentence: Insured ’ s Signature Date  \n",
      "-\n",
      "Input sentence: Printed NmameSocialj Secuurity Number\n",
      "GT sentence: Printed Name Social Security Number\n",
      "Decoded sentence: MedExpress granting Care  \n",
      "-\n",
      "Input sentence: CLo-1116 (111/4)\n",
      "GT sentence: CL-1116 (11/14)\n",
      "Decoded sentence: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Unm\n",
      "GT sentence: Unum\n",
      "Decoded sentence: Unum  \n",
      "-\n",
      "Input sentence: Confirmation of Cvoergae\n",
      "GT sentence: Confirmation of Coverage\n",
      "Decoded sentence: Confirmation of Coverage  \n",
      "-\n",
      "Input sentence: nEmployer\n",
      "\n",
      "GT sentence: Employer:\n",
      "Decoded sentence: Employer UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Group Polic y#:\n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: Group Policy UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Customer  Poicy #:\n",
      "GT sentence: Customer  Policy #:\n",
      "Decoded sentence: Customer Policy UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: E Name:\n",
      "GT sentence: EE Name:\n",
      "Decoded sentence: EE Name UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: sured tCosrage Type Covrage Effectiv eDate\n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective Date  \n",
      "-\n",
      "Input sentence: mployee Off-Job Acc Januarny1 , r2017\n",
      "GT sentence: Employee Off-Job Acc January 1, 2017\n",
      "Decoded sentence: Employee UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Employee Wellness hBenefni tJanuary , 2017x\n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "Decoded sentence: Employee UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Total Monhly Premuim:\n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Total Monthly Premium UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Total Employee Montly Payroll Deduction:\n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "Decoded sentence: Total Employee Weekly Payroll Deduction UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK  \n",
      "-\n",
      "Input sentence: Nae:\n",
      "GT sentence: Name:\n",
      "Decoded sentence: Name UNK UNK UNK UNK UNK UNK UNK  \n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: text\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Policyholder/Owner Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: First Name:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Middle Name/Initial:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Last Name:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Social Security Number:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Birth Date:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Gender:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Language Preference:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Fai\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 10\n",
      "GT sentence: City:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 7521509\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: (FISTDEOO)\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: at\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 11/3/2017\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 5:23:19\n",
      "GT sentence: Email Address:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: from\n",
      "GT sentence: Page 1 of 1\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: -9373834004\n",
      "GT sentence: RADIOLOGY\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Req\n",
      "GT sentence: REPORT\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: IC\n",
      "GT sentence: www.rays.net\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 2017:1030525109:292E.\n",
      "GT sentence: Patient MRN Accession No. Ref. Physician\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Page\n",
      "GT sentence: UNKNOWN\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 4\n",
      "GT sentence: Study\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: of\n",
      "GT sentence: Study Date:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 5\n",
      "GT sentence: Hospital Code: 2026\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: (C)\n",
      "GT sentence: DOB:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: 43F.\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Technique: 3 views left wrist\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Cormarison: None availabie\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Comparison: None available\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: FINDINGS:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: IMPRESSION:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Daytime Phone:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Claim Event Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Stopped Working?: Yes\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Date Last Physically at Work:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 11/3/2017\n",
      "GT sentence: Hours Worked on Last Day: 8\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: FRI\n",
      "GT sentence: Hours Scheduled to Work on Last Day:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 8:26\n",
      "GT sentence: Date First Missed Work:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: FAX\n",
      "GT sentence: Returned to Work?: No\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 2373834004\n",
      "GT sentence: Accident Work Related: Yes\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Kjooas00s\n",
      "GT sentence: Time of Accident:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Accident Date:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Diagnosis Code: Arthiscopic surgery left wrist.\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Surgery Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: as3-ursasy3\n",
      "GT sentence: Is Surgery Required: Yes\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 11:30:11\n",
      "GT sentence: Surgery Date:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 11/2/2017\n",
      "GT sentence: Inpatient/Outpatient Indicator: Outpatient\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: vis\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Provider First Name: Patrick\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Provider Last Name: Emerson\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: ®\n",
      "GT sentence: Address Line 1 :\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: ®\n",
      "GT sentence: City:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: &\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: ACCIDENT\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: CLAIM\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: FORM\n",
      "GT sentence: Business Telephone:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Business Fax\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: uu\n",
      "GT sentence: Date of First Visit:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: num’\n",
      "GT sentence: Date of Next Visit:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Tha\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Benelits\n",
      "GT sentence: Hospital Name:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Canter\n",
      "GT sentence: Address Line 1:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: City:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: P.O.\n",
      "GT sentence: State/Province:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Bax\n",
      "GT sentence: Postal Code:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 100158,\n",
      "GT sentence: Country:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Calumbin,\n",
      "GT sentence: Date of Visit/Admission:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: EC\n",
      "GT sentence: Date of Discharge:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 20202-3150\n",
      "GT sentence: Procedure: Left wrist arthiscopic surgery\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employment Information\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Tol-frea:\n",
      "GT sentence: Employer Name:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 1-800-635-5587\n",
      "GT sentence: Policy Number:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Fax:\n",
      "GT sentence: Electronic Submission\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 1-800-447-2488\n",
      "GT sentence: Claim Event Identifier:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Submission Date:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Gall\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: toll-free\n",
      "GT sentence: Fraud Statements Reviewed and Electronically\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Monday\n",
      "GT sentence: Signed Date:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: through\n",
      "GT sentence: unum\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Friday,\n",
      "GT sentence: The Benefits Center\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 8\n",
      "GT sentence: (Not for FMLA Requests)\n",
      "Decoded sentence: Employment Information  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: a.m.\n",
      "GT sentence: Electronically Signed 02/28/2018\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: lo\n",
      "GT sentence: Insured’s Signature Date Signed\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: 8\n",
      "GT sentence: Printed Name Social Security Number\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: p.m,\n",
      "GT sentence: CL-1116 (11/14)\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Eagtarn\n",
      "GT sentence: Unum\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: Time.\n",
      "GT sentence: Confirmation of Coverage\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employer:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Group Policy #:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Customer  Policy #:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: EE Name:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employee Off-Job Acc January 1, 2017\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence:  \n",
      "GT sentence: Total Monthly Premium:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "Decoded sentence: Employment Information  \n",
      "-\n",
      "Input sentence: \n",
      "GT sentence: Name:\n",
      "Decoded sentence: Employment Information  \n"
     ]
    }
   ],
   "source": [
    "input_texts = ['text',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Fai',\n",
    "'10',\n",
    "'7521509',\n",
    "'(FISTDEOO)',\n",
    "'at',\n",
    "'11/3/2017',\n",
    "'5:23:19',\n",
    "'from',\n",
    "'-9373834004',\n",
    "'Req',\n",
    "'IC',\n",
    "'2017:1030525109:292E.',\n",
    "'Page',\n",
    "'4',\n",
    "'of',\n",
    "'5',\n",
    "'(C)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'11/3/2017',\n",
    "'FRI',\n",
    "'8:26',\n",
    "'FAX',\n",
    "'2373834004',\n",
    "'Kjooas00s',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'as3-ursasy3',\n",
    "'11:30:11',\n",
    "'11/2/2017',\n",
    "'vis',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'®',\n",
    "'®',\n",
    "'&',\n",
    "'ACCIDENT',\n",
    "'CLAIM',\n",
    "'FORM',\n",
    "'',\n",
    "'uu',\n",
    "'num’',\n",
    "'Tha',\n",
    "'Benelits',\n",
    "'Canter',\n",
    "'',\n",
    "'P.O.',\n",
    "'Bax',\n",
    "'100158,',\n",
    "'Calumbin,',\n",
    "'EC',\n",
    "'20202-3150',\n",
    "'',\n",
    "'Tol-frea:',\n",
    "'1-800-635-5587',\n",
    "'Fax:',\n",
    "'1-800-447-2488',\n",
    "'',\n",
    "'Gall',\n",
    "'toll-free',\n",
    "'Monday',\n",
    "'through',\n",
    "'Friday,',\n",
    "'8',\n",
    "'a.m.',\n",
    "'lo',\n",
    "'8',\n",
    "'p.m,',\n",
    "'Eagtarn',\n",
    "'Time.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[',\n",
    "'ATTENDING',\n",
    "'PHYSICIAN',\n",
    "'STATEMENT',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'IneurexiPolicyt',\n",
    "'alcar',\n",
    "'Hama',\n",
    "'(Lael',\n",
    "'Name,',\n",
    "'Flis!',\n",
    "'Nama,',\n",
    "'MI,',\n",
    "'Suffix)',\n",
    "'Data',\n",
    "'of',\n",
    "'Risth',\n",
    "'{msmidrfyy)',\n",
    "'-',\n",
    "'',\n",
    "'',\n",
    "'Faupi',\n",
    "'Nana',\n",
    "'{Laut',\n",
    "'Hume,',\n",
    "'Flial',\n",
    "'Numa,',\n",
    "'1',\n",
    "'Sut)',\n",
    "'Dats',\n",
    "'al',\n",
    "'Bln',\n",
    "'rAvad)',\n",
    "'Ul',\n",
    "'_',\n",
    "'',\n",
    "'-[ECIpENT',\n",
    "'DETAILS',\n",
    "']',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'a',\n",
    "'thls',\n",
    "'Gundilan',\n",
    "'the',\n",
    "'result',\n",
    "'of',\n",
    "'a',\n",
    "'acddental',\n",
    "'inury?',\n",
    "'ves',\n",
    "'O',\n",
    "'No',\n",
    "'if',\n",
    "'yas,',\n",
    "'dale',\n",
    "'of',\n",
    "'accident',\n",
    "'qre/ddlyy)',\n",
    "'[1',\n",
    "'0]',\n",
    "'[z]e',\n",
    "'[=]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Is',\n",
    "'Mig',\n",
    "'condition',\n",
    "'Lhe',\n",
    "'result',\n",
    "'of',\n",
    "'hefer',\n",
    "'employment',\n",
    "'£1',\n",
    "'Yes',\n",
    "'pNo',\n",
    "'[1',\n",
    "'Unknown',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Plaaze',\n",
    "'verily',\n",
    "'treatment',\n",
    "'for',\n",
    "'the',\n",
    "'accident',\n",
    "'lalad',\n",
    "'above.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Dalaw',\n",
    "'of',\n",
    "'Diagnosis',\n",
    "'Diagncsis',\n",
    "'Description',\n",
    "'Prosadure',\n",
    "'Procedure',\n",
    "'Dascription',\n",
    "'',\n",
    "'Branden',\n",
    "'(Including',\n",
    "'|',\n",
    "'Cudo',\n",
    "'(GD)',\n",
    "'ous',\n",
    "'',\n",
    "'Confinement)',\n",
    "'eR',\n",
    "'ap',\n",
    "'HAS',\n",
    "'TTT',\n",
    "'',\n",
    "'BEEF',\n",
    "'eR',\n",
    "'',\n",
    "'wiz]',\n",
    "'.',\n",
    "'S33,5XxA',\n",
    "'Hh',\n",
    "'rioes',\n",
    "'ey',\n",
    "'race',\n",
    "'Word',\n",
    "'',\n",
    "'awqd]',\n",
    "'',\n",
    "'weak',\n",
    "'3',\n",
    "'n',\n",
    "'[aveny',\n",
    "'[d',\n",
    "'',\n",
    "'wifi',\n",
    "'Wl',\n",
    "'',\n",
    "'oa',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Has',\n",
    "'lhe',\n",
    "'pallet',\n",
    "'bean',\n",
    "'trastad',\n",
    "'for',\n",
    "'tha',\n",
    "'same',\n",
    "'ar',\n",
    "'&',\n",
    "'S(tilar',\n",
    "'candillan',\n",
    "'by',\n",
    "'anolher',\n",
    "'phyalelan',\n",
    "'In',\n",
    "'tha',\n",
    "'past?',\n",
    "'[1',\n",
    "'Yen',\n",
    "'Bho',\n",
    "'',\n",
    "'M',\n",
    "'yor,',\n",
    "'pioona',\n",
    "'provid',\n",
    "'tha',\n",
    "'fares:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Diageosis:',\n",
    "'Tramiment',\n",
    "'Daten:',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'id',\n",
    "'ya.1',\n",
    "'#dving',\n",
    "'Lhe',\n",
    "'patient',\n",
    "'to',\n",
    "'clap',\n",
    "'working?',\n",
    "'RECEIVED',\n",
    "'',\n",
    "'It',\n",
    "'yes,',\n",
    "'B8',\n",
    "'of',\n",
    "'what',\n",
    "'cate?',\n",
    "'(mmidkyy)',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[23]',\n",
    "'[117]',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'[Ih',\n",
    "'cielih',\n",
    "'fa',\n",
    "'rotated',\n",
    "'to',\n",
    "'normal',\n",
    "'prepnency,',\n",
    "'please',\n",
    "'grovida',\n",
    "'tha',\n",
    "'idliawing:',\n",
    "'NOV',\n",
    "'',\n",
    "'Expecigd',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'(mimicd/yy)',\n",
    "'Aclual',\n",
    "'Delivery',\n",
    "'Dale',\n",
    "'{mmiddlyy',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Phyeiclan',\n",
    "'informaiton',\n",
    "'HUMAN',\n",
    "'REGOURCITE',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'FRAUD',\n",
    "'NOTICE:',\n",
    "'Any',\n",
    "'person',\n",
    "'wha',\n",
    "'knowingly',\n",
    "'files',\n",
    "'&',\n",
    "'statement',\n",
    "'of',\n",
    "'clalm',\n",
    "'containing',\n",
    "'FALSE',\n",
    "'or',\n",
    "'misleading',\n",
    "'information',\n",
    "'8',\n",
    "'',\n",
    "'subject',\n",
    "'to',\n",
    "'criminal',\n",
    "'and',\n",
    "'elvil',\n",
    "'penallies.',\n",
    "'This',\n",
    "'includes',\n",
    "'Attending',\n",
    "'Physician',\n",
    "'portions',\n",
    "'of',\n",
    "'the',\n",
    "'claim',\n",
    "'farm.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'CS',\n",
    "'yma',\n",
    "'SEAS',\n",
    "'Ta',\n",
    "'hve',\n",
    "'glan',\n",
    "'=',\n",
    "'',\n",
    "'The',\n",
    "'above',\n",
    "'statements',\n",
    "'ara',\n",
    "'trun',\n",
    "'And',\n",
    "'rompints',\n",
    "'to',\n",
    "'tho',\n",
    "'bot',\n",
    "'of',\n",
    "'my',\n",
    "'knowledge',\n",
    "'and',\n",
    "'bolluf.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physician',\n",
    "'Name',\n",
    "'(Lea!',\n",
    "'Name,',\n",
    "'Firat',\n",
    "'Name,',\n",
    "'MI,',\n",
    "'Suita)',\n",
    "'Plases',\n",
    "'Print',\n",
    "'Co',\n",
    "'FHman',\n",
    "'log',\n",
    "'Mm',\n",
    "'',\n",
    "'/',\n",
    "'‘',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Medical',\n",
    "'Speclaty',\n",
    "'[Tr',\n",
    "'eactal-',\n",
    "']',\n",
    "'|',\n",
    "'D',\n",
    "'of',\n",
    "'r',\n",
    "'of',\n",
    "'Ch',\n",
    "'2',\n",
    "'',\n",
    "'2',\n",
    "'Le',\n",
    "'',\n",
    "'',\n",
    "'==',\n",
    "'Zoi!',\n",
    "'M',\n",
    "'o',\n",
    "'“Fanart',\n",
    "'',\n",
    "'',\n",
    "'=',\n",
    "'Balfrone',\n",
    "'ie',\n",
    "'2',\n",
    "'Sle',\n",
    "'iu',\n",
    "'',\n",
    "'il',\n",
    "'HY',\n",
    "'BY',\n",
    "'1942',\n",
    "'Fax',\n",
    "'Number',\n",
    "'yz—',\n",
    "'43',\n",
    "'-8',\n",
    "'7775',\n",
    "'Fhyalafans',\n",
    "'Tax',\n",
    "'ID',\n",
    "'Number.',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Aro',\n",
    "'you',\n",
    "'refateq',\n",
    "'to',\n",
    "'hiv',\n",
    "'pollen?',\n",
    "'0',\n",
    "'Yoe',\n",
    "'LlMo',\n",
    "'|',\n",
    "'yes,',\n",
    "'wal',\n",
    "'iv',\n",
    "'the',\n",
    "'relelianshipT',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'Physlclan',\n",
    "'Slgnature',\n",
    "'Date',\n",
    "'',\n",
    "'CL-1023',\n",
    "'-2717',\n",
    "'=',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "' ',\n",
    "'',\n",
    "'',\n",
    "'',\n",
    "'—',]\n",
    "\n",
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_hier_data(input_texts, \n",
    "                                                                                                [], \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
