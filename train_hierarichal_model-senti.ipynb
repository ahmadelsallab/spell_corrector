{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding, Lambda, Concatenate, Reshape\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            if (len(sents) < 2):\n",
    "                continue             \n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index].strip() + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                if (len(sents) < 2):\n",
    "                    continue                 \n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1].strip() + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:       \n",
    "        for word in word_tokenize(sentence):\n",
    "            word = process_word(word)\n",
    "            if word not in vocab_to_int:\n",
    "                vocab_to_int[word] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to word'''\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chars_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_char_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_words_data(input_texts, target_texts, max_words_seq_length, max_chars_seq_length, num_char_tokens, num_word_tokens, word2int, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    encoder_char_input_data = np.zeros(\n",
    "    (len(input_texts), max_words_seq_length, max_chars_seq_length),\n",
    "    dtype='float32')\n",
    "    \n",
    "    decoder_word_input_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length),\n",
    "        dtype='float32')\n",
    "    \n",
    "    decoder_word_target_data = np.zeros(\n",
    "        (len(input_texts), max_words_seq_length, num_word_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        words_lst = word_tokenize(input_text)\n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue\n",
    "        for j, word in enumerate(words_lst):\n",
    "            if(len(word) > max_chars_seq_length):\n",
    "                continue\n",
    "            for k, char in enumerate(word):\n",
    "                # c0..cn\n",
    "                if(char in char2int):\n",
    "                    encoder_char_input_data[i, j, k] = char2int[char]\n",
    "                    \n",
    "        words_lst = word_tokenize(target_text)# word_tokenize removes the \\t and \\n, we need them to start and end a sequence\n",
    "        words_lst.insert(0, '\\t')\n",
    "        words_lst.append('\\n')        \n",
    "        if(len(words_lst) > max_words_seq_length):\n",
    "            continue                \n",
    "        for j, word in enumerate(words_lst):\n",
    "            processed_word = process_word(word)\n",
    "            if not processed_word in word2int:\n",
    "                continue\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_word_input_data[i, j] = word2int[processed_word]\n",
    "            if j > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_word_target_data[i, j - 1, word2int[processed_word]] = 1.\n",
    "                \n",
    "    return encoder_char_input_data, decoder_word_input_data, decoder_word_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences_data(input_texts, target_labels, max_sents_per_doc, max_words_per_sent, max_chars_per_word, \n",
    "                             num_classes, char2int):\n",
    "\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    # \n",
    "    hier_input_data = np.zeros((len(input_texts), \n",
    "                                max_sents_per_doc, \n",
    "                                max_words_per_sent, \n",
    "                                max_chars_per_word), dtype='float32')\n",
    "    \n",
    "        \n",
    "    hier_target_data = np.zeros((len(input_texts), num_classes), dtype='float32')\n",
    "\n",
    "    # TODO: input_texts = data_train.review, target_labels = data_train.label\n",
    "    for i, (input_text, target_label) in enumerate(zip(input_texts, target_labels)):\n",
    "        sents_lst = sent_tokenize(clean_str(BeautifulSoup(input_text).get_text())) # TODO: Move to clean str\n",
    "        \n",
    "        '''\n",
    "        if len(sents_lst) > max_sents_per_doc:\n",
    "            continue\n",
    "        '''\n",
    "        for j, sent in enumerate(sents_lst):\n",
    "                \n",
    "            words_lst = word_tokenize(input_text)\n",
    "            '''\n",
    "            if(len(words_lst) > max_words_per_sent):\n",
    "                continue\n",
    "            '''\n",
    "            \n",
    "            for k, word in enumerate(words_lst):\n",
    "                \n",
    "                '''\n",
    "                if(len(word) > max_chars_per_word):\n",
    "                    continue\n",
    "                '''\n",
    "                for l, char in enumerate(word):\n",
    "                    # c0..cn\n",
    "                    if(char in char2int):\n",
    "                        hier_input_data[i, j, k, l] = char2int[char]\n",
    "                        hier_target_data[i, target_label] = 1\n",
    "\n",
    "                \n",
    "    return hier_input_data, hier_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_word = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_word + ' '\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_words_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_words_seq_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "       \n",
    "        \n",
    "        #orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(word_tokenize(decoded_sentence)) > max_words_seq_len):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        '''\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        '''\n",
    "        decoded_sentence += sampled_char + ' '\n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        #target_seq = np.zeros((1, max_words_seq_len))\n",
    "        if i < max_words_seq_len:\n",
    "            target_seq[0, i] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        i += 1\n",
    "        #if i > 48:\n",
    "        #    i = 0\n",
    "        \n",
    "\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_char_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    i = 0\n",
    "    special_chars = ['\\\\', '/', '-', '—' , ':', '[', ']', ',', '.', '\"', ';', '%', '~', '(', ')', '{', '}', '$']\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        orig_char = int_to_vocab[int(input_seq[:,i][0])]\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            sampled_char = ''\n",
    "\n",
    "        # Copy digits as it, since the spelling corrector is not good at digit corrections\n",
    "        if(orig_char.isdigit() or orig_char in special_chars):\n",
    "            decoded_sentence += orig_char            \n",
    "        else:\n",
    "            if(sampled_char.isdigit() or sampled_char in special_chars):\n",
    "                decoded_sentence += ''\n",
    "            else:\n",
    "                decoded_sentence += sampled_char\n",
    "        \n",
    "\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        i += 1\n",
    "        if(i > 48):\n",
    "            i = 0\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    #print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    #print(decoder_outputs)\n",
    "    #print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_encoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    #print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #print(encoder_inputs)\n",
    "    #print(encoder_outputs)\n",
    "    #print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "    #print(encoder_outputs.shape)\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_word_embedding_model = Model(input=encoder_inputs, output=encoder_embedding_output)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, encoder_word_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_model(encoder_word_embedding_model, max_words_seq_len, max_char_seq_len, num_word_tokens, num_char_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "\n",
    "    inputs = Input(shape=(max_words_seq_len, max_char_seq_len,), dtype='float32')\n",
    "    decoder_inputs_words = Input(shape=(max_words_seq_len,), dtype='float32')\n",
    "    words_states = []\n",
    "    '''\n",
    "    for w in range(max_words_seq_len):\n",
    "        \n",
    "        encoder_char_inputs = Lambda(lambda x: x[:,w,:])(inputs)\n",
    "        _, h, c = encoder_char_model(encoder_char_inputs)\n",
    "        encoder_chars_states = Concatenate()([h,c])\n",
    "        #print(encoder_chars_states)\n",
    "        encoder_chars_states = Reshape((1,latent_dim*4))(encoder_chars_states)\n",
    "        words_states.append(encoder_chars_states)\n",
    "    \n",
    "    input_words = Concatenate(axis=-2)(words_states)\n",
    "\n",
    "    '''\n",
    "    #input_words = TimeDistributed(Dense(10))(inputs)\n",
    "\n",
    "    input_words = TimeDistributed(encoder_word_embedding_model)(inputs)\n",
    "\n",
    "    encoder_inputs_ = input_words   \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    \n",
    "    decoder_inputs = decoder_inputs_words\n",
    "    decoder_inputs_ = Embedding(num_word_tokens, latent_dim*4,                           \n",
    "                            #weights=[np.eye(num_word_tokens)],\n",
    "                            mask_zero=True, trainable=True)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_word_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([inputs, decoder_inputs_words], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=inputs, output=[encoder_outputs] + encoder_states)\n",
    "    encoder_embedding_output = Lambda(lambda x: x[:,-1,:])(encoder_outputs)\n",
    "    encoder_sentence_embedding_model = Model(input=inputs, output=encoder_embedding_output)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(max_words_seq_len, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs_words, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model, encoder_sentence_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_words_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\—\\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\●\\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        #processed_word = word.lower()\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []\n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata_path = '../../dat/'\\nmax_sent_len = 50\\nmin_sent_len = 0\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data_path = '../../dat/'\n",
    "max_sent_len = 50\n",
    "min_sent_len = 0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 1000\\ninput_texts = []\\ntarget_texts = []\\nfiles_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\\ndesired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\\nnoise_threshold = 0.9\\n\\nfor file_name, num_file_samples in zip(files_list, desired_file_sizes):\\n    tess_correction_data = os.path.join(data_path, file_name)\\n    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\\n\\n    input_texts += input_texts_OCR\\n    target_texts += target_texts_OCR\\n\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 1000\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "files_list = ['all_ocr_data_2.txt', 'field_class_21.txt', 'field_class_32.txt', 'field_class_30.txt']\n",
    "desired_file_sizes = [num_samples, num_samples, num_samples, num_samples]\n",
    "noise_threshold = 0.9\n",
    "\n",
    "for file_name, num_file_samples in zip(files_list, desired_file_sizes):\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_noise(tess_correction_data, num_file_samples, noise_threshold, max_sent_len, min_sent_len)\n",
    "\n",
    "    input_texts += input_texts_OCR\n",
    "    target_texts += target_texts_OCR\n",
    "\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "data_file = 'imdb/labeledTrainData.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(os.path.join(data_path, data_file), sep='\\t')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor text in data_train.review:\\n    for sent in sent_tokenize(clean_str(BeautifulSoup(text).get_text())):\\n        print(sent + '\\n')\\n        for word in word_tokenize(sent):\\n            print(word + '\\n')\\n    print('****************\\n')\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for text in data_train.review:\n",
    "    for sent in sent_tokenize(clean_str(BeautifulSoup(text).get_text())):\n",
    "        print(sent + '\\n')\n",
    "        for word in word_tokenize(sent):\n",
    "            print(word + '\\n')\n",
    "    print('****************\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chars_per_words_lengths = []\n",
    "words_per_sents_lengths = []\n",
    "sents_per_docs_lengths = []\n",
    "\n",
    "# Chars per word should be on all text\n",
    "for text in data_train.review:\n",
    "    \n",
    "    sents = sent_tokenize(clean_str(BeautifulSoup(text).get_text()))\n",
    "    sents_per_docs_lengths.append(len(sents))\n",
    "    for sent in sents:       \n",
    "    \n",
    "        words = word_tokenize(text)\n",
    "        words_per_sents_lengths.append(len(words))\n",
    "        for word in words:\n",
    "            chars_per_words_lengths.append(len(word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3779c14be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_s = plt.hist(sents_per_docs_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f377006e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_w = plt.hist(words_per_sents_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3770250198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_c = plt.hist(chars_per_words_lengths, bins=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char vocab (all text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-65b51c79b853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_texts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_chars_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab_char-{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sent_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_to_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sent_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sent_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_sent_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_sent_len\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchar2int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_to_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mint2char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_texts' is not defined"
     ]
    }
   ],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_chars_vocab(all_texts)\n",
    "np.savez('vocab_char-{}'.format(max_sent_len), vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )\n",
    "char2int = vocab_to_int\n",
    "int2char = int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize char data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_char_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model, encoder_word_embedding_model = build_char_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit char model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1  \n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_char-{}.hdf5\".format(max_sent_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_model_file = 'encoder_char_model-{}.hdf5'\n",
    "decoder_char_model_file = 'decoder_char_model-{}.hdf5'\n",
    "encoder_model.save('encoder_char_model-{}.hdf5'.format(max_sent_len))\n",
    "decoder_model.save('decoder_char_model-{}.hdf5'.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word vocab (target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_seq_len=15\n",
    "max_chars_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts\n",
    "vocab_to_int, int_to_vocab = build_words_vocab(all_texts)\n",
    "word2int = vocab_to_int\n",
    "int2word = int_to_vocab\n",
    "np.savez('vocab_hier-{}-{}'.format(max_words_seq_len, max_chars_seq_len), char2int=char2int, int2char=int2char, word2int=word2int, int2word=int2word, max_words_seq_len=max_words_seq_len, max_char_seq_len=max_chars_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word_tokens=len(sorted(list(word2int)))\n",
    "num_char_tokens=len(sorted(list(char2int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecotrize words data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_input_data, decoder_word_input_data, decoder_word_target_data = vectorize_words_data(input_texts, \n",
    "                                                                                                target_texts, \n",
    "                                                                                                max_words_seq_len, \n",
    "                                                                                                max_chars_seq_len, \n",
    "                                                                                                num_char_tokens, \n",
    "                                                                                                num_word_tokens, \n",
    "                                                                                                word2int, \n",
    "                                                                                                char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load char encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_char_model = load_model(encoder_char_model_file.format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_word_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model, encoder_sentence_embedding_model = build_hier_model(encoder_word_embedding_model=encoder_word_embedding_model, \n",
    "                              max_words_seq_len=max_words_seq_len,\n",
    "                              max_char_seq_len=max_chars_seq_len,\n",
    "                              num_word_tokens=num_word_tokens,\n",
    "                              num_char_tokens=num_char_tokens, \n",
    "                              latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_sentence_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_hier_model-{}-{}.hdf5\".format(max_words_seq_len,max_chars_seq_len) # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "model.fit([encoder_char_input_data, decoder_word_input_data], decoder_word_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_char_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_word_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    \n",
    "    decoded_sentence, _ = decode_words_sequence(input_seq, encoder_model, decoder_model, max_words_seq_len, num_word_tokens, int2word)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train sentences model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SENTS_PER_DOC = 15\n",
    "MAX_WORDS_PER_SENT = 40\n",
    "MAX_CHARS_PER_WORD = 20\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for text in data_train.review:\n",
    "    for sent in sent_tokenize(clean_str(BeautifulSoup(text).get_text())):\n",
    "        print(sent + '\\n')\n",
    "        for word in word_tokenize(sent):\n",
    "            print(word + '\\n')\n",
    "    print('****************\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hier_input_data, hier_input_targets = vectorize_sentences_data(input_texts=data_train.review, \n",
    "                                                               target_labels=data_train.sentiment, \n",
    "                                                               max_sents_per_doc=MAX_SENTS_PER_DOC, \n",
    "                                                               max_words_per_sent=MAX_WORDS_PER_SENT, \n",
    "                                                               max_chars_per_word=MAX_CHARS_PER_WORD, \n",
    "                                                               num_classes=NUM_CLASSES, \n",
    "                                                               char2int=char2int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
