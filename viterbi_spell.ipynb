{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "Spelling correction can be handled word by word according to the potential mistakes (edit distance).\n",
    "\n",
    "However, to handle the context, we need some sort of Language Modeling.\n",
    "Since the OCR mistakes are unpredictable, we cannot depend on known vocabulary at the input. If we do so we suffer a lot of Out-of-Vocabulary (OOV). So we have to do it at the character level.\n",
    "\n",
    "The problem falls under unmatched sequence learning. Seq2seq models are the choice to solve such problems, as used in machine translation.\n",
    "\n",
    "Using char level seq2seq yields good results, but has two issues:\n",
    "\n",
    "1- Not good with long sequences > 50 char\n",
    "2- Sometimes it produces \"Halucination\", replacing a word completely with other valid words according to the language model, instead of correcting the word.\n",
    "\n",
    "Example: Date of acdent--> Seq2Seq --> Date of birth.\n",
    "\n",
    "The purpose of this notebook is to correct the words in the classical way (based on edit distance), but taking the language model into account.\n",
    "\n",
    "First we build n-gram LM, which yields a transition probability matrix (A). Then build a spelling corrector based on edit distance, which gives emission probability matrix (B). Finally, we use a Viterbi decoder to find the best sequence of corrections for a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build word spelling corrector (autocorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "import tarfile\n",
    "import csv\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        sents = row.split(delimiter)\n",
    "        if (len(sents) < 2):\n",
    "            continue\n",
    "        input_text = sents[prediction_index]\n",
    "        gt_texts.append(sents[gt_index])\n",
    "    return input_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_medical_terms(json_file):\n",
    "    texts = []\n",
    "    with open(json_file) as f:\n",
    "        med_terms_dict = json.load(f)\n",
    "    texts += list(med_terms_dict.keys())\n",
    "    texts += list(med_terms_dict.values())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accidents_terms(file_name):\n",
    "\n",
    "    f = open(file_name, encoding='utf8')\n",
    "    line = 0  \n",
    "    texts = []\n",
    "    try:\n",
    "        for r in f:\n",
    "            for term in r.split('|'):\n",
    "                    texts += term.replace('\\\"', '')\n",
    "    except:\n",
    "        print('finished')\n",
    "\n",
    "                \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # Try to correct the word from known dict\n",
    "    #word = spell(word)\n",
    "    # Option 1: Replace special chars and digits\n",
    "    #processed_word = re.sub(r'[\\\\\\/\\-\\窶能\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\笳十\@\\+\\-\\*\\d]', r'', w.lower())\n",
    "    \n",
    "    # Option 2: skip all words with special chars or digits\n",
    "    if(len(re.findall(r'[\\\\\\/\\-\\窶能\:\\[\\]\\,\\.\\\"\\;\\%\\~\\(\\)\\{\\}\\$\\#\\?\\笳十\@\\+\\-\\*\\d]', word.lower())) == 0):\n",
    "        processed_word = word\n",
    "    else:\n",
    "        processed_word = 'UNK'\n",
    "\n",
    "    # Skip stop words\n",
    "    #stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]        \n",
    "    stop_words = []        \n",
    "    if processed_word in stop_words:\n",
    "        processed_word = 'UNK'\n",
    "        \n",
    "    return processed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Load tesseract correction\n",
    "\n",
    "files_list = ['all_ocr_data_2.txt', \n",
    "              'field_class_21.txt', \n",
    "              'field_class_22.txt', \n",
    "              'field_class_23.txt', \n",
    "              'field_class_24.txt', \n",
    "              'field_class_25.txt', \n",
    "              'field_class_26.txt', \n",
    "              'field_class_27.txt', \n",
    "              'field_class_28.txt', \n",
    "              'field_class_29.txt', \n",
    "              'field_class_30.txt', \n",
    "              'field_class_31.txt', \n",
    "              'field_class_32.txt', \n",
    "              'field_class_33.txt', \n",
    "              'field_class_34.txt', \n",
    "              'NL-14622714.txt', \n",
    "              'NL-14627449.txt', \n",
    "              'NL-14628986.txt', \n",
    "              'NL-14631911.txt', \n",
    "              'NL-14640007.txt']\n",
    "\n",
    "#files_list = ['all_ocr_data_2.txt']\n",
    "#files_list = ['field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt', 'field_class_21.txt']\n",
    "\n",
    "for file_name in files_list:\n",
    "    tess_correction_data = os.path.join(data_path, file_name)\n",
    "    _, gt = load_data_with_gt(tess_correction_data)\n",
    "    texts += gt\n",
    "    \n",
    "# Load HW terms\n",
    "\n",
    "hw_correction_data = os.path.join(data_path, 'handwritten_output.txt')\n",
    "_, gt = load_data_with_gt(hw_correction_data, delimiter='|', gt_index=0, prediction_index=1)\n",
    "texts += gt\n",
    "\n",
    "# Load clean claims forms\n",
    "\n",
    "num_samples = 10000\n",
    "file_name = os.path.join(data_path, 'claims.txt')\n",
    "#texts += load_raw_data(file_name)\n",
    "\n",
    "# Load Medical Terms dictionary\n",
    "json_file = os.path.join(data_path, 'abbrevs.json')\n",
    "texts += load_medical_terms(json_file)\n",
    "\n",
    "# Load Medical Instruction dictionary\n",
    "file_name = os.path.join(data_path, 'medical_instructions.txt')\n",
    "texts += load_raw_data(file_name)\n",
    "\n",
    "# Load accident terms\n",
    "file_name = os.path.join(data_path, 'AccidentsL.txt')\n",
    "texts += load_accidents_terms(file_name)\n",
    "\n",
    "# Load procedures and tests\n",
    "file_name = os.path.join(data_path, 'procedures_tests.txt')\n",
    "texts += load_raw_data(file_name)\n",
    "\n",
    "# Load Diagnosis Descriptions\n",
    "file_name = os.path.join(data_path, 'ICD10.csv')\n",
    "t = pd.read_csv(file_name)\n",
    "texts += list(t['DESCRIPTION'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1163183\n",
      "Claim Type: VB Accident - Accidental Injury\n",
      " \n",
      "\n",
      "Who The Reported Event Happened To: Employee/Policyholder\n",
      " \n",
      "\n",
      "Policyholder/Owner Information\n",
      " \n",
      "\n",
      "First Name:\n",
      " \n",
      "\n",
      "Middle Name/Initial:\n",
      " \n",
      "\n",
      "Last Name:\n",
      " \n",
      "\n",
      "Social Security Number:\n",
      " \n",
      "\n",
      "Birth Date:\n",
      " \n",
      "\n",
      "Gender:\n",
      " \n",
      "\n",
      "Language Preference:\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(texts))\n",
    "for i in range(10):\n",
    "    print(texts[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('med.txt', 'w') as f:\n",
    "    for text in texts:\n",
    "        f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words/\r\n",
      "words/en_US_GB_CA_mixed.txt\r\n",
      "words/big_orig.txt\r\n",
      "words/._big.txt\r\n",
      "words/big.txt\r\n",
      "words/en_US_GB_CA_lower.txt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf autocorrect/words.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_big_orig = open('words/big_orig.txt', 'r')\n",
    "f_big_orig = open('med.txt', 'r')\n",
    "f_med = open('med.txt', 'r')\n",
    "f_big = open('words/big.txt', 'w')\n",
    "for line in f_big_orig:\n",
    "    f_big.write(line + '\\n')\n",
    "for line in f_med:\n",
    "    f_big.write(line + '\\n')\n",
    "    \n",
    "\n",
    "f_big_orig.close()\n",
    "f_big.close()\n",
    "f_med.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words/\r\n",
      "words/en_US_GB_CA_mixed.txt\r\n",
      "words/big_orig.txt\r\n",
      "words/._big.txt\r\n",
      "words/big.txt\r\n",
      "words/en_US_GB_CA_lower.txt\r\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf autocorrect/words.tar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf words/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build n-gram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(w):\n",
    "    if w is not None: w = w.lower() \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_lm(corpus):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for sentence in corpus:\n",
    "        sentence = word_tokenize(sentence)\n",
    "        #print(sentence)\n",
    "        #print(word_tokenize(sentence))\n",
    "        #print(list(trigrams(sentence, pad_right=True, pad_left=True)))\n",
    "        #print(list(trigrams(word_tokenize(sentence), pad_right=True, pad_left=True)))\n",
    "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model[(w1, w2)][w3] += 1\n",
    "\n",
    "    '''\n",
    "    print(model[\"what\", \"the\"][\"economists\"])  # \"economists\" follows \"what the\" 2 times\n",
    "    print(model[\"what\", \"the\"][\"nonexistingword\"])  # 0 times\n",
    "    print(model[None, None][\"The\"])  # 8839 sentences start with \"The\"\n",
    "    '''\n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1_w2 in model:\n",
    "        total_count = float(sum(model[w1_w2].values()))\n",
    "        for w3 in model[w1_w2]:\n",
    "            model[w1_w2][w3] /= total_count\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_lm(corpus):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for sentence in corpus:\n",
    "        sentence = word_tokenize(sentence)\n",
    "        #print(sentence)\n",
    "        #print(word_tokenize(sentence))\n",
    "        #print(list(trigrams(sentence, pad_right=True, pad_left=True)))\n",
    "        #print(list(trigrams(word_tokenize(sentence), pad_right=True, pad_left=True)))\n",
    "        for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "            w1 = lower_case(w1)\n",
    "            w2 = lower_case(w2)\n",
    "            model[w1][w2] += 1\n",
    "\n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1 in model:\n",
    "        w1 = lower_case(w1)\n",
    "        total_count = float(sum(model[w1].values()))\n",
    "        for w2 in model[w1]:\n",
    "            w2 = lower_case(w2)\n",
    "            model[w1][w2] /= total_count\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = build_bigram_lm(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11124845488257108"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['first']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 'First'\n",
    "w = None\n",
    "if w is not None: w = w.lower() \n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    \n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t-1][states[0]][\"prob\"]*trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "                    \n",
    "            max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "    '''\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "    '''    \n",
    "    opt = []\n",
    "    # The highest probability\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    previous = None\n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            opt.append(st)\n",
    "            previous = st\n",
    "            break\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    #print('The steps of states are ' + ' '.join(opt) + ' with highest probability of %s' % max_prob)\n",
    "\n",
    "    return opt\n",
    "\n",
    "def dptable(V):\n",
    "    # Print a table of steps from dictionary\n",
    "    yield(\" \".join((\"%12d\" % i) for i in range(len(V))))\n",
    "    for state in V[0]:\n",
    "        yield(\"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence path decoder\n",
    "Given a sequence of words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build states: set of all possible states at each word location. \n",
    "s_K, K=number of states=All possible candidates at all words locations.\n",
    "s is a dict, keys=words, values=state index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the emission probability for each observed word from the states set. \n",
    "B_TxK, T=number of words in the sentence.\n",
    "B is a an array of dicts.\n",
    "B[i] is dict: key=word from s_k, value=emission probability score for each word from s_K, if the input is w_i (NLP_COUNT.get(candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build state transition matrix A_KxK.\n",
    "use p, the n-gram LM.\n",
    "double loop on all states (word from s_k), and get for each A[si][sj]=p[si][sj]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build initial states array Pi_K.\n",
    "Pi_K is a dict, keys=word from s_k, value=scores (normalized) of the candidates of word at loc i=0--> B[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run Viterbi on:\n",
    "```\n",
    "obs = nltk.tokenize(sentence)\n",
    "states = s_K\n",
    "trans_p=A\n",
    "emit_p=B\n",
    "output = viterbi(obs, states, start_p, trans_p, emit_p)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "from autocorrect.nlp_parser import NLP_COUNTS\n",
    "from autocorrect.word import Word, common, exact, known, get_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(word):\n",
    "    w = Word(word)\n",
    "    \n",
    "    candidates = (common([word]) | exact([word]) | known([word]) |\n",
    "              known(w.typos()) | common(w.double_typos()) or\n",
    "              [word])\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    candidates = (common([word]) | exact([word]) | known([word]) |\n",
    "              known(w.typos()) or common(w.double_typos()) or\n",
    "              [word])\n",
    "    '''\n",
    "    '''\n",
    "    candidates = (common([word]) or exact([word]) or known([word]) or\n",
    "                  known(w.typos()) or common(w.double_typos()) or\n",
    "                  [word])\n",
    "    '''\n",
    "    candidates_dict = {}\n",
    "    for candidate in candidates:\n",
    "        candidates_dict[candidate] = NLP_COUNTS.get(candidate)\n",
    "    return candidates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(word):\n",
    "    w = Word(word)\n",
    "    \n",
    "    candidates = (common([word]) | exact([word]) | known([word]) |\n",
    "              known(w.typos()) | common(w.double_typos()) or\n",
    "              [word])\n",
    "\n",
    "    w_common = 10\n",
    "    w_exact = 10\n",
    "    w_known = 5\n",
    "    w_known_typo = 2\n",
    "    w_known_double_typos = 0.05\n",
    "    w_word = 0.5\n",
    "    \n",
    "    tot = w_common + w_exact + w_known + w_known_typo + w_known_double_typos + w_word\n",
    "    w_common /= tot\n",
    "    w_exact  /= tot\n",
    "    w_known  /= tot\n",
    "    w_known_typo  /= tot\n",
    "    w_known_double_typos  /= tot\n",
    "    w_word  /= tot\n",
    "    \n",
    "    candidates_dict = {}\n",
    "    for i, candidate in enumerate(common([word])):\n",
    "        candidates_dict[candidate] = w_common * NLP_COUNTS.get(candidate)\n",
    "    for i, candidate in enumerate(exact([word])):\n",
    "        candidates_dict[candidate] = w_exact * NLP_COUNTS.get(candidate)                                  \n",
    "    for i, candidate in enumerate(known([word])):\n",
    "        candidates_dict[candidate] = w_known * NLP_COUNTS.get(candidate)                                  \n",
    "    for i, candidate in enumerate(known(w.typos())):\n",
    "        candidates_dict[candidate] = w_known_typo * NLP_COUNTS.get(candidate)                                  \n",
    "    for i, candidate in enumerate(common(w.double_typos())):\n",
    "        candidates_dict[candidate] = w_known_double_typos * NLP_COUNTS.get(candidate) \n",
    "    candidates_dict[word] = w_word * NLP_COUNTS.get(word) \n",
    "                                  \n",
    "    return candidates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'Nane'\n",
    "\n",
    "candidates_dict = get_candidates(word)\n",
    "\n",
    "s = set()\n",
    "s |= set(candidates_dict.keys())\n",
    "#s\n",
    "exact([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nane'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ane',\n",
       " 'anne',\n",
       " 'bane',\n",
       " 'cane',\n",
       " 'dane',\n",
       " 'fane',\n",
       " 'gane',\n",
       " 'inane',\n",
       " 'jane',\n",
       " 'kane',\n",
       " 'lane',\n",
       " 'mane',\n",
       " 'nabe',\n",
       " 'nace',\n",
       " 'nae',\n",
       " 'nage',\n",
       " 'nake',\n",
       " 'nale',\n",
       " 'name',\n",
       " 'nan',\n",
       " 'nana',\n",
       " 'nance',\n",
       " 'nand',\n",
       " 'nane',\n",
       " 'nanes',\n",
       " 'nani',\n",
       " 'nanp',\n",
       " 'nans',\n",
       " 'nant',\n",
       " 'nape',\n",
       " 'nare',\n",
       " 'nate',\n",
       " 'nave',\n",
       " 'naze',\n",
       " 'nene',\n",
       " 'nine',\n",
       " 'nne',\n",
       " 'none',\n",
       " 'pane',\n",
       " 'rane',\n",
       " 'sane',\n",
       " 'tane',\n",
       " 'vane',\n",
       " 'wane',\n",
       " 'zane'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(word)\n",
    "known(w.typos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age',\n",
       " 'an',\n",
       " 'and',\n",
       " 'ang',\n",
       " 'ank',\n",
       " 'ant',\n",
       " 'any',\n",
       " 'are',\n",
       " 'ave',\n",
       " 'axe',\n",
       " 'bale',\n",
       " 'band',\n",
       " 'bank',\n",
       " 'bare',\n",
       " 'base',\n",
       " 'bene',\n",
       " 'bone',\n",
       " 'cafe',\n",
       " 'cage',\n",
       " 'came',\n",
       " 'can',\n",
       " 'canoe',\n",
       " 'care',\n",
       " 'case',\n",
       " 'cave',\n",
       " 'cone',\n",
       " 'crane',\n",
       " 'date',\n",
       " 'done',\n",
       " 'dune',\n",
       " 'ease',\n",
       " 'face',\n",
       " 'fame',\n",
       " 'fan',\n",
       " 'fine',\n",
       " 'gade',\n",
       " 'game',\n",
       " 'gang',\n",
       " 'gate',\n",
       " 'gave',\n",
       " 'gene',\n",
       " 'gone',\n",
       " 'han',\n",
       " 'hand',\n",
       " 'hang',\n",
       " 'have',\n",
       " 'igne',\n",
       " 'lake',\n",
       " 'lance',\n",
       " 'land',\n",
       " 'late',\n",
       " 'line',\n",
       " 'lune',\n",
       " 'made',\n",
       " 'make',\n",
       " 'male',\n",
       " 'man',\n",
       " 'mane',\n",
       " 'maneq',\n",
       " 'many',\n",
       " 'mine',\n",
       " 'na',\n",
       " 'nail',\n",
       " 'nal',\n",
       " 'name',\n",
       " 'named',\n",
       " 'names',\n",
       " 'napa',\n",
       " 'nee',\n",
       " 'nice',\n",
       " 'nine',\n",
       " 'node',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nose',\n",
       " 'note',\n",
       " 'one',\n",
       " 'pace',\n",
       " 'page',\n",
       " 'pale',\n",
       " 'pan',\n",
       " 'panel',\n",
       " 'plane',\n",
       " 'race',\n",
       " 'ran',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'safe',\n",
       " 'sake',\n",
       " 'same',\n",
       " 'sand',\n",
       " 'snake',\n",
       " 'take',\n",
       " 'tank',\n",
       " 'tape',\n",
       " 'tone',\n",
       " 'une',\n",
       " 'van',\n",
       " 'wake',\n",
       " 'want',\n",
       " 'ware',\n",
       " 'wave',\n",
       " 'zone'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common(w.double_typos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nane'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_case(word, 'nane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = {}\n",
    "word = 'Cender'\n",
    "candidates_dict = get_candidates(word)\n",
    "for candidate in candidates_dict.keys():\n",
    "    if not candidate in B:\n",
    "        B[candidate] = {}\n",
    "    B[candidate][word] = candidates_dict[candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_p = {'a':1, 'b':2}\n",
    "tot = np.sum(list(emit_p.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction: Patient was not be needing this to school on the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'Patiet last tet is needing this for school at this'\n",
    "#sentence = 'First Nane'\n",
    "sentence = word_tokenize(sentence)\n",
    "\n",
    "# K = n_states\n",
    "# N = n_obs\n",
    "# Build s and emit_p (B_KxN)\n",
    "states = set()\n",
    "emit_p = {}\n",
    "for obs in sentence:\n",
    "    obs = lower_case(obs)\n",
    "    candidates_dict = get_candidates(obs)\n",
    "    states |= set(candidates_dict.keys())\n",
    "    for state in candidates_dict.keys():\n",
    "        if not state in emit_p:\n",
    "            emit_p[state] = {}\n",
    "        emit_p[state][obs] = candidates_dict[state]\n",
    "\n",
    "# Normalize emit_p\n",
    "for state in emit_p.keys():\n",
    "    #for obs in emit_p[state].keys():\n",
    "    for obs in sentence:     \n",
    "        obs = lower_case(obs)\n",
    "        if obs in emit_p[state].keys():\n",
    "            emit_p[state][obs] /= (np.sum(list(emit_p[state].values()))+1)\n",
    "        else:\n",
    "            emit_p[state][obs] = 0\n",
    "\n",
    "# Build trans_p (A_KxK)\n",
    "trans_p = {}\n",
    "for state_i in states:\n",
    "    \n",
    "    if not state_i in trans_p:\n",
    "        trans_p[state_i] = {}\n",
    "    \n",
    "    for state_j in states:\n",
    "        trans_p[state_i][state_j] = p[state_i][state_j]\n",
    "        \n",
    "# Build start_p (Pi_Kx1)\n",
    "start_p = {}\n",
    "w_0 = lower_case(sentence[0])\n",
    "candidates_dict = get_candidates(w_0)\n",
    "for state in states:\n",
    "       \n",
    "    #if state in candidates_dict.keys():\n",
    "    if state == spell(w_0):\n",
    "        start_p[state] = candidates_dict[state]\n",
    "    else:\n",
    "        start_p[state] = 0\n",
    "\n",
    "# Normalize start_p\n",
    "for state in states:\n",
    "    start_p[state] /= np.sum(list(start_p.values()))\n",
    "\n",
    "obs = [lower_case(obs) for obs in sentence] \n",
    "\n",
    "#print('obs: ' + str(obs))\n",
    "#print('states: ' + str(states))\n",
    "#print('start_p: ' + str(start_p))\n",
    "#print('trans_p: ' + str(trans_p))\n",
    "#print('emit_p: ' + str(emit_p))\n",
    "correct_words = viterbi(obs, tuple(states), start_p, trans_p, emit_p)\n",
    "\n",
    "correct_words_ = []\n",
    "for i, w in enumerate(sentence):\n",
    "    correct_words_.append(get_case(w, correct_words[i]))\n",
    "    \n",
    "print('Correction: ' + ' '.join(correct_words_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004418159562346848"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_p['patient']['was']*emit_p['was']['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.858439201451908"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('last')['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.058076225045372"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates('last')['was']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009819011862144665"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_p['patient']['last']*emit_p['last']['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01362088535754824"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_p['patient']['was']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011350737797956867"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_p['patient']['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient last tet is needing this for school at this\n"
     ]
    }
   ],
   "source": [
    "spell_corrected_sentence = []\n",
    "for w in sentence:\n",
    "    spell_corrected_sentence.append(spell(w))\n",
    "print(' '.join(spell_corrected_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'wrist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d4bcb905c15c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memit_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wrist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'wrist'"
     ]
    }
   ],
   "source": [
    "emit_p['wrist']['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_p['first']['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_p['name']['nane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_p['and']['nane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_p['wrist']['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_p['first']['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_p['first']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_p['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_p['wrist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_p['nane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_p['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_candidates('nane')['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_COUNTS.get('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_viterbi(sentence):\n",
    "    sentence = word_tokenize(sentence)\n",
    "\n",
    "    # K = n_states\n",
    "    # N = n_obs\n",
    "    # Build s and emit_p (B_KxN)\n",
    "    states = set()\n",
    "    emit_p = {}\n",
    "    for obs in sentence:\n",
    "        obs = lower_case(obs)\n",
    "        candidates_dict = get_candidates(obs)\n",
    "        states |= set(candidates_dict.keys())\n",
    "        for state in candidates_dict.keys():\n",
    "            if not state in emit_p:\n",
    "                emit_p[state] = {}\n",
    "            emit_p[state][obs] = candidates_dict[state]\n",
    "\n",
    "    # Normalize emit_p\n",
    "    for state in emit_p.keys():\n",
    "        #for obs in emit_p[state].keys():\n",
    "        for obs in sentence:     \n",
    "            obs = lower_case(obs)\n",
    "            if obs in emit_p[state].keys():\n",
    "                emit_p[state][obs] /= (np.sum(list(emit_p[state].values()))+1)\n",
    "            else:\n",
    "                emit_p[state][obs] = 0\n",
    "\n",
    "    # Build trans_p (A_KxK)\n",
    "    trans_p = {}\n",
    "    for state_i in states:\n",
    "\n",
    "        if not state_i in trans_p:\n",
    "            trans_p[state_i] = {}\n",
    "\n",
    "        for state_j in states:\n",
    "            trans_p[state_i][state_j] = p[state_i][state_j]\n",
    "\n",
    "    # Build start_p (Pi_Kx1)\n",
    "    start_p = {}\n",
    "    w_0 = lower_case(sentence[0])\n",
    "    candidates_dict = get_candidates(w_0)\n",
    "    for state in states:\n",
    "\n",
    "        #if state in candidates_dict.keys():\n",
    "        if state == spell(w_0):\n",
    "            start_p[state] = candidates_dict[state]\n",
    "        else:\n",
    "            start_p[state] = 0\n",
    "\n",
    "    # Normalize start_p\n",
    "    for state in states:\n",
    "        start_p[state] /= np.sum(list(start_p.values()))\n",
    "\n",
    "    obs = [lower_case(obs) for obs in sentence] \n",
    "\n",
    "    #print('obs: ' + str(obs))\n",
    "    #print('states: ' + str(states))\n",
    "    #print('start_p: ' + str(start_p))\n",
    "    #print('trans_p: ' + str(trans_p))\n",
    "    #print('emit_p: ' + str(emit_p))\n",
    "    correct_words = viterbi(obs, tuple(states), start_p, trans_p, emit_p)\n",
    "\n",
    "    correct_words_ = []\n",
    "    for i, w in enumerate(sentence):\n",
    "        correct_words_.append(get_case(w, correct_words[i])) \n",
    "    return ' '.join(correct_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who The Reported Event Happened To my My'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = spell_viterbi('Who The Reported Event Happened To: Employee/Policyholder')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT: Claim Type: VB Accident - Accidental Injury\n",
      "\n",
      "Correction: Claim Type i OR Accident to Accidental Injury\n",
      "GT: Who The Reported Event Happened To: Employee/Policyholder\n",
      "\n",
      "Correction: Who The Reported Event Happened To my My\n",
      "GT: Policyholder/Owner Information\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e3f6694855f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GT: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Correction: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspell_viterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-79f56e69f2ed>\u001b[0m in \u001b[0;36mspell_viterbi\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#print('trans_p: ' + str(trans_p))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#print('emit_p: ' + str(emit_p))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mcorrect_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memit_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcorrect_words_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2cb5b7536037>\u001b[0m in \u001b[0;36mviterbi\u001b[0;34m(obs, states, start_p, trans_p, emit_p)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Follow the backtrack till the first observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prev\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prev\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print('GT: ' + text)\n",
    "    print('Correction: ' + spell_viterbi(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
