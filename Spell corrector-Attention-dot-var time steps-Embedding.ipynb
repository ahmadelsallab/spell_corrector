{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        #for row in open(file_name, encoding='utf8'):\n",
    "        for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    print(decoder_outputs)\n",
    "    print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    print(encoder_inputs)\n",
    "    print(encoder_outputs)\n",
    "    print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 1000000\n",
    "min_sent_len = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of lenghts\n",
    "lengths = []\n",
    "for text in input_texts:\n",
    "    lengths.append(len(text))\n",
    "    lengths.append(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56cc6b3e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = plt.hist(lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1180.,   940.,  1384.,  1322.,  1174.,   722.,   592.,   536.,\n",
       "         332.,   286.,   242.,   186.,   180.,   214.,   112.,   154.,\n",
       "          68.,    82.,    58.,    88.,    70.,    64.,    32.,    30.,\n",
       "          20.,    26.,    32.,    24.,    58.,    14.,    64.,     6.,\n",
       "          28.,    16.,    24.,    28.,     8.,    18.,    14.,    18.,\n",
       "          12.,    24.,    14.,    28.,    14.,     4.,    12.,    44.,\n",
       "           4.,     6.,     2.,     4.,     6.,     0.,    36.,     0.,\n",
       "           4.,     4.,     8.,     6.,    14.,     8.,     8.,     8.,\n",
       "           2.,     0.,     6.,     2.,     2.,     4.,    12.,    14.,\n",
       "           8.,    12.,     6.,     0.,     4.,     4.,     2.,     0.,\n",
       "           2.,     0.,     4.,     6.,     0.,     4.,    14.,    26.,\n",
       "           4.,     0.,     2.,     4.,     4.,     2.,     4.,     2.,\n",
       "           6.,     0.,     2.,     2.,     2.,     6.,     4.,     2.,\n",
       "          40.,     4.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,    20.,     8.,    18.,     4.,     0.,\n",
       "           0.,     0.,     0.,     6.,    24.,     2.,     0.,     0.,\n",
       "           0.,     2.,     2.,     0.,     0.,     2.,     2.,     4.,\n",
       "           2.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     4.,     4.,\n",
       "          16.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     4.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     2.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   ,     4.878,     9.756,    14.634,    19.512,    24.39 ,\n",
       "          29.268,    34.146,    39.024,    43.902,    48.78 ,    53.658,\n",
       "          58.536,    63.414,    68.292,    73.17 ,    78.048,    82.926,\n",
       "          87.804,    92.682,    97.56 ,   102.438,   107.316,   112.194,\n",
       "         117.072,   121.95 ,   126.828,   131.706,   136.584,   141.462,\n",
       "         146.34 ,   151.218,   156.096,   160.974,   165.852,   170.73 ,\n",
       "         175.608,   180.486,   185.364,   190.242,   195.12 ,   199.998,\n",
       "         204.876,   209.754,   214.632,   219.51 ,   224.388,   229.266,\n",
       "         234.144,   239.022,   243.9  ,   248.778,   253.656,   258.534,\n",
       "         263.412,   268.29 ,   273.168,   278.046,   282.924,   287.802,\n",
       "         292.68 ,   297.558,   302.436,   307.314,   312.192,   317.07 ,\n",
       "         321.948,   326.826,   331.704,   336.582,   341.46 ,   346.338,\n",
       "         351.216,   356.094,   360.972,   365.85 ,   370.728,   375.606,\n",
       "         380.484,   385.362,   390.24 ,   395.118,   399.996,   404.874,\n",
       "         409.752,   414.63 ,   419.508,   424.386,   429.264,   434.142,\n",
       "         439.02 ,   443.898,   448.776,   453.654,   458.532,   463.41 ,\n",
       "         468.288,   473.166,   478.044,   482.922,   487.8  ,   492.678,\n",
       "         497.556,   502.434,   507.312,   512.19 ,   517.068,   521.946,\n",
       "         526.824,   531.702,   536.58 ,   541.458,   546.336,   551.214,\n",
       "         556.092,   560.97 ,   565.848,   570.726,   575.604,   580.482,\n",
       "         585.36 ,   590.238,   595.116,   599.994,   604.872,   609.75 ,\n",
       "         614.628,   619.506,   624.384,   629.262,   634.14 ,   639.018,\n",
       "         643.896,   648.774,   653.652,   658.53 ,   663.408,   668.286,\n",
       "         673.164,   678.042,   682.92 ,   687.798,   692.676,   697.554,\n",
       "         702.432,   707.31 ,   712.188,   717.066,   721.944,   726.822,\n",
       "         731.7  ,   736.578,   741.456,   746.334,   751.212,   756.09 ,\n",
       "         760.968,   765.846,   770.724,   775.602,   780.48 ,   785.358,\n",
       "         790.236,   795.114,   799.992,   804.87 ,   809.748,   814.626,\n",
       "         819.504,   824.382,   829.26 ,   834.138,   839.016,   843.894,\n",
       "         848.772,   853.65 ,   858.528,   863.406,   868.284,   873.162,\n",
       "         878.04 ,   882.918,   887.796,   892.674,   897.552,   902.43 ,\n",
       "         907.308,   912.186,   917.064,   921.942,   926.82 ,   931.698,\n",
       "         936.576,   941.454,   946.332,   951.21 ,   956.088,   960.966,\n",
       "         965.844,   970.722,   975.6  ,   980.478,   985.356,   990.234,\n",
       "         995.112,   999.99 ,  1004.868,  1009.746,  1014.624,  1019.502,\n",
       "        1024.38 ,  1029.258,  1034.136,  1039.014,  1043.892,  1048.77 ,\n",
       "        1053.648,  1058.526,  1063.404,  1068.282,  1073.16 ,  1078.038,\n",
       "        1082.916,  1087.794,  1092.672,  1097.55 ,  1102.428,  1107.306,\n",
       "        1112.184,  1117.062,  1121.94 ,  1126.818,  1131.696,  1136.574,\n",
       "        1141.452,  1146.33 ,  1151.208,  1156.086,  1160.964,  1165.842,\n",
       "        1170.72 ,  1175.598,  1180.476,  1185.354,  1190.232,  1195.11 ,\n",
       "        1199.988,  1204.866,  1209.744,  1214.622,  1219.5  ,  1224.378,\n",
       "        1229.256,  1234.134,  1239.012,  1243.89 ,  1248.768,  1253.646,\n",
       "        1258.524,  1263.402,  1268.28 ,  1273.158,  1278.036,  1282.914,\n",
       "        1287.792,  1292.67 ,  1297.548,  1302.426,  1307.304,  1312.182,\n",
       "        1317.06 ,  1321.938,  1326.816,  1331.694,  1336.572,  1341.45 ,\n",
       "        1346.328,  1351.206,  1356.084,  1360.962,  1365.84 ,  1370.718,\n",
       "        1375.596,  1380.474,  1385.352,  1390.23 ,  1395.108,  1399.986,\n",
       "        1404.864,  1409.742,  1414.62 ,  1419.498,  1424.376,  1429.254,\n",
       "        1434.132,  1439.01 ,  1443.888,  1448.766,  1453.644,  1458.522,\n",
       "        1463.4  ,  1468.278,  1473.156,  1478.034,  1482.912,  1487.79 ,\n",
       "        1492.668,  1497.546,  1502.424,  1507.302,  1512.18 ,  1517.058,\n",
       "        1521.936,  1526.814,  1531.692,  1536.57 ,  1541.448,  1546.326,\n",
       "        1551.204,  1556.082,  1560.96 ,  1565.838,  1570.716,  1575.594,\n",
       "        1580.472,  1585.35 ,  1590.228,  1595.106,  1599.984,  1604.862,\n",
       "        1609.74 ,  1614.618,  1619.496,  1624.374,  1629.252,  1634.13 ,\n",
       "        1639.008,  1643.886,  1648.764,  1653.642,  1658.52 ,  1663.398,\n",
       "        1668.276,  1673.154,  1678.032,  1682.91 ,  1687.788,  1692.666,\n",
       "        1697.544,  1702.422,  1707.3  ,  1712.178,  1717.056,  1721.934,\n",
       "        1726.812,  1731.69 ,  1736.568,  1741.446,  1746.324,  1751.202,\n",
       "        1756.08 ,  1760.958,  1765.836,  1770.714,  1775.592,  1780.47 ,\n",
       "        1785.348,  1790.226,  1795.104,  1799.982,  1804.86 ,  1809.738,\n",
       "        1814.616,  1819.494,  1824.372,  1829.25 ,  1834.128,  1839.006,\n",
       "        1843.884,  1848.762,  1853.64 ,  1858.518,  1863.396,  1868.274,\n",
       "        1873.152,  1878.03 ,  1882.908,  1887.786,  1892.664,  1897.542,\n",
       "        1902.42 ,  1907.298,  1912.176,  1917.054,  1921.932,  1926.81 ,\n",
       "        1931.688,  1936.566,  1941.444,  1946.322,  1951.2  ,  1956.078,\n",
       "        1960.956,  1965.834,  1970.712,  1975.59 ,  1980.468,  1985.346,\n",
       "        1990.224,  1995.102,  1999.98 ,  2004.858,  2009.736,  2014.614,\n",
       "        2019.492,  2024.37 ,  2029.248,  2034.126,  2039.004,  2043.882,\n",
       "        2048.76 ,  2053.638,  2058.516,  2063.394,  2068.272,  2073.15 ,\n",
       "        2078.028,  2082.906,  2087.784,  2092.662,  2097.54 ,  2102.418,\n",
       "        2107.296,  2112.174,  2117.052,  2121.93 ,  2126.808,  2131.686,\n",
       "        2136.564,  2141.442,  2146.32 ,  2151.198,  2156.076,  2160.954,\n",
       "        2165.832,  2170.71 ,  2175.588,  2180.466,  2185.344,  2190.222,\n",
       "        2195.1  ,  2199.978,  2204.856,  2209.734,  2214.612,  2219.49 ,\n",
       "        2224.368,  2229.246,  2234.124,  2239.002,  2243.88 ,  2248.758,\n",
       "        2253.636,  2258.514,  2263.392,  2268.27 ,  2273.148,  2278.026,\n",
       "        2282.904,  2287.782,  2292.66 ,  2297.538,  2302.416,  2307.294,\n",
       "        2312.172,  2317.05 ,  2321.928,  2326.806,  2331.684,  2336.562,\n",
       "        2341.44 ,  2346.318,  2351.196,  2356.074,  2360.952,  2365.83 ,\n",
       "        2370.708,  2375.586,  2380.464,  2385.342,  2390.22 ,  2395.098,\n",
       "        2399.976,  2404.854,  2409.732,  2414.61 ,  2419.488,  2424.366,\n",
       "        2429.244,  2434.122,  2439.   ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  9.756\n",
      "Count of most probable lenght =  1384.0\n",
      "Min length =  4.878\n"
     ]
    }
   ],
   "source": [
    "max_sent_len =  h[1][np.argmax(h[0])]\n",
    "min_sent_len = h[1][1]\n",
    "print('Most probable length = ', max_sent_len)\n",
    "print('Count of most probable lenght = ', np.max(h[0]))\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len =  100#int(np.ceil(max_sent_len))\n",
    "min_sent_len = 4#int(np.floor(min_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  100\n",
      "Min length =  4\n"
     ]
    }
   ],
   "source": [
    "print('Most probable length = ', max_sent_len)\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR_tess, target_texts_tess, gt_tess = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "num_samples = 0\n",
    "OCR_data = os.path.join(data_path, 'output_handwritten.txt')\n",
    "input_texts_OCR_hand, target_texts_OCR_hand, gt_texts_OCR_hand = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texts_OCR\n",
    "#target_texts = target_texts_OCR\n",
    "input_texts_OCR = input_texts_OCR_tess + input_texts_OCR_hand\n",
    "target_texts_OCR = target_texts_tess + target_texts_OCR_hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4316"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "threshold = 0.9\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = target_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14316\n",
      "Claim Type: VB Accident - Accidental njury \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "\n",
      "Who The Reported EventH appened To: Employe/ePolicyholder \n",
      " \tWho The Reported Event Happened To: Employee/Policyholder\n",
      "\n",
      "\n",
      "PolicyholdderO/wnerI nformalthiony \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "\n",
      "Firs Name: \n",
      " \tFirst Name:\n",
      "\n",
      "\n",
      "Middle Name/Initial: \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "\n",
      "Last Name: \n",
      " \tLast Name:\n",
      "\n",
      "\n",
      "Soial Shecuritvy Numebr: \n",
      " \tSocial Security Number:\n",
      "\n",
      "\n",
      "Birth Date: \n",
      " \tBirth Date:\n",
      "\n",
      "\n",
      "Gender: \n",
      " \tGender:\n",
      "\n",
      "\n",
      "Language Preference: \n",
      " \tLanguage Preference:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)\n",
    "np.savez('vocab', vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 14316\n",
      "Number of unique input tokens: 117\n",
      "Number of unique output tokens: 117\n",
      "Max sequence length for inputs: 99\n",
      "Max sequence length for outputs: 99\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '!': 75,\n",
       " '\"': 82,\n",
       " '#': 69,\n",
       " '$': 83,\n",
       " '%': 79,\n",
       " '&': 76,\n",
       " \"'\": 87,\n",
       " '(': 65,\n",
       " ')': 66,\n",
       " '*': 80,\n",
       " '+': 78,\n",
       " ',': 58,\n",
       " '-': 21,\n",
       " '.': 50,\n",
       " '/': 33,\n",
       " '0': 54,\n",
       " '1': 48,\n",
       " '2': 53,\n",
       " '3': 56,\n",
       " '4': 68,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 71,\n",
       " '8': 62,\n",
       " '9': 73,\n",
       " ':': 13,\n",
       " ';': 77,\n",
       " '<': 101,\n",
       " '=': 96,\n",
       " '>': 102,\n",
       " '?': 61,\n",
       " '@': 85,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 45,\n",
       " 'E': 30,\n",
       " 'F': 38,\n",
       " 'G': 46,\n",
       " 'H': 32,\n",
       " 'I': 22,\n",
       " 'J': 70,\n",
       " 'K': 52,\n",
       " 'L': 42,\n",
       " 'M': 41,\n",
       " 'N': 40,\n",
       " 'O': 35,\n",
       " 'P': 34,\n",
       " 'Q': 81,\n",
       " 'R': 29,\n",
       " 'S': 43,\n",
       " 'T': 9,\n",
       " 'U': 51,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 26,\n",
       " 'X': 57,\n",
       " 'Y': 49,\n",
       " 'Z': 72,\n",
       " '[': 94,\n",
       " '\\\\': 103,\n",
       " ']': 95,\n",
       " '^': 89,\n",
       " '_': 109,\n",
       " 'a': 6,\n",
       " 'b': 44,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 37,\n",
       " 'g': 47,\n",
       " 'h': 27,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 60,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 28,\n",
       " 'p': 11,\n",
       " 'q': 59,\n",
       " 'r': 25,\n",
       " 's': 39,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 31,\n",
       " 'w': 36,\n",
       " 'x': 63,\n",
       " 'y': 10,\n",
       " 'z': 64,\n",
       " '{': 108,\n",
       " '|': 86,\n",
       " '}': 107,\n",
       " '~': 104,\n",
       " '': 115,\n",
       " '': 111,\n",
       " '': 113,\n",
       " '': 116,\n",
       " '': 93,\n",
       " '': 112,\n",
       " '': 110,\n",
       " '': 84,\n",
       " '': 97,\n",
       " '': 98,\n",
       " '': 106,\n",
       " '': 67,\n",
       " '': 99,\n",
       " '': 92,\n",
       " '': 88,\n",
       " '': 114,\n",
       " '': 90,\n",
       " '': 100,\n",
       " '': 91,\n",
       " '': 105}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int # Some special chars need to be removed TODO: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'W',\n",
       " 27: 'h',\n",
       " 28: 'o',\n",
       " 29: 'R',\n",
       " 30: 'E',\n",
       " 31: 'v',\n",
       " 32: 'H',\n",
       " 33: '/',\n",
       " 34: 'P',\n",
       " 35: 'O',\n",
       " 36: 'w',\n",
       " 37: 'f',\n",
       " 38: 'F',\n",
       " 39: 's',\n",
       " 40: 'N',\n",
       " 41: 'M',\n",
       " 42: 'L',\n",
       " 43: 'S',\n",
       " 44: 'b',\n",
       " 45: 'D',\n",
       " 46: 'G',\n",
       " 47: 'g',\n",
       " 48: '1',\n",
       " 49: 'Y',\n",
       " 50: '.',\n",
       " 51: 'U',\n",
       " 52: 'K',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: '3',\n",
       " 57: 'X',\n",
       " 58: ',',\n",
       " 59: 'q',\n",
       " 60: 'k',\n",
       " 61: '?',\n",
       " 62: '8',\n",
       " 63: 'x',\n",
       " 64: 'z',\n",
       " 65: '(',\n",
       " 66: ')',\n",
       " 67: '',\n",
       " 68: '4',\n",
       " 69: '#',\n",
       " 70: 'J',\n",
       " 71: '7',\n",
       " 72: 'Z',\n",
       " 73: '9',\n",
       " 74: '5',\n",
       " 75: '!',\n",
       " 76: '&',\n",
       " 77: ';',\n",
       " 78: '+',\n",
       " 79: '%',\n",
       " 80: '*',\n",
       " 81: 'Q',\n",
       " 82: '\"',\n",
       " 83: '$',\n",
       " 84: '',\n",
       " 85: '@',\n",
       " 86: '|',\n",
       " 87: \"'\",\n",
       " 88: '',\n",
       " 89: '^',\n",
       " 90: '',\n",
       " 91: '',\n",
       " 92: '',\n",
       " 93: '',\n",
       " 94: '[',\n",
       " 95: ']',\n",
       " 96: '=',\n",
       " 97: '',\n",
       " 98: '',\n",
       " 99: '',\n",
       " 100: '',\n",
       " 101: '<',\n",
       " 102: '>',\n",
       " 103: '\\\\',\n",
       " 104: '~',\n",
       " 105: '',\n",
       " 106: '',\n",
       " 107: '}',\n",
       " 108: '{',\n",
       " 109: '_',\n",
       " 110: '',\n",
       " 111: '',\n",
       " 112: '',\n",
       " 113: '',\n",
       " 114: '',\n",
       " 115: '',\n",
       " 116: ''}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12168, 99)\n",
      "(12168, 99, 117)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n",
      "Tensor(\"lstm_2/transpose_2:0\", shape=(?, ?, 512), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 117)    13689       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  765952      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 117)    13689       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1290240     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 117)    119925      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,203,495\n",
      "Trainable params: 2,176,117\n",
      "Non-trainable params: 27,378\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Tensor(\"input_1:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  \n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12168 samples, validate on 2148 samples\n",
      "Epoch 1/50\n",
      "12168/12168 [==============================] - 153s 13ms/step - loss: 2.3897 - categorical_accuracy: 0.3445 - val_loss: 1.2287 - val_categorical_accuracy: 0.6335\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.63346, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "12168/12168 [==============================] - 153s 13ms/step - loss: 0.5402 - categorical_accuracy: 0.8294 - val_loss: 0.3190 - val_categorical_accuracy: 0.8876\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.63346 to 0.88761, saving model to best_model.hdf5\n",
      "Epoch 3/50\n",
      "12168/12168 [==============================] - 153s 13ms/step - loss: 0.2550 - categorical_accuracy: 0.9029 - val_loss: 0.2516 - val_categorical_accuracy: 0.9046\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.88761 to 0.90462, saving model to best_model.hdf5\n",
      "Epoch 4/50\n",
      "12168/12168 [==============================] - 150s 12ms/step - loss: 0.1782 - categorical_accuracy: 0.9218 - val_loss: 0.2227 - val_categorical_accuracy: 0.9118\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.90462 to 0.91182, saving model to best_model.hdf5\n",
      "Epoch 5/50\n",
      " 6016/12168 [=============>................] - ETA: 1:14 - loss: 0.1377 - categorical_accuracy: 0.9309"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-362e39851e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0;31m#validation_split=0.2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('encoder_model.hdf5')\n",
    "decoder_model.save('decoder_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_gt_sequence(encoder_input_data[5:6], int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Daytime Phone:\n",
      "GT sentence: Daytime Phone:\n",
      "\n",
      "Decoded sentence: Daytime Phone:\n",
      "\n",
      "-\n",
      "Input sentence: Customer Policy #:\n",
      "GT sentence: Customer Policy #:\n",
      "\n",
      "Decoded sentence: Customer Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: ue DATE _\n",
      "GT sentence: DUE DATE\n",
      "\n",
      "Decoded sentence: CaDATE\n",
      "\n",
      "-\n",
      "Input sentence: Printed by\n",
      "GT sentence: Printed by\n",
      "\n",
      "Decoded sentence: Printed by\n",
      "\n",
      "-\n",
      "Input sentence: Total:\n",
      "GT sentence: Total:\n",
      "\n",
      "Decoded sentence: Total:\n",
      "\n",
      "-\n",
      "Input sentence: Grasp _.\n",
      "GT sentence: Grasp\n",
      "\n",
      "Decoded sentence: Grasp Caspersp Choras paraspera\n",
      "\n",
      "-\n",
      "Input sentence: Arddress Line 1 :\n",
      "GT sentence: Address Line 1 :\n",
      "\n",
      "Decoded sentence: Address Line 1 :\n",
      "\n",
      "-\n",
      "Input sentence: Is tihs conditiodnth eresult ouf his/her qemploymetn Ypest No Unknown\n",
      "GT sentence: Is this condition the result of his/her employment Yes No Unknown\n",
      "\n",
      "Decoded sentence: Is this condition the result ou his/her ent No Unknown No Unknown\n",
      "\n",
      "-\n",
      "Input sentence: No acute fracture.\n",
      "GT sentence: No acute fracture.\n",
      "\n",
      "Decoded sentence: No acute fracture.\n",
      "\n",
      "-\n",
      "Input sentence: D. Inofrmation About Your Condition\n",
      "GT sentence: D. Information About Your Condition\n",
      "\n",
      "Decoded sentence: D. Information About Your Condition\n",
      "\n",
      "-\n",
      "Input sentence: Time ofAccident:\n",
      "GT sentence: Time of Accident:\n",
      "\n",
      "Decoded sentence: Time of Accident:\n",
      "\n",
      "-\n",
      "Input sentence: Pln Earninkgs\n",
      "GT sentence: Plan Earnings:\n",
      "\n",
      "Decoded sentence: Plin Earnings\n",
      "\n",
      "-\n",
      "Input sentence: Chronic arm pain\n",
      "GT sentence: Chronic arm pain\n",
      "\n",
      "Decoded sentence: Chonic arm pain\n",
      "\n",
      "-\n",
      "Input sentence: City State Zip\n",
      "GT sentence: City State Zip\n",
      "\n",
      "Decoded sentence: City State Zip\n",
      "\n",
      "-\n",
      "Input sentence: Encounter Date:\n",
      "GT sentence: Encounter Date:\n",
      "\n",
      "Decoded sentence: Encounter Date:\n",
      "\n",
      "-\n",
      "Input sentence: Righ knee injury\n",
      "GT sentence: Right knee injury\n",
      "\n",
      "Decoded sentence: Right knee injury\n",
      "\n",
      "-\n",
      "Input sentence: Z87819 ?\n",
      "GT sentence: Z87891 ?\n",
      "\n",
      "Decoded sentence: Z8789 ?\n",
      "\n",
      "-\n",
      "Input sentence: MedicalSpceicaltyDegree\n",
      "GT sentence: Medical Specialty Degree\n",
      "\n",
      "Decoded sentence: Medical Specialty Degree\n",
      "\n",
      "-\n",
      "Input sentence: CHaCl\n",
      "GT sentence: PCHC\n",
      "\n",
      "Decoded sentence: CHail\n",
      "\n",
      "-\n",
      "Input sentence: o Identies as female gender\n",
      "GT sentence:  Identifies as female gender\n",
      "\n",
      "Decoded sentence: Identifies as female gender\n",
      "\n",
      "-\n",
      "Input sentence: Diwctated: Dr. Jon J. Dewiptte, .MD.,\n",
      "GT sentence: Dictated: Dr. Jon J. Dewitte, M.D.,\n",
      "\n",
      "Decoded sentence: Dictated: Dr. Jon J. Dewipte, MD.,\n",
      "\n",
      "-\n",
      "Input sentence: Bes tPhone Number to bxe Reachd Dpuring them Da:y\n",
      "GT sentence: Best Phone Number to be Reached During the Day:\n",
      "\n",
      "Decoded sentence: Best Phone Number to be Reachid During the Day:\n",
      "\n",
      "-\n",
      "Input sentence: Contra tAdjustment\n",
      "GT sentence: Contract Adjustment\n",
      "\n",
      "Decoded sentence: ContraltAdjustment\n",
      "\n",
      "-\n",
      "Input sentence: Address\n",
      "GT sentence: Address\n",
      "\n",
      "Decoded sentence: Address\n",
      "\n",
      "-\n",
      "Input sentence: NAME:\n",
      "GT sentence: NAME: \n",
      "\n",
      "Decoded sentence: NAME:\n",
      "\n",
      "-\n",
      "Input sentence: First Choice Healh\n",
      "GT sentence: First Choice Health\n",
      "\n",
      "Decoded sentence: First Choice Health\n",
      "\n",
      "-\n",
      "Input sentence: (Nam e/ mRelationship) (Teleephone Number)\n",
      "GT sentence: (Name / Relationship) (Telephone Number)\n",
      "\n",
      "Decoded sentence: (Name / Relationship) (Telephone Number)\n",
      "\n",
      "-\n",
      "Input sentence: rOders (All orders will be implementd ulness corsswe doff.\n",
      "GT sentence: Orders (All orders will be implemented unless crossed off.\n",
      "\n",
      "Decoded sentence: Orders (Alllorers will be implement bollement corss cors dof.\n",
      "\n",
      "-\n",
      "Input sentence: Date otDiscllarge:\n",
      "GT sentence: Date of Discharge:\n",
      "\n",
      "Decoded sentence: Date of Disclarge:\n",
      "\n",
      "-\n",
      "Input sentence: No exposure tho unnprotectd heights\n",
      "GT sentence: No exposure to unprotected heights\n",
      "\n",
      "Decoded sentence: No exposure tho notectid - hights\n",
      "\n",
      "-\n",
      "Input sentence: AMUONpT BILLED\n",
      "GT sentence: AMOUNT BILLED\n",
      "\n",
      "Decoded sentence: AMOUNT BILLED\n",
      "\n",
      "-\n",
      "Input sentence: Patient wis in HS\n",
      "GT sentence: Patient is in HS\n",
      "\n",
      "Decoded sentence: Patient wis in HS\n",
      "\n",
      "-\n",
      "Input sentence: Turansportaton :Private veihclwe\n",
      "GT sentence: Transportation : Private vehicle\n",
      "\n",
      "Decoded sentence: Turansportation: Private velivile velich\n",
      "\n",
      "-\n",
      "Input sentence: IS PERMAhNENT IMPAIRMNET EXPECTED? YES tNO\n",
      "GT sentence: IS PERMANENT IMPAIRMENT EXPECTED? YES NO\n",
      "\n",
      "Decoded sentence: IS PRENTINE IMPARIMENT EXPECCEDYED? YES NO\n",
      "\n",
      "-\n",
      "Input sentence: Hopital Nmae\n",
      "GT sentence: Hospital Name\n",
      "\n",
      "Decoded sentence: Hoppital Name\n",
      "\n",
      "-\n",
      "Input sentence: Do you work for any other employers?\n",
      "GT sentence: Do you work for any other employers? No\n",
      "\n",
      "Decoded sentence: Do you work for any other employers?\n",
      "\n",
      "-\n",
      "Input sentence: First Choci eHealth\n",
      "GT sentence: First Choice Health\n",
      "\n",
      "Decoded sentence: First Choice Health\n",
      "\n",
      "-\n",
      "Input sentence: *c Inas. Pending\n",
      "GT sentence: * Ins. Pending\n",
      "\n",
      "Decoded sentence: * Ins. Pending\n",
      "\n",
      "-\n",
      "Input sentence: PROC OCDE\n",
      "GT sentence: PROC CODE\n",
      "\n",
      "Decoded sentence: PROC COCDE\n",
      "\n",
      "-\n",
      "Input sentence: Condtinuuosly 67-010%\n",
      "GT sentence: Continuously 67-100%\n",
      "\n",
      "Decoded sentence: Condinusuol 67-10%\n",
      "\n",
      "-\n",
      "Input sentence: Claim Event Iforamhtio\n",
      "GT sentence: Claim Event Information\n",
      "\n",
      "Decoded sentence: Claim Event Information\n",
      "\n",
      "-\n",
      "Input sentence: Patient haying increased pan ni loewr batck. All on lef tside.\n",
      "GT sentence: Patient haying increased pain in lower back. All on left side.\n",
      "\n",
      "Decoded sentence: Patient haying increased patient back. Allewr bation lef the fon lef the fon lef the fon lef the fon\n",
      "-\n",
      "Input sentence: Is your claim pending a Workere'Cemponeallorl decision? El Yes it: No\n",
      "GT sentence: Is your claim pending a Worker Compensation decision? Yes No\n",
      "\n",
      "Decoded sentence: Is your claim pending a Workeres eccistory decision? Yes No\n",
      "\n",
      "-\n",
      "Input sentence: Other instructinos/limitations\n",
      "GT sentence: Other instructions/limitations\n",
      "\n",
      "Decoded sentence: Other instructions limitations\n",
      "\n",
      "-\n",
      "Input sentence: Co-pauy\n",
      "GT sentence: Co-pay\n",
      "\n",
      "Decoded sentence: Co-pay\n",
      "\n",
      "-\n",
      "Input sentence: SUN CITY\n",
      "GT sentence: SUN CITY\n",
      "\n",
      "Decoded sentence: SUN CITY\n",
      "\n",
      "-\n",
      "Input sentence: Cheief Complaint\n",
      "GT sentence: Chief Complaint\n",
      "\n",
      "Decoded sentence: Cheif Complaint\n",
      "\n",
      "-\n",
      "Input sentence: John J Lark'rn, MD\n",
      "GT sentence: John J Larkin, MD\n",
      "\n",
      "Decoded sentence: Johe J Larkin, MD\n",
      "\n",
      "-\n",
      "Input sentence: FROM: Ext.#\n",
      "GT sentence: FROM: Ext. #\n",
      "\n",
      "Decoded sentence: FROM: Ext. #\n",
      "\n",
      "-\n",
      "Input sentence: The Hospiatlof rSpecial Surgery\n",
      "GT sentence: The Hospital for Special Surgery\n",
      "\n",
      "Decoded sentence: The Hospital Specialty Surgery\n",
      "\n",
      "-\n",
      "Input sentence: eRachAbove Right Svhoulder\n",
      "GT sentence: Reach Above Right Shoulder\n",
      "\n",
      "Decoded sentence: Rech Above Right Svoulder\n",
      "\n",
      "-\n",
      "Input sentence: Physicia nSignatukre Date\n",
      "GT sentence: Physician Signature Date\n",
      "\n",
      "Decoded sentence: Physician Signature Date\n",
      "\n",
      "-\n",
      "Input sentence: UuNUM LIFE INSURANCECOMPANY OF MREICA 2211 CNGRESS ST PORTADN ME 041f22-002\n",
      "GT sentence: UNUM LIFE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "\n",
      "Decoded sentence: UNUM LIFE INSURANCECCOMPANY OF AMREICA 2211 CONGRESS ST PORTION 0412112-0002\n",
      "\n",
      "-\n",
      "Input sentence: I Medical Center Drive\n",
      "GT sentence: 1 Medical Center Drive\n",
      "\n",
      "Decoded sentence: Medical Center Drive\n",
      "\n",
      "-\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Surg el')lnf01mati0n\n",
      "GT sentence: Surgery Information\n",
      "\n",
      "Decoded sentence: Surgery Information\n",
      "\n",
      "-\n",
      "Input sentence: PLEASE REMEMBR: Yo are them ost important factor in oyu rredcovery.u\n",
      "GT sentence: PLEASE REMEMBER: You are the most important factor in your recovery.\n",
      "\n",
      "Decoded sentence: PLEASE REMEMBRY: Yos are the fist mortant factor in oul rred covery.\n",
      "\n",
      "-\n",
      "Input sentence: Cslaim Detail\n",
      "GT sentence: Claim Detail\n",
      "\n",
      "Decoded sentence: Claim Detail\n",
      "\n",
      "-\n",
      "Input sentence: Paid Amouvnt\n",
      "GT sentence: Paid Amount\n",
      "\n",
      "Decoded sentence: Paid Amount\n",
      "\n",
      "-\n",
      "Input sentence: Date of Next iVsit(mm/dd/yy)\n",
      "GT sentence: Date of Next Visit (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Date of Next Visit (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: Surgical Procedure \n",
      "GT sentence: Surgical Procedure \n",
      "\n",
      "Decoded sentence: Surgical Procedure \n",
      "\n",
      "-\n",
      "Input sentence: Postal Code:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code:\n",
      "\n",
      "-\n",
      "Input sentence: Sign up for paperless at myFirstChoice.fchn.com\n",
      "GT sentence: Sign up for paperless at myFirstChoice.fchn.com\n",
      "\n",
      "Decoded sentence: Sign up for paperless at myFirstChoice.fchn.com\n",
      "\n",
      "-\n",
      "Input sentence: CnLAIM 1 TOAL:\n",
      "GT sentence: CLAIM 1 TOTAL:\n",
      "\n",
      "Decoded sentence: CLAIM 1 TOIL:\n",
      "\n",
      "-\n",
      "Input sentence: Electonically Signed Indicator: Yes\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "\n",
      "Decoded sentence: Electronically Signed Indicator: Yes\n",
      "\n",
      "-\n",
      "Input sentence: Page 3\n",
      "GT sentence: Page 3\n",
      "\n",
      "Decoded sentence: Page 3\n",
      "\n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "\n",
      "Decoded sentence: EE Name:\n",
      "\n",
      "-\n",
      "Input sentence: Heart Rate: 64\n",
      "GT sentence: Heart Rate: 64\n",
      "\n",
      "Decoded sentence: Heart Rate: 64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "GT sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "\n",
      "Decoded sentence: Responsible Provider: D. Thompson McGuire MD\n",
      "\n",
      "-\n",
      "Input sentence: Paitient:\n",
      "GT sentence: Patient:\n",
      "\n",
      "Decoded sentence: Patient:\n",
      "\n",
      "-\n",
      "Input sentence: Address\n",
      "GT sentence: Address\n",
      "\n",
      "Decoded sentence: Address\n",
      "\n",
      "-\n",
      "Input sentence: Past edicla History:\n",
      "GT sentence: Past Medical History:\n",
      "\n",
      "Decoded sentence: Past dical History:\n",
      "\n",
      "-\n",
      "Input sentence: yClaim EvetnI nformaiton\n",
      "GT sentence: Claim Event Information\n",
      "\n",
      "Decoded sentence: Claim Event Information\n",
      "\n",
      "-\n",
      "Input sentence: Medical Problems:\n",
      "GT sentence: Medical Problems:\n",
      "\n",
      "Decoded sentence: Medical Problems:\n",
      "\n",
      "-\n",
      "Input sentence: Provider Last Name Holm\n",
      "GT sentence: Provider Last Name: Holm\n",
      "\n",
      "Decoded sentence: Provider Last Name Molmplome\n",
      "\n",
      "-\n",
      "Input sentence: I yes, have you received Workers Compensatoin benefits for yuor ocucpational injury? Yes No\n",
      "GT sentence: If yes, have you received Workers Compensation benefits for your occupational injury? Yes No\n",
      "\n",
      "Decoded sentence: If yes, have you received Workers Compensation benefits for ocupational injury? Yes No\n",
      "\n",
      "-\n",
      "Input sentence: Date Signed\n",
      "GT sentence: Date Signed\n",
      "\n",
      "Decoded sentence: Date Signed\n",
      "\n",
      "-\n",
      "Input sentence: G. Tax consideraitons\n",
      "GT sentence: G. Tax considerations\n",
      "\n",
      "Decoded sentence: G. Tax considerations\n",
      "\n",
      "-\n",
      "Input sentence: Climb stairs/stpe/sadydre\n",
      "GT sentence: Climb stairs/steps/ladders\n",
      "\n",
      "Decoded sentence: Claim statisted/staped states/\n",
      "\n",
      "-\n",
      "Input sentence: Patient ED:\n",
      "GT sentence: Patient ID:\n",
      "\n",
      "Decoded sentence: Patient ID:\n",
      "\n",
      "-\n",
      "Input sentence: Page: 1of 6\n",
      "GT sentence: Page: 1 of 6\n",
      "\n",
      "Decoded sentence: Page: 1 of 6\n",
      "\n",
      "-\n",
      "Input sentence: First treatment date  03/21/2018\n",
      "GT sentence: First treatment date - 03/21/2018\n",
      "\n",
      "Decoded sentence: First treatment date - 03/21/2018\n",
      "\n",
      "-\n",
      "Input sentence: Date ofVisit/Admissiorr\n",
      "GT sentence: Date of Visit/Admission:\n",
      "\n",
      "Decoded sentence: Date of Visit/Admission\n",
      "\n",
      "-\n",
      "Input sentence: No acute fracture.\n",
      "GT sentence: No acute fracture.\n",
      "\n",
      "Decoded sentence: No acute fracture.\n",
      "\n",
      "-\n",
      "Input sentence: DOCUMENT NAME: ED Vital Signls and Painv - Tex\n",
      "\n",
      "GT sentence: DOCUMENT NAME: ED Vital Signs and Pain - Text\n",
      "\n",
      "Decoded sentence: DOCUMENT NAME: ED Vital Signis and Pain - Tex:\n",
      "\n",
      "-\n",
      "Input sentence: Last Name uSffix First Namel  MI\n",
      "GT sentence: Last Name Suffix First Name  MI\n",
      "\n",
      "Decoded sentence: Last Name Suffix First Name\n",
      "\n",
      "-\n",
      "Input sentence: nIsureds Signature Date Signedm\n",
      "GT sentence: Insureds Signature Date Signed\n",
      "\n",
      "Decoded sentence: Insureds Signature Date Signed\n",
      "\n",
      "-\n",
      "Input sentence: Middle Ne/Initial\n",
      "GT sentence: Middle Name/Initial:\n",
      "\n",
      "Decoded sentence: Middle Ne/Initial\n",
      "\n",
      "-\n",
      "Input sentence: CLAIMANT INFOMATION\n",
      "GT sentence: CLAIMANT INFORMATION\n",
      "\n",
      "Decoded sentence: CLAIMANT INFORMATION\n",
      "\n",
      "-\n",
      "Input sentence: RoomiEed EDGSCCEEDGSCC\n",
      "GT sentence: Room/Bed EDGSCC/EDGSCC\n",
      "\n",
      "Decoded sentence: Roomal EDGSCCEDIGS\n",
      "\n",
      "-\n",
      "Input sentence: Detaisl\n",
      "GT sentence: Details\n",
      "\n",
      "Decoded sentence: Detail\n",
      "\n",
      "-\n",
      "Input sentence: St. Joseph Hospital\n",
      "GT sentence: St. Joseph Hospital\n",
      "\n",
      "Decoded sentence: St. Joseph Hospital\n",
      "\n",
      "-\n",
      "Input sentence: St. Joseph Hospital\n",
      "GT sentence: St. Joseph Hospital\n",
      "\n",
      "Decoded sentence: St. Joseph Hospital\n",
      "\n",
      "-\n",
      "Input sentence: oTtasl For Claim\n",
      "GT sentence: Totals For Claim\n",
      "\n",
      "Decoded sentence: Total For Claim\n",
      "\n",
      "-\n",
      "Input sentence: Postl Coed:b\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code: \n",
      "\n",
      "-\n",
      "Input sentence: DEAPRTMENT FAYETEIVLLE\n",
      "GT sentence: DEPARTMENT FAYETTEVILLE\n",
      "\n",
      "Decoded sentence: DEAPRTMENT FAYETTERETILLE\n",
      "\n",
      "-\n",
      "Input sentence: Date of First Visit:\n",
      "GT sentence: Date of First Visit:\n",
      "\n",
      "Decoded sentence: Date of First Visit:\n",
      "\n",
      "-\n",
      "Input sentence: Occasional 11-33%i\n",
      "GT sentence: Occasional 11-33%\n",
      "\n",
      "Decoded sentence: Occasional 11-33%\n",
      "\n",
      "-\n",
      "Input sentence: Address Line 1:\n",
      "GT sentence: Address Line 1:\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: UUT\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: TE'LEPHO'NE' .\" ' .\n",
      "GT sentence: TELEPHONE\n",
      "\n",
      "Decoded sentence: TELEPHONE PONTE COD\n",
      "\n",
      "-\n",
      "Input sentence: Childa On & Off-Jbo Acc Ma y1, 201y5\n",
      "GT sentence: Child On & Off-Job Acc May 1, 2015\n",
      "\n",
      "Decoded sentence: Child On & Off-Job Acc May 1, 2015\n",
      "\n",
      "-\n",
      "Input sentence: Cu. Compleet this esctiuon forH OSPITAL CONFIENMENT/INTENSIVE CARE BENzEFITj lcaim\n",
      "GT sentence: C. Complete this section for HOSPITAL CONFINEMENT/INTENSIVE CARE BENEFIT claims\n",
      "\n",
      "Decoded sentence: C. Complete this section for HOSPITAL CONFINEMENT/INTENSIVE CARE BENESIT TO DITAL INSTIVE INSTIVE IN\n",
      "-\n",
      "Input sentence: Never n0%\n",
      "GT sentence: Never 0%\n",
      "\n",
      "Decoded sentence: Never 10%\n",
      "\n",
      "-\n",
      "Input sentence: BALANCE UE\n",
      "GT sentence: BALANCE DUE\n",
      "\n",
      "Decoded sentence: BALANCE URE\n",
      "\n",
      "-\n",
      "Input sentence: PAYMENS ON 03/01/2018\n",
      "GT sentence: PAYMENTS ON 03/01/2018\n",
      "\n",
      "Decoded sentence: PAYMENS NO 03/01/2018\n",
      "\n",
      "-\n",
      "Input sentence: [5 claim being led by ER/HR? N0\n",
      "GT sentence: Is claim being filed by ER/HR? No\n",
      "\n",
      "Decoded sentence: Is alaim being filed by ID\n",
      "\n",
      "-\n",
      "Input sentence: Home:\n",
      "GT sentence: Home:\n",
      "\n",
      "Decoded sentence: Home:\n",
      "\n",
      "-\n",
      "Input sentence: (IO-SIGN PROVIDER:\n",
      "GT sentence: CO-SIGN PROVIDER:\n",
      "\n",
      "Decoded sentence: ICOSIGN PROVIDER:\n",
      "\n",
      "-\n",
      "Input sentence: 1. Assessment Patient Pian Low back pain (M545).\n",
      "GT sentence: 1. Assessment Patient Plan Low back pain (M545).\n",
      "\n",
      "Decoded sentence: 1. Assessment Patient Pain Low back pain (M5/5).\n",
      "\n",
      "-\n",
      "Input sentence: Appointmetn Wit\n",
      "GT sentence: Appointment With\n",
      "\n",
      "Decoded sentence: Appointment Witwent With\n",
      "\n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "\n",
      "Decoded sentence: EE Name:\n",
      "\n",
      "-\n",
      "Input sentence: SMOKING STATUS - .\n",
      "GT sentence: SMOKING STATUS\n",
      "\n",
      "Decoded sentence: SMONING STATUS - .\n",
      "\n",
      "-\n",
      "Input sentence: Sedentary Work u(liting cmpaximmu of 10 lbs)e\n",
      "GT sentence: Sedentary Work (lifting maximum of 10 lbs)\n",
      "\n",
      "Decoded sentence: Sedentary Work (cmpling complimm of 10 lbs)\n",
      "\n",
      "-\n",
      "Input sentence: Reatiosn to Pt Sel\n",
      "\n",
      "GT sentence: Relation to Pt Self\n",
      "\n",
      "Decoded sentence: Reation to Pt SelM\n",
      "\n",
      "-\n",
      "Input sentence: Conrtact Ajusmtent\n",
      "GT sentence: Contract Adjustment\n",
      "\n",
      "Decoded sentence: Contract Adjustment\n",
      "\n",
      "-\n",
      "Input sentence: Authorization toC ollect nadDisclose Infrmation\n",
      "GT sentence: Authorization to Collect and Disclose Information\n",
      "\n",
      "Decoded sentence: Authorization to Collect and Disclose Information\n",
      "\n",
      "-\n",
      "Input sentence: Neurologicnal: no neurological symptos.\n",
      "GT sentence: Neurological: no neurological symptoms.\n",
      "\n",
      "Decoded sentence: Neurological: no neurological symptoms.\n",
      "\n",
      "-\n",
      "Input sentence: Ttoal mployee Bi-Weekyl Payroll Deduction:q\n",
      "GT sentence: Total Employee Bi-Weekly Payroll Deduction:\n",
      "\n",
      "Decoded sentence: Total Employee Bi-Weekly Payroll Deduction:\n",
      "\n",
      "-\n",
      "Input sentence: 02/02/18 27405 REPAIR, PRIMARY, OTRN LIGAMNET A\n",
      "GT sentence: 02/02/18 27405 REPAIR, PRIMARY, TORN LIGAMENT A\n",
      "\n",
      "Decoded sentence: 02/02/18 2015 REPAIR, PRIMARY, TRIMARINT ANT ANGURY TA\n",
      "\n",
      "-\n",
      "Input sentence: Tl pho e b - ~. _\n",
      "GT sentence: Telephone Number \n",
      "\n",
      "Decoded sentence: The phove Chob c\n",
      "\n",
      "-\n",
      "Input sentence: Indigviudual Summary\n",
      "GT sentence: Individual Summary\n",
      "\n",
      "Decoded sentence: Individual Summary\n",
      "\n",
      "-\n",
      "Input sentence: Date of suurgery\n",
      "GT sentence: Date of surgery:\n",
      "\n",
      "Decoded sentence: Date of surgery\n",
      "\n",
      "-\n",
      "Input sentence: FAYETI'EVILLE . printed\n",
      "GT sentence: FAYETTEVILLE printed\n",
      "\n",
      "Decoded sentence: FAYETTEVILE Sprinted\n",
      "\n",
      "-\n",
      "Input sentence: PARTz :I TOBE COMPLETED BYI NSURED/PATETN\n",
      "GT sentence: PART I: TO BE COMPLETED BY INSURED/PATIENT\n",
      "\n",
      "Decoded sentence: PART IN TOBE COMPLETED BY INSURED/PATIENT\n",
      "\n",
      "-\n",
      "Input sentence: oaniiioiaTr'yis\n",
      "GT sentence: TWIN CITIES ORTHOPEDICS\n",
      "\n",
      "Decoded sentence: TWIN CITTERTHOPERTICST\n",
      "\n",
      "-\n",
      "Input sentence: Date of Visit/Admission:\n",
      "GT sentence: Date of Visit/Admission:\n",
      "\n",
      "Decoded sentence: Date of Visit/Admission:\n",
      "\n",
      "-\n",
      "Input sentence: Studly Result\n",
      "GT sentence: Study Result\n",
      "\n",
      "Decoded sentence: Studed/Result\n",
      "\n",
      "-\n",
      "Input sentence: Skinno ksin sydmptoms.\n",
      "GT sentence: Skin no skin symptoms.\n",
      "\n",
      "Decoded sentence: Skin ksin sydmptoms.\n",
      "\n",
      "-\n",
      "Input sentence: Z87819 ?\n",
      "GT sentence: Z87891 ?\n",
      "\n",
      "Decoded sentence: Z8789 ?\n",
      "\n",
      "-\n",
      "Input sentence: hPage3 f 3\n",
      "GT sentence: Page 3 of 3\n",
      "\n",
      "Decoded sentence: Phage of C\n",
      "\n",
      "-\n",
      "Input sentence: Denies skin leions, rcash, molek, poo wound healing, and ychanges in olorv of skin.\n",
      "GT sentence: Denies skin lesions, rash, mole, poor wound healing, and changes in color of skin.\n",
      "\n",
      "Decoded sentence: Denies skin ledions, rach, molek, poo wound in ochanges in ollor of sing of sing.\n",
      "\n",
      "-\n",
      "Input sentence: Postal odqe:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code:\n",
      "\n",
      "-\n",
      "Input sentence: [MT-[AL STATE OF MAINE\n",
      "GT sentence: STATE OF MAINE\n",
      "\n",
      "Decoded sentence: STON STATE OF MAINE\n",
      "\n",
      "-\n",
      "Input sentence: INSUR. PENDING\n",
      "GT sentence: INSUR. PENDING\n",
      "\n",
      "Decoded sentence: INSUR. PENIING\n",
      "\n",
      "-\n",
      "Input sentence: History of Present Illness:\n",
      "GT sentence: History of Present Illness:\n",
      "\n",
      "Decoded sentence: History of Present Illness:\n",
      "\n",
      "-\n",
      "Input sentence: Lats Naem:\n",
      "GT sentence: Last Name:\n",
      "\n",
      "Decoded sentence: Last Name:\n",
      "\n",
      "-\n",
      "Input sentence: Ptrocedure Descrfption\n",
      "GT sentence: Procedure Description\n",
      "\n",
      "Decoded sentence: Procedure Description\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: xHate you stopped pworking? Yes N oIf yes, wht was the last day that you worked? (mm/ddyy)\n",
      "GT sentence: Have you stopped working? Yes No If yes, what was the last day that you worked? (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: Hate ou stopped popperking? Yes No If yes, what you worked? (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: Unum is a regnisdtered trademark and marketnig brnad of Unum Group and ts insuirng subsidiaries.\n",
      "GT sentence: Unum is a registered trademark and marketing brand of Unum Group and its insuring subsidiaries.\n",
      "\n",
      "Decoded sentence: Unum is a reginistered trademark and marketing brand of Unum Group and its insuring subsidiaries.\n",
      "\n",
      "-\n",
      "Input sentence: ALLOWED MOUNT\n",
      "GT sentence: ALLOWED AMOUNT\n",
      "\n",
      "Decoded sentence: ALLOWED MOUNT\n",
      "\n",
      "-\n",
      "Input sentence: Moderat eWork( liftingmaximum of 30i lbs)\n",
      "GT sentence: Moderate Work (lifting maximum of 30 lbs)\n",
      "\n",
      "Decoded sentence: Moderate Work (lifting maximum of 30 lbs)\n",
      "\n",
      "-\n",
      "Input sentence: eidcal Proidesr olse: Treatinhg\n",
      "GT sentence: Medical Provider Roles: Treating\n",
      "\n",
      "Decoded sentence: Tedact Provider Provider Treating\n",
      "\n",
      "-\n",
      "Input sentence: State/Province:\n",
      "GT sentence: State/Province:\n",
      "\n",
      "Decoded sentence: State/Province:\n",
      "\n",
      "-\n",
      "Input sentence: No School :\n",
      "GT sentence: No School :\n",
      "\n",
      "Decoded sentence: No School :\n",
      "\n",
      "-\n",
      "Input sentence: Reptr Redquest ID: 9612087\n",
      "GT sentence: Report Request ID: 49612087\n",
      "\n",
      "Decoded sentence: Report Redurt ID: S612087\n",
      "\n",
      "-\n",
      "Input sentence: Postal ode:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Postal Code:\n",
      "\n",
      "-\n",
      "Input sentence: Addres Line 1: \n",
      "GT sentence: Address Line 1: \n",
      "\n",
      "Decoded sentence: Addres Line 1: \n",
      "\n",
      "-\n",
      "Input sentence: Dales of Service {including Confinemeni)\n",
      "GT sentence: Dates of Service (including Confinement)\n",
      "\n",
      "Decoded sentence: Dates of Service (including Confinement) \n",
      "\n",
      "-\n",
      "Input sentence: b.Epiisodiic flra eps:\n",
      "GT sentence: b. Episodic flare ups:\n",
      "\n",
      "Decoded sentence: Emplisodic for and Repera es:\n",
      "\n",
      "-\n",
      "Input sentence: TCO Vitals Signs Panel\n",
      "GT sentence: TCO Vitals Signs Panel\n",
      "\n",
      "Decoded sentence: TCO Vitals Signs Panel\n",
      "\n",
      "-\n",
      "Input sentence: DATE CIF OPERATION:\n",
      "GT sentence: DATE OF OPERATION:\n",
      "\n",
      "Decoded sentence: DATE CIF OPERATION:\n",
      "\n",
      "-\n",
      "Input sentence: Addrss\n",
      "GT sentence: Address\n",
      "\n",
      "Decoded sentence: Address\n",
      "\n",
      "-\n",
      "Input sentence: SERVICE IEN #\n",
      "GT sentence: SERVICE LINE #\n",
      "\n",
      "Decoded sentence: SERVICE INS #\n",
      "\n",
      "-\n",
      "Input sentence: Do oyu have healthinsurance with yfour employer? Yes\n",
      "GT sentence: Do you have health insurance with your employer? Yes\n",
      "\n",
      "Decoded sentence: Do you have healthinsurance with y foure employer? Yes\n",
      "\n",
      "-\n",
      "Input sentence: First Name:\n",
      "GT sentence: First Name:\n",
      "\n",
      "Decoded sentence: First Name:\n",
      "\n",
      "-\n",
      "Input sentence: 1. Right nee diganostic arthroscopy.\n",
      "GT sentence: 1. Right knee diagnostic arthroscopy.\n",
      "\n",
      "Decoded sentence: 1. Right knee diganostic arthroscopy.\n",
      "\n",
      "-\n",
      "Input sentence: S P E C IA  L  I N S T R Uj C T I O N S\n",
      "GT sentence: S P E C I A L  I N S T R U C T I O N S\n",
      "\n",
      "Decoded sentence: S P E C IN L INS INS RO DeT C N T I N S\n",
      "\n",
      "-\n",
      "Input sentence: City:\n",
      "GT sentence: City:\n",
      "\n",
      "Decoded sentence: City:\n",
      "\n",
      "-\n",
      "Input sentence: he above statements are true and comptlete ot the bst of my knowledbge and blief.\n",
      "GT sentence: The above statements are true and complete to the best of my knowledge and belief.\n",
      "\n",
      "Decoded sentence: est bove statements are true and complete to the bst of my knowled belief.\n",
      "\n",
      "-\n",
      "Input sentence: Group Poligcye #\n",
      "GT sentence: Group Policy #:\n",
      "\n",
      "Decoded sentence: Group Policy #\n",
      "\n",
      "-\n",
      "Input sentence: 27984\n",
      "GT sentence: Z7984 ?\n",
      "\n",
      "Decoded sentence: Z7984\n",
      "\n",
      "-\n",
      "Input sentence: Last Name:\n",
      "GT sentence: Last Name:\n",
      "\n",
      "Decoded sentence: Last Name:\n",
      "\n",
      "-\n",
      "Input sentence: atient Naem:e\n",
      "GT sentence: Patient Name:\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: Admit Type: Admit Date:\n",
      "GT sentence: Admit Type: Select... Admit Date:\n",
      "\n",
      "Decoded sentence: Admit Type: Admitien\n",
      "\n",
      "-\n",
      "Input sentence: Resdpiratory: eNgativ for cough, sohrtness of breath uand wheezing.\n",
      "GT sentence: Respiratory: Negative for cough, shortness of breath and wheezing.\n",
      "\n",
      "Decoded sentence: Respiratory: Negativ for cough, sorthess of breath und wreath und wheraing.\n",
      "\n",
      "-\n",
      "Input sentence: Data Surgery Parforrnad (mmi'ddlyy):\n",
      "GT sentence: Date Surgery Performed (mm/dd/yy):\n",
      "\n",
      "Decoded sentence: Date Surgery Parformad (mm/dd/yy):\n",
      "\n",
      "-\n",
      "Input sentence: 2 .No alute osesous abnormaliy identxfied.\n",
      "GT sentence: 2. No acute osseous abnormality identified.\n",
      "\n",
      "Decoded sentence: 2. No alute oss abnormality indentified.\n",
      "\n",
      "-\n",
      "Input sentence: ICD-10 Codeb:S89q.92XtA 2nd ICD-10 Cde:\n",
      "GT sentence: ICD-10 Code: S89.92XA 2nd ICD-10 Code:\n",
      "\n",
      "Decoded sentence: ICD-10 Code: S89.92-102 ICD-10 - Code:\n",
      "\n",
      "-\n",
      "Input sentence: Marshfiel Clqinc - Marshfield Cneter\n",
      "GT sentence: Marshfield Clinic - Marshfield Center\n",
      "\n",
      "Decoded sentence: Marshfiel Clince - Marishfield Conters\n",
      "\n",
      "-\n",
      "Input sentence: Acession No:\n",
      "GT sentence: Accession No:\n",
      "\n",
      "Decoded sentence: Accession No:\n",
      "\n",
      "-\n",
      "Input sentence: Ditagnosis Code (CD) \n",
      "GT sentence: Diagnosis Code (ICD) \n",
      "\n",
      "Decoded sentence: Ditagnosis Code (IDD\n",
      "\n",
      "-\n",
      "Input sentence: Stop Date\n",
      "GT sentence: Stop Date\n",
      "\n",
      "Decoded sentence: Stop Date\n",
      "\n",
      "-\n",
      "Input sentence: Diagnosis Description\n",
      "GT sentence: Diagnosis Description\n",
      "\n",
      "Decoded sentence: Diagnosis Description\n",
      "\n",
      "-\n",
      "Input sentence: CL-1023(02/17) 7\n",
      "GT sentence: CL-1023(02/17) 7\n",
      "\n",
      "Decoded sentence: CL-1023(02/17) 7\n",
      "\n",
      "-\n",
      "Input sentence: Fho ne\n",
      "GT sentence: Phone\n",
      "\n",
      "Decoded sentence: Fhon \n",
      "\n",
      "-\n",
      "Input sentence: Transaction identier:\n",
      "GT sentence: Transaction identifier:\n",
      "\n",
      "Decoded sentence: Transaction identifier:\n",
      "\n",
      "-\n",
      "Input sentence: Fractures\n",
      "GT sentence: Fractures\n",
      "\n",
      "Decoded sentence: Fractures\n",
      "\n",
      "-\n",
      "Input sentence: Exercise: yes\n",
      "GT sentence: Exercise: yes\n",
      "\n",
      "Decoded sentence: Exercise: yes\n",
      "\n",
      "-\n",
      "Input sentence: Hospital Name:\n",
      "GT sentence: Hospital Name:\n",
      "\n",
      "Decoded sentence: Hospital Name:\n",
      "\n",
      "-\n",
      "Input sentence: Mejdical Provide rInformaotion - kHospitalliatio\n",
      "GT sentence: Medical Provider Information - Hospitalization\n",
      "\n",
      "Decoded sentence: Medical Provider Information - Hospitalization\n",
      "\n",
      "-\n",
      "Input sentence: SEE REVERSE SIDE FOR IMPORTANT BILLING I FORMATION \n",
      "GT sentence: SEE REVERSE SIDE FOR IMPORTANT BILLING INFORMATION\n",
      "\n",
      "Decoded sentence: SEE REVERSE SIDE FOR IMPORING INSURING INFORMATION \n",
      "\n",
      "-\n",
      "Input sentence: Call tolzl-rfxeeM onda ythrough Friday, 8 a.m. to 8 p.m. (Eastern Time.\n",
      "GT sentence: Call toll-free Monday through Friday, 8 a.m. to 8 p.m. (Eastern Time).\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through Friday, 8 a.m. to 8 p.m. (Eastern Time.\n",
      "\n",
      "-\n",
      "Input sentence: Total:\n",
      "GT sentence: Total:\n",
      "\n",
      "Decoded sentence: Total:\n",
      "\n",
      "-\n",
      "Input sentence: Adndress Lien1 :\n",
      "GT sentence: Address Line 1:\n",
      "\n",
      "Decoded sentence: Address Line 1 :\n",
      "\n",
      "-\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "\n",
      "Decoded sentence: Gender:\n",
      "\n",
      "-\n",
      "Input sentence: Employer N ame:\n",
      "GT sentence: Employer Name:\n",
      "\n",
      "Decoded sentence: Employer Name:\n",
      "\n",
      "-\n",
      "Input sentence: SERVICE LINE #\n",
      "GT sentence: SERVICE LINE #\n",
      "\n",
      "Decoded sentence: SERVICE LINE #\n",
      "\n",
      "-\n",
      "Input sentence: The Beneits Center\n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: The Benefits Center\n",
      "\n",
      "-\n",
      "Input sentence: Is the caller thei employyee? Yes\n",
      "GT sentence: Is the caller the employee? Yes\n",
      "\n",
      "Decoded sentence: Is the caller the Deployee? Yes\n",
      "\n",
      "-\n",
      "Input sentence: Acceassion No:\n",
      "GT sentence: Accession No:\n",
      "\n",
      "Decoded sentence: Acceassion No:\n",
      "\n",
      "-\n",
      "Input sentence: Meicla Specialty eDgree\n",
      "GT sentence: Medical Specialty Degree\n",
      "\n",
      "Decoded sentence: Medical Specialty Degree\n",
      "\n",
      "-\n",
      "Input sentence: Fiirst Choice Health\n",
      "GT sentence: First Choice Health\n",
      "\n",
      "Decoded sentence: First Choice Health\n",
      "\n",
      "-\n",
      "Input sentence: Certication of Health Care Provider for Employees Serious Heaith Condltlon\n",
      "GT sentence: Certification of Health Care Provider for Employees Serious Health Condition\n",
      "\n",
      "Decoded sentence: Certification of Health Care Provider for Employees Serious Health Condition\n",
      "\n",
      "-\n",
      "Input sentence: Arriavl tme\n",
      "GT sentence: Arrival time\n",
      "\n",
      "Decoded sentence: Arroral Name\n",
      "\n",
      "-\n",
      "Input sentence: Caffeine use: 0 rdinks per day\n",
      "GT sentence: Caffeine use: 0 drinks per day\n",
      "\n",
      "Decoded sentence: Caffeine use: 0 drinks per day\n",
      "\n",
      "-\n",
      "Input sentence: Page 5 f 5\n",
      "GT sentence: Page 5 of 5\n",
      "\n",
      "Decoded sentence: Page 5 fo\n",
      "\n",
      "-\n",
      "Input sentence: 2. 0.3 cm extrsusion ofz thle medial meniscal body.\n",
      "GT sentence: 2. 0.3 cm extrusion of the medial meniscal body.\n",
      "\n",
      "Decoded sentence: 2. 0.3 cm extrsusion of the medial meniscal body.\n",
      "\n",
      "-\n",
      "Input sentence: Atddrss\n",
      "GT sentence: Address\n",
      "\n",
      "Decoded sentence: Address\n",
      "\n",
      "-\n",
      "Input sentence: Group Policy #:\n",
      "GT sentence: Group Policy #:\n",
      "\n",
      "Decoded sentence: Group Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: iWho Th eReported Evenet Hwppbened To:h mEploeye/Policvyholder\n",
      "GT sentence: Who The Reported Event Happened To: Employee/Policyholder\n",
      "\n",
      "Decoded sentence: Who The Reported Event Heppened To: Employee/Policyholder\n",
      "\n",
      "-\n",
      "Input sentence: Birth Date:\n",
      "GT sentence: Birth Date:\n",
      "\n",
      "Decoded sentence: Birth Date:\n",
      "\n",
      "-\n",
      "Input sentence: Page 2\n",
      "GT sentence: Page 2\n",
      "\n",
      "Decoded sentence: Page 2\n",
      "\n",
      "-\n",
      "Input sentence: CL-1116 (11q/41)\n",
      "GT sentence: CL-1116 (11/14)\n",
      "\n",
      "Decoded sentence: CL-1116 (11/11)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Automated Attendatn\n",
      "GT sentence: Automated Attendant\n",
      "\n",
      "Decoded sentence: Automated Attendang\n",
      "\n",
      "-\n",
      "Input sentence: cTeh Benefits Ceterw\n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: The Benefits Centerw\n",
      "\n",
      "-\n",
      "Input sentence: Patient ID\n",
      "GT sentence: Patient ID\n",
      "\n",
      "Decoded sentence: Patient ID\n",
      "\n",
      "-\n",
      "Input sentence: Policyholder/Owner Infformation\n",
      "GT sentence: Policyholder/Owner Information\n",
      "\n",
      "Decoded sentence: Policyholder/Owner Information\n",
      "\n",
      "-\n",
      "Input sentence: grade 3 oepning t ovalgs at 30 degrels\n",
      "GT sentence: grade 3 opening to valgus at 30 degrees\n",
      "\n",
      "Decoded sentence: grade 3 opepning to the degrels\n",
      "\n",
      "-\n",
      "Input sentence: Co-nsurance\n",
      "GT sentence: Co-Insurance\n",
      "\n",
      "Decoded sentence: Co-nsurance\n",
      "\n",
      "-\n",
      "Input sentence: Diatgnostic Results:\n",
      "GT sentence: Diagnostic Results:\n",
      "\n",
      "Decoded sentence: Diagnostic Results:\n",
      "\n",
      "-\n",
      "Input sentence:  Idnetifies as femael genader\n",
      "GT sentence:  Identifies as female gender\n",
      "\n",
      "Decoded sentence:  Indetifies as femare Sendader\n",
      "\n",
      "-\n",
      "Input sentence: Ayccessoin\n",
      "\n",
      "GT sentence: Accession:\n",
      "\n",
      "Decoded sentence: Accessoint\n",
      "\n",
      "-\n",
      "Input sentence: IEDMONT HEATHCRAE\n",
      "GT sentence: PIEDMONT HEALTHCARE\n",
      "\n",
      "Decoded sentence: EMONT HEATHCRATHCRATICRA\n",
      "\n",
      "-\n",
      "Input sentence: 3 Family level maximums apply only when multiple family members are enrolled on the plan.\n",
      "GT sentence: Family level maximums apply only when multiple family members are enrolled on the plan.\n",
      "\n",
      "Decoded sentence:  Family level maximums apply only when multiple family members are enrolled on the plan.\n",
      "\n",
      "-\n",
      "Input sentence: CORONARY HEART DISEASE RISK\n",
      "GT sentence: CORONARY HEART DISEASE RISK\n",
      "\n",
      "Decoded sentence: CORONAR HEART DISERTIOSK\n",
      "\n",
      "-\n",
      "Input sentence: INSURER MAILING ADDRESS:\n",
      "GT sentence: INSURER MAILING ADDRESS:\n",
      "\n",
      "Decoded sentence: INSURER MAILING ADDRESS:\n",
      "\n",
      "-\n",
      "Input sentence: N .be m _ ' )\n",
      "GT sentence: Telephone Number\n",
      "\n",
      "Decoded sentence: No. 1 mal Sto\n",
      "\n",
      "-\n",
      "Input sentence: CL-1023(02/17) \n",
      "GT sentence: CL-1023(02/17) 7\n",
      "\n",
      "Decoded sentence: CL-1023(02/17) 7\n",
      "\n",
      "-\n",
      "Input sentence: is this oondiijon the result ofan accidental injury? ee D No\n",
      "GT sentence: Is this condition the result of an accidental injury? Yes No \n",
      "\n",
      "Decoded sentence: Is this condition the result of an accidental injury? No\n",
      "\n",
      "-\n",
      "Input sentence: Service Date: 02/09/2018 Procedrue: p29085 - Apply hand/wrist acst Modifier:s 58 RT\n",
      "GT sentence: Service Date: 02/09/2018 Procedure: 29085 - Apply hand/wrist cast Modifiers: 58 RT\n",
      "\n",
      "Decoded sentence: Service Date: 02/09/2018 Procedure: Procedure: 29.85 - Apply hand Modifier: 58 RT\n",
      "\n",
      "-\n",
      "Input sentence: LIwED PRIMAR YTO FIRS CHOICE HAELHT ADMIN (FI005)\n",
      "GT sentence: FILED PRIMARY TO FIRST CHOICE HEALTH ADMIN (FI005)\n",
      "\n",
      "Decoded sentence: LIDED PRIMARY TO FIRS CHOICE HALE T ID N(05).5)\n",
      "\n",
      "-\n",
      "Input sentence: Informatibonwritten in other langbuage\n",
      "\n",
      "GT sentence: Information written in other languages\n",
      "\n",
      "Decoded sentence: Information written in other lang buaner\n",
      "\n",
      "-\n",
      "Input sentence: Transaction reference number:\n",
      "GT sentence: Transaction reference number:\n",
      "\n",
      "Decoded sentence: Transaction reference number:\n",
      "\n",
      "-\n",
      "Input sentence: Methodof aPyment f\n",
      "GT sentence: Method of Payment \n",
      "\n",
      "Decoded sentence: Method of Payment forment forment forment forment forment forment forment forment forment forment fo\n",
      "-\n",
      "Input sentence:  Married\n",
      "GT sentence:  Married\n",
      "\n",
      "Decoded sentence:  Married\n",
      "\n",
      "-\n",
      "Input sentence: ostla Code:\n",
      "GT sentence: Postal Code:\n",
      "\n",
      "Decoded sentence: Sostal Code:\n",
      "\n",
      "-\n",
      "Input sentence: lEetcronically Sined aDteh:\n",
      "GT sentence: Electronically Signed Date:\n",
      "\n",
      "Decoded sentence: Electronically Signed Date:\n",
      "\n",
      "-\n",
      "Input sentence: Comparison made to\n",
      "GT sentence: Comparison made to\n",
      "\n",
      "Decoded sentence: Comparison made to\n",
      "\n",
      "-\n",
      "Input sentence: El Long Term Disability El Voluntary Benefits ConcorlGrilical Illness insurance\n",
      "GT sentence: Long Term Disability Voluntary Benefits Cancer/Critical Illness Insurance\n",
      "\n",
      "Decoded sentence: Lang Term Disability Voluntary Benefits Center Benefits Cinsurance Illness Insurance\n",
      "\n",
      "-\n",
      "Input sentence: lEectroncally iSged Idicaat:r Yes\n",
      "GT sentence: Electronically Signed Indicator: Yes\n",
      "\n",
      "Decoded sentence: Electronically Signed Indicate: Yes\n",
      "\n",
      "-\n",
      "Input sentence: hTis is hthe toal amount bilnled from the dautes ofservicce of 01/26/e2081 trhu 01/26/208.\n",
      "GT sentence: This is the total amount billed from the dates of service of 01/26/2018 thru 01/26/2018.\n",
      "\n",
      "Decoded sentence: This is the tolal amount billed from the daute of 01/26/2018 the thu 01/26/2018. thru 01/26/2018.\n",
      "\n",
      "-\n",
      "Input sentence: Ins: Mainecare\n",
      "GT sentence: Ins: Mainecare\n",
      "\n",
      "Decoded sentence: Ins: Mainecare\n",
      "\n",
      "-\n",
      "Input sentence: Ties per week: \n",
      "GT sentence: Times per week: 5\n",
      "\n",
      "Decoded sentence: Ties per week: \n",
      "\n",
      "-\n",
      "Input sentence: Patient MRN: \n",
      "GT sentence: Patient MRN: \n",
      "\n",
      "Decoded sentence: Patient MRN: \n",
      "\n",
      "-\n",
      "Input sentence: Birrth Date:\n",
      "GT sentence: Birth Date:\n",
      "\n",
      "Decoded sentence: Birth Date:\n",
      "\n",
      "-\n",
      "Input sentence: CL1-02(302/17 )7\n",
      "GT sentence: CL-1023(02/17) 7\n",
      "\n",
      "Decoded sentence: CL-1023(02/17) 7\n",
      "\n",
      "-\n",
      "Input sentence: Accident Work Related: No\n",
      "GT sentence: Accident Work Related: No\n",
      "\n",
      "Decoded sentence: Accident Work Related: No\n",
      "\n",
      "-\n",
      "Input sentence: Total Employee BiWeekly Payroll Deduction:\n",
      "GT sentence: Total Employee Bi-Weekly Payroll Deduction:\n",
      "\n",
      "Decoded sentence: Total Employee Bi-Weekly Payroll Deduction:\n",
      "\n",
      "-\n",
      "Input sentence: Patients Preferred Pain Tool  :Nmueric ating\n",
      "GT sentence: Patients Preferred Pain Tool : Numeric rating\n",
      "\n",
      "Decoded sentence: Patients Prefered Pain Tol Numberic ating\n",
      "\n",
      "-\n",
      "Input sentence: TIER 3 Fan1Ily 000001100\n",
      "GT sentence: TIER 3 Family Deductible\n",
      "\n",
      "Decoded sentence: TIER 3 Fan 10001100\n",
      "\n",
      "-\n",
      "Input sentence: Provider: David Feivo PA-\n",
      "GT sentence: Provider: David Feivor PA-C\n",
      "\n",
      "Decoded sentence: Provider: David Feive PA-\n",
      "\n",
      "-\n",
      "Input sentence: PREOP DIAGNSIS: Unnremittig ridght knee patellofenroral pain, patellar malalignment.\n",
      "GT sentence: PREOP DIAGNOSIS: Unremitting right knee patellofernoral pain, patellar malalignment.\n",
      "\n",
      "Decoded sentence: PREOP DIAGNSSIS: Unremitight knee patellar matellar malare allar mal allar mal allaignment.\n",
      "\n",
      "-\n",
      "Input sentence: Zp7984 ?\n",
      "GT sentence: Z7984 ?\n",
      "\n",
      "Decoded sentence: Z7984 ?\n",
      "\n",
      "-\n",
      "Input sentence: 3 viweso f the right ankle:\n",
      "GT sentence: 3 views of the right ankle:\n",
      "\n",
      "Decoded sentence: 3 viewes of the right ankle:\n",
      "\n",
      "-\n",
      "Input sentence: Middle Narmflnitial:\n",
      "GT sentence: Middle Name/Initial:\n",
      "\n",
      "Decoded sentence: Middle Name/Initial:\n",
      "\n",
      "-\n",
      "Input sentence: Spouse Off-Job Acc January 1, 2018 \n",
      "GT sentence: Spouse Off-Job Acc January 1, 2018\n",
      "\n",
      "Decoded sentence: Spouse Off-Job Acc January 1, 2018 \n",
      "\n",
      "-\n",
      "Input sentence: Insured Coverme TIE W Coverage\n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "\n",
      "Decoded sentence: Insured Coverage Type Coverage\n",
      "\n",
      "-\n",
      "Input sentence: Employer:\n",
      "GT sentence: Employer:\n",
      "\n",
      "Decoded sentence: Employer:\n",
      "\n",
      "-\n",
      "Input sentence: S mte/Irou'nee :\n",
      "GT sentence: State/Province :\n",
      "\n",
      "Decoded sentence: State/Provinee :\n",
      "\n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "\n",
      "Decoded sentence: EE Name:\n",
      "\n",
      "-\n",
      "Input sentence: Variawble Sckhedule: Yes\n",
      "GT sentence: Variable Schedule: Yes\n",
      "\n",
      "Decoded sentence: Variable Schhedule: Yes\n",
      "\n",
      "-\n",
      "Input sentence: POERATIV REPORTw\n",
      "GT sentence: OPERATIVE REPORT\n",
      "\n",
      "Decoded sentence: POPERATIV REPORT\n",
      "\n",
      "-\n",
      "Input sentence: 3 views ofthe right foot\n",
      "GT sentence: 3 views of the right foot\n",
      "\n",
      "Decoded sentence: 3 views of the right foot\n",
      "\n",
      "-\n",
      "Input sentence: -5DIrGIT 57701\n",
      "GT sentence: 5-DIGIT 57701\n",
      "\n",
      "Decoded sentence: 5DIGIT 57701\n",
      "\n",
      "-\n",
      "Input sentence: Selected Coverages:  LMS STD\n",
      "GT sentence: Selected Coverages: - LMS STD\n",
      "\n",
      "Decoded sentence: Selected Coverages: - LMS ST\n",
      "\n",
      "-\n",
      "Input sentence: Remarks \n",
      "GT sentence: Remarks \n",
      "\n",
      "Decoded sentence: Remarks \n",
      "\n",
      "-\n",
      "Input sentence: Signaures\n",
      "GT sentence: Signatures\n",
      "\n",
      "Decoded sentence: Signature\n",
      "\n",
      "-\n",
      "Input sentence: Claimant Addresses\n",
      "GT sentence: Claimant Addresses\n",
      "\n",
      "Decoded sentence: Claimant Addresses\n",
      "\n",
      "-\n",
      "Input sentence: 1. No Known Allergies\n",
      "GT sentence: 1. No Known Allergies\n",
      "\n",
      "Decoded sentence: 1. No Known Allergies\n",
      "\n",
      "-\n",
      "Input sentence: Transaction typo:\n",
      "GT sentence: Transaction type:\n",
      "\n",
      "Decoded sentence: Transaction typo:\n",
      "\n",
      "-\n",
      "Input sentence: DEPARTMENT- FAYETTEVILLE\n",
      "GT sentence: DEPARTMENT FAYETTEVILLE\n",
      "\n",
      "Decoded sentence: DEPARTMENT - FAYETTEVILLE\n",
      "\n",
      "-\n",
      "Input sentence: Office iVsit\n",
      "GT sentence: Office Visit\n",
      "\n",
      "Decoded sentence: Office Visit\n",
      "\n",
      "-\n",
      "Input sentence: AlCTgIVE: ablujterol sulfate\n",
      "GT sentence: ACTIVE: albuterol sulfate\n",
      "\n",
      "Decoded sentence: AlTIVE: abluterol sulfate\n",
      "\n",
      "-\n",
      "Input sentence: First name  Debra\n",
      "GT sentence: First name - Debra\n",
      "\n",
      "Decoded sentence: First name - Debra\n",
      "\n",
      "-\n",
      "Input sentence: Country:a\n",
      "GT sentence: Country:\n",
      "\n",
      "Decoded sentence: Country: \n",
      "\n",
      "-\n",
      "Input sentence: RDW~CV  12.70 Range: 11504.5 .. %\n",
      "GT sentence:  RDW-CV - 12.70 Range: 11.5-04.5 - %\n",
      "\n",
      "Decoded sentence: DOWICV - 12.70 Range: 115-11. %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Accident Descriptino: Acdent cocurred while partiipahtngz n a qhockye\n",
      "GT sentence: Accident Description: Accident occurred while participating in a hockey\n",
      "\n",
      "Decoded sentence: Accident Description: Acdent cocurred while partiphtng in a hockey\n",
      "\n",
      "-\n",
      "Input sentence: Patient Name:\n",
      "GT sentence: Patient Name:\n",
      "\n",
      "Decoded sentence: Patient Name:\n",
      "\n",
      "-\n",
      "Input sentence: PARAM 0 UNT PAID\n",
      "GT sentence: PARAMOUNT PAID\n",
      "\n",
      "Decoded sentence: PARAM OUNT PAID\n",
      "\n",
      "-\n",
      "Input sentence: iFndal result\n",
      "GT sentence: Final result\n",
      "\n",
      "Decoded sentence: Fundal result\n",
      "\n",
      "-\n",
      "Input sentence: UNUM LFE INSURANCE COMPANY kOFA MwERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "GT sentence: UNUM LIFE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "\n",
      "Decoded sentence: UNUM LE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "\n",
      "-\n",
      "Input sentence: Emlpoyeez Wellcness Benefit January 1, 2017n\n",
      "GT sentence: Employee Wellness Benefit January 1, 2017\n",
      "\n",
      "Decoded sentence: Employee Wellness Benefit January 1, 2017\n",
      "\n",
      "-\n",
      "Input sentence: ember No:\n",
      "GT sentence: Member No:\n",
      "\n",
      "Decoded sentence: Member No:\n",
      "\n",
      "-\n",
      "Input sentence: Claim Type: VB Accient - Accidetal Injury\n",
      "GT sentence: Claim Type: VB Accident - Accidental Injury\n",
      "\n",
      "Decoded sentence: Claim Type: VB Accident - Accidental Injury\n",
      "\n",
      "-\n",
      "Input sentence: Dlagnnsis\n",
      "GT sentence: Diagnosis\n",
      "\n",
      "Decoded sentence: Diagnosis\n",
      "\n",
      "-\n",
      "Input sentence: Accwiident Datle:\n",
      "GT sentence: Accident Date:\n",
      "\n",
      "Decoded sentence: Accident Date:\n",
      "\n",
      "-\n",
      "Input sentence: Signatures\n",
      "GT sentence: Signatures\n",
      "\n",
      "Decoded sentence: Signature\n",
      "\n",
      "-\n",
      "Input sentence: insuredfPoticynotder Name (Last Name First Name, Mi. Sufx)\n",
      "GT sentence: Insured/Policyholder Name (Last Name, First Name, MI, Suffix) \n",
      "\n",
      "Decoded sentence: Insured/Poticyhonder Name (Last Name, MI. Suffix) MI, Suffix)\n",
      "\n",
      "-\n",
      "Input sentence: Countyr:y\n",
      "GT sentence: Country:\n",
      "\n",
      "Decoded sentence: Country:\n",
      "\n",
      "-\n",
      "Input sentence: Earnlngs Mode:\n",
      "GT sentence: Earnings Mode:\n",
      "\n",
      "Decoded sentence: Earnings Mode:\n",
      "\n",
      "-\n",
      "Input sentence: v Townsend ROM-SS RT; Status:Complete: Done: 24Jan2018\n",
      "GT sentence:  Townsend ROM-SS RT; Status:Complete: Done: 24Jan2018\n",
      "\n",
      "Decoded sentence: Sonsend ROMISS RT; Status:Complete: Done: 24Jan2018\n",
      "\n",
      "-\n",
      "Input sentence: Company rotocols (required):\n",
      "GT sentence: Company protocols (required):\n",
      "\n",
      "Decoded sentence: Company of cols (required):\n",
      "\n",
      "-\n",
      "Input sentence: riginal Print Date: \n",
      "GT sentence: Original Print Date: \n",
      "\n",
      "Decoded sentence: Original Print Date: \n",
      "\n",
      "-\n",
      "Input sentence: G.T ax onsideraitons\n",
      "GT sentence: G. Tax considerations\n",
      "\n",
      "Decoded sentence: G. Ty ax onsiderations\n",
      "\n",
      "-\n",
      "Input sentence: Standing Mariah Larsen MA\n",
      "GT sentence: Standing Mariah Larsen MA\n",
      "\n",
      "Decoded sentence: Standing Mariah Larsen MA\n",
      "\n",
      "-\n",
      "Input sentence: Stts: Signedr\n",
      "GT sentence: Status: Signed\n",
      "\n",
      "Decoded sentence: State: Signedr\n",
      "\n",
      "-\n",
      "Input sentence: Wer you at wocrk at the timde of your accident? Yes o\n",
      "GT sentence: Were you at work at the time of your accident? Yes No\n",
      "\n",
      "Decoded sentence: Were you at work at the time of your accident? Yes No \n",
      "\n",
      "-\n",
      "Input sentence: Primary Crae Pysciian Nmae Mailing ddress Telephone N.\n",
      "GT sentence: Primary Care Physician Name Mailing Address Telephone No.\n",
      "\n",
      "Decoded sentence: Primary Care Print Name Mailing Address Telephone N.\n",
      "\n",
      "-\n",
      "Input sentence: I Specialisl relerrol\n",
      "GT sentence: Specialist referral Date (m/d/y)\n",
      "\n",
      "Decoded sentence: Specialty relerrol\n",
      "\n",
      "-\n",
      "Input sentence: Soial Security Number:\n",
      "GT sentence: Social Security Number:\n",
      "\n",
      "Decoded sentence: Social Security Number:\n",
      "\n",
      "-\n",
      "Input sentence: Ciy Statieg Zipm\n",
      "GT sentence: City State Zip\n",
      "\n",
      "Decoded sentence: City Statient Zipm\n",
      "\n",
      "-\n",
      "Input sentence: Pain Medicine: Plaese tak eas needed.\n",
      "GT sentence: Pain Medicine: Please take as needed.\n",
      "\n",
      "Decoded sentence: Pain Medicine: Plane take enterded.\n",
      "\n",
      "-\n",
      "Input sentence: Impression 8. Recommendations:\n",
      "GT sentence: Impression & Recommendations:\n",
      "\n",
      "Decoded sentence: Impression 8. Recommendations:\n",
      "\n",
      "-\n",
      "Input sentence: EMERGENCY MEDICAL ASSOCIATES\n",
      "GT sentence: EMERGENCY MEDICAL ASSOCIATES\n",
      "\n",
      "Decoded sentence: EMERGENCY MEDICAL ASSOCIATES\n",
      "\n",
      "-\n",
      "Input sentence: a sthis a motor vehicl cacyident? Yes o\n",
      "GT sentence: Was this a motor vehicle accident? Yes No\n",
      "\n",
      "Decoded sentence: as this a motor vehicl caccident? Yes No If yes, Paydent? Yes No If yes, Paydent? Yes No If yes, Pay\n",
      "-\n",
      "Input sentence: Retain For Tax Purposes\n",
      "GT sentence: Retain For Tax Purposes\n",
      "\n",
      "Decoded sentence: Retain For Tax Purposes\n",
      "\n",
      "-\n",
      "Input sentence: Right knee MRI IMPRESSION:\n",
      "GT sentence: Right knee MRI IMPRESSION:\n",
      "\n",
      "Decoded sentence: Right knee MRI IMPRESSION:\n",
      "\n",
      "-\n",
      "Input sentence: E] Return to School & Full Release in P.E./SPORTS on:\n",
      "GT sentence: Return to School & Full Release in P.E./SPORTS on:\n",
      "\n",
      "Decoded sentence: Return to School & Full Release in P.E./SPORTS on:\n",
      "\n",
      "-\n",
      "Input sentence: Primay Adress:\n",
      "GT sentence: Primary Address:\n",
      "\n",
      "Decoded sentence: Primary Address:\n",
      "\n",
      "-\n",
      "Input sentence: INSUR. PENDIN\n",
      "GT sentence: INSUR. PENDING\n",
      "\n",
      "Decoded sentence: INSUR. PENINS\n",
      "\n",
      "-\n",
      "Input sentence: EQUETS FOR LEAVEO F AiBSbENCE FOMR\n",
      "GT sentence: REQUEST FOR LEAVE OF ABSENCE FORM\n",
      "\n",
      "Decoded sentence: EXULT FOR LEAVEOV FOR SENCENCE FOMMR\n",
      "\n",
      "-\n",
      "Input sentence: Tem pAgency and timeframe - n/a\n",
      "GT sentence: Temp Agency and timeframe - n/a\n",
      "\n",
      "Decoded sentence: Temp Agency and timeframe - n/a\n",
      "\n",
      "-\n",
      "Input sentence: Buisness Tlehone:\n",
      "GT sentence: Business Telephone:\n",
      "\n",
      "Decoded sentence: Business Telephone:\n",
      "\n",
      "-\n",
      "Input sentence: Completed Oredrsf (tthis encounter\n",
      "GT sentence: Completed Orders (this encounter)\n",
      "\n",
      "Decoded sentence: Completed Oreerisf Sthis encounter\n",
      "\n",
      "-\n",
      "Input sentence: Dependent Information\n",
      "GT sentence: Dependent Information\n",
      "\n",
      "Decoded sentence: Dependent Information\n",
      "\n",
      "-\n",
      "Input sentence: TELEPrHONE \n",
      "GT sentence: TELEPHONE #\n",
      "\n",
      "Decoded sentence: TELEPPONE \n",
      "\n",
      "-\n",
      "Input sentence: Compazrison: None available\n",
      "GT sentence: Comparison: None available\n",
      "\n",
      "Decoded sentence: Comparison: None available\n",
      "\n",
      "-\n",
      "Input sentence: pOm Note\n",
      "GT sentence: Op Notes\n",
      "\n",
      "Decoded sentence: ON Note\n",
      "\n",
      "-\n",
      "Input sentence: Date of Visit:\n",
      "GT sentence: Date of Visit:\n",
      "\n",
      "Decoded sentence: Date of Visit:\n",
      "\n",
      "-\n",
      "Input sentence: TWINC ITIES\n",
      "GT sentence: TWIN CITIES\n",
      "\n",
      "Decoded sentence: TWIN CITIES\n",
      "\n",
      "-\n",
      "Input sentence: PgatientI D:\n",
      "GT sentence: Patient ID:\n",
      "\n",
      "Decoded sentence: Patient ID:\n",
      "\n",
      "-\n",
      "Input sentence: SURGEON :JASON HOLM, M.D. \n",
      "GT sentence: SURGEON: JASON HOLM, M.D. \n",
      "\n",
      "Decoded sentence: SURGEON: JASON HOLM, M.D. \n",
      "\n",
      "-\n",
      "Input sentence: Knee Exam:\n",
      "GT sentence: Knee Exam:\n",
      "\n",
      "Decoded sentence: Knee Exam:\n",
      "\n",
      "-\n",
      "Input sentence: Tagalog: Kung kauilangan niyou ang tulon gsa Tagalog tmuawa sa\n",
      "GT sentence: Tagalog: Kung kailangan niyou ang tulong sa Tagalog tumawag sa\n",
      "\n",
      "Decoded sentence: Tagalog: Kung kaulangan niyou ang tulong sa Tagalog tumawa sa\n",
      "\n",
      "-\n",
      "Input sentence: Requesting Physician: Dr. Earl 1'). Bryan\n",
      "GT sentence:  Physician: Dr. Earl D. Bryan \n",
      "\n",
      "Decoded sentence: Requesting Physician: Dr. Earl 13). Bryan\n",
      "\n",
      "-\n",
      "Input sentence: ccouvnt Numbper\n",
      "GT sentence: Account Number\n",
      "\n",
      "Decoded sentence: count Number\n",
      "\n",
      "-\n",
      "Input sentence: Employee 0n 3L Off-Job Acc September 1, 2017\n",
      "GT sentence: Employee On & Off-Job Acc September 1, 2017\n",
      "\n",
      "Decoded sentence: Employee On & Off-Job Acc September 1, 2017\n",
      "\n",
      "-\n",
      "Input sentence: Down East Orthopedics Sports Medicine\n",
      "GT sentence: Down East Orthopedics Sports Medicine\n",
      "\n",
      "Decoded sentence: Down East Orthopedics Sports Medicine\n",
      "\n",
      "-\n",
      "Input sentence: H .Signature of Insuerd/Policyholdr\n",
      "GT sentence: H. Signature of Insured/Policyholder\n",
      "\n",
      "Decoded sentence: H. Signature of Insured/Policyholder\n",
      "\n",
      "-\n",
      "Input sentence: Employee On & OffJob Aoc May 1, 2015\n",
      "GT sentence: Employee On & Off-Job Acc May 1, 2015\n",
      "\n",
      "Decoded sentence: Employee On & Off-Job Acc May 1, 2015\n",
      "\n",
      "-\n",
      "Input sentence: Note: Final cost may var yslightly due to roundingw differences.\n",
      "GT sentence: Note: Final cost may vary slightly due to rounding differences.\n",
      "\n",
      "Decoded sentence: Note: Final cost may vary slightly due to rounding differences.\n",
      "\n",
      "-\n",
      "Input sentence: Acct ID\n",
      "GT sentence: Acct ID\n",
      "\n",
      "Decoded sentence: Acct ID\n",
      "\n",
      "-\n",
      "Input sentence: Paid Amotunt\n",
      "GT sentence: Paid Amount\n",
      "\n",
      "Decoded sentence: Paid Amount\n",
      "\n",
      "-\n",
      "Input sentence: .9 Lyrcia.\n",
      "GT sentence: 9. Lyrica.\n",
      "\n",
      "Decoded sentence: 9. Lyrica.\n",
      "\n",
      "-\n",
      "Input sentence: lfthis claim is related to normal pregnancy, plea-s- provide the following:\n",
      "GT sentence: If this claim is related to normal pregnancy, please provide the following:\n",
      "\n",
      "Decoded sentence: If this claim is related to normal pregnancy, please the following:\n",
      "\n",
      "-\n",
      "Input sentence: Page 1 of2  (conptinued on back\n",
      "GT sentence: Page 1 of 2 (continued on back)\n",
      "\n",
      "Decoded sentence: Page 1 of 2 (continued on back\n",
      "\n",
      "-\n",
      "Input sentence: Total time absent -\n",
      "GT sentence: Total time absent -\n",
      "\n",
      "Decoded sentence: Total time absent -\n",
      "\n",
      "-\n",
      "Input sentence: Gender:\n",
      "GT sentence: Gender:\n",
      "\n",
      "Decoded sentence: Gender:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: DEDUCTIBLEp oOUT OF POKET\n",
      "GT sentence: DEDUCTIBLE OUT OF POCKET\n",
      "\n",
      "Decoded sentence: DEDUCTIBLE OUT OF POPKET\n",
      "\n",
      "-\n",
      "Input sentence: Call to-lfreep Monaday throug Frinday, 8 ak.m. to 8 pm.. (Eastern ime).\n",
      "GT sentence: Call toll-free Monday through Friday, 8 a.m. to 8 p.m. (Eastern Time).\n",
      "\n",
      "Decoded sentence: Call toll-free Monday through Frinday, 8 a.m.. to 8 p.m. (Eastern ime).\n",
      "\n",
      "-\n",
      "Input sentence: 1 . Right anke  views.\n",
      "GT sentence: 1 . Right ankle 3 views.\n",
      "\n",
      "Decoded sentence: 1. Right anke views.\n",
      "\n",
      "-\n",
      "Input sentence: FAYTTEVILL Epinted\n",
      "GT sentence: FAYETTEVILLE printed\n",
      "\n",
      "Decoded sentence: FATTERTTEVILL Eppinted\n",
      "\n",
      "-\n",
      "Input sentence: Grxoup Polciy #:\n",
      "GT sentence: Group Policy #:\n",
      "\n",
      "Decoded sentence: Group Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: Additional Information:\n",
      "GT sentence: Additional Information:\n",
      "\n",
      "Decoded sentence: Additional Information:\n",
      "\n",
      "-\n",
      "Input sentence: Awmmossicdunyuencdalbmdmco\n",
      "GT sentence: A well-corticated ossific density seen distal to the ulnar styloid.\n",
      "\n",
      "Decoded sentence: Admossicficidunt Secficed\n",
      "\n",
      "-\n",
      "Input sentence: Creatde By: Hughes,B rittany\n",
      "GT sentence: Created By: Hughes, Brittany\n",
      "\n",
      "Decoded sentence: Created By: Hughes, Brittany\n",
      "\n",
      "-\n",
      "Input sentence: Closedc Opuen Unknokwpn Name ofboen farctured orislocated:\n",
      "GT sentence: Closed Open Unknown Name of bone fractured or dislocated:\n",
      "\n",
      "Decoded sentence: Closed Op work Unknown Name of boen from located:\n",
      "\n",
      "-\n",
      "Input sentence: Data oi Saar affine visit (mmrddfyyi:\n",
      "GT sentence: Date of last office visit (mm/dd/yy):\n",
      "\n",
      "Decoded sentence: Date of Sart fine visit (mm/dd/yy):\n",
      "\n",
      "-\n",
      "Input sentence: Tagalog: Kong kailangan niyou any rulong sa Tagalog tumawag so\n",
      "GT sentence: Tagalog: Kung kailangan niyou ang tulong sa Tagalog tumawag sa\n",
      "\n",
      "Decoded sentence: Tagalog: Know kailangan niyou ang tulong sa Tagalog tumawag so\n",
      "\n",
      "-\n",
      "Input sentence: 6 hours/day\n",
      "GT sentence: 6 hours/day\n",
      "\n",
      "Decoded sentence: 6 hours/day\n",
      "\n",
      "-\n",
      "Input sentence: tr Ego: Physioan\n",
      "GT sentence: Author Type: Physician\n",
      "\n",
      "Decoded sentence: Tre Eo: Physional\n",
      "\n",
      "-\n",
      "Input sentence: DOB: ACCT :#\n",
      "GT sentence: DOB: ACCT #:\n",
      "\n",
      "Decoded sentence: DOB: ACCT #:\n",
      "\n",
      "-\n",
      "Input sentence: Disposition:\n",
      "GT sentence: Disposition:\n",
      "\n",
      "Decoded sentence: Disposition:\n",
      "\n",
      "-\n",
      "Input sentence: Influenza. injectable, quadrivalent, split virus, 3 years or older Status: Completed.\n",
      "GT sentence: Influenza, injectable, quadrivalent, split virus, 3 years or older Status: Completed.\n",
      "\n",
      "Decoded sentence: Informena, injectable, quardivalent, split virus, split virus, Completed. Completed.\n",
      "\n",
      "-\n",
      "Input sentence: Gatsrointestinxal: no gasrointestinal symuptms.\n",
      "GT sentence: Gastrointestinal: no gastrointestinal symptoms.\n",
      "\n",
      "Decoded sentence: Gats ontestinal: no gascinal symptinal symptinal.\n",
      "\n",
      "-\n",
      "Input sentence: Surgrey Informaition\n",
      "GT sentence: Surgery Information\n",
      "\n",
      "Decoded sentence: Surgery Information\n",
      "\n",
      "-\n",
      "Input sentence: From:r WorkWell @ SMMC\n",
      "GT sentence: From: WorkWell @ SMMC\n",
      "\n",
      "Decoded sentence: Fromary WorkWell @ SMMM\n",
      "\n",
      "-\n",
      "Input sentence: LongT erm Disabiliyt\n",
      "GT sentence: Long Term Disability\n",
      "\n",
      "Decoded sentence: Long Term Disability\n",
      "\n",
      "-\n",
      "Input sentence: Sign up for pzaperliess at myFirstChioce.fcnh.om\n",
      "GT sentence: Sign up for paperless at myFirstChoice.fchn.com\n",
      "\n",
      "Decoded sentence: Sign up for paperliss at myFirstChoice.fchisching form.\n",
      "\n",
      "-\n",
      "Input sentence: U.S. Department of Helath and Human Srvices\n",
      "GT sentence: U.S. Department of Health and Human Services\n",
      "\n",
      "Decoded sentence: U.S. Department of Health and Human Services\n",
      "\n",
      "-\n",
      "Input sentence: 'T'Prm Darn:\n",
      "GT sentence: Term Date:\n",
      "\n",
      "Decoded sentence: Provider:\n",
      "\n",
      "-\n",
      "Input sentence: Ttoal Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "\n",
      "Decoded sentence: Total Monthly Premium:\n",
      "\n",
      "-\n",
      "Input sentence: Add another doctor  no\n",
      "GT sentence: Add another doctor - no\n",
      "\n",
      "Decoded sentence: Add and anctor - no\n",
      "\n",
      "-\n",
      "Input sentence: Medical Provdier Rolds: Primary are andTreating\n",
      "GT sentence: Medical Provider Roles: Primary Care and Treating\n",
      "\n",
      "Decoded sentence: Medical Provider Roleds: Primary are and Treating\n",
      "\n",
      "-\n",
      "Input sentence: Total Monthly Premium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "\n",
      "Decoded sentence: Total Monthly Premium:\n",
      "\n",
      "-\n",
      "Input sentence:  Estimated tretment slchedule: times er week(s) smonth(s) year(s)\n",
      "GT sentence:  Estimated treatment schedule: times per week(s) month(s) year(s)\n",
      "\n",
      "Decoded sentence:  Estimated treatment sledule: times fer week(s) sonth(s) year(s)\n",
      "\n",
      "-\n",
      "Input sentence: evised\n",
      "GT sentence: Revised\n",
      "\n",
      "Decoded sentence: Sevised\n",
      "\n",
      "-\n",
      "Input sentence: Regional Health Inc\n",
      "GT sentence: Regional Health Inc\n",
      "\n",
      "Decoded sentence: Regional Health Inc\n",
      "\n",
      "-\n",
      "Input sentence:  J01.10 Acte frontal sinusjiit, unspecified\n",
      "GT sentence:  J01.10 Acute frontal sinusitis, unspecified\n",
      "\n",
      "Decoded sentence:  J01.10 Acte frontal sinusitisitis unspecified\n",
      "\n",
      "-\n",
      "Input sentence: Cliam Details\n",
      "GT sentence: Claim Details\n",
      "\n",
      "Decoded sentence: Claim Details\n",
      "\n",
      "-\n",
      "Input sentence: Page 1\n",
      "GT sentence: Page 1\n",
      "\n",
      "Decoded sentence: Page 1\n",
      "\n",
      "-\n",
      "Input sentence: UIBQnOSIS Description\n",
      "GT sentence: Diagnosis Description\n",
      "\n",
      "Decoded sentence: Unum OSIS cription\n",
      "\n",
      "-\n",
      "Input sentence: Provider: Kari A. Lugn\n",
      "\n",
      "GT sentence: Provider: Kari A. Lund\n",
      "\n",
      "Decoded sentence: Provider: Kari A. Lungen\n",
      "\n",
      "-\n",
      "Input sentence: Timed\n",
      "GT sentence: Time\n",
      "\n",
      "Decoded sentence: Timed\n",
      "\n",
      "-\n",
      "Input sentence: Partiiant Nam:e\n",
      "GT sentence: Participant Name:\n",
      "\n",
      "Decoded sentence: Partina Name:\n",
      "\n",
      "-\n",
      "Input sentence: Conrmation of Coveraoe\n",
      "GT sentence: Confirmation of Coverage\n",
      "\n",
      "Decoded sentence: Confirmation of Coverage\n",
      "\n",
      "-\n",
      "Input sentence: Dictated:Dr. Jon J. Dewftte, M.D.Electronically Singed by:\n",
      "GT sentence: Dictated: Dr. Jon J. Dewitte, M.D. Electronically Signed by:\n",
      "\n",
      "Decoded sentence: Dictated:Dr. Jon J. Dewitte, M.D.Ely Signed by: Signed by:\n",
      "\n",
      "-\n",
      "Input sentence: Musculoskeletal: See HPI section for remaining musculoskeletal symptoms.\n",
      "GT sentence: Musculoskeletal: See HPI section for remaining musculoskeletal symptoms.\n",
      "\n",
      "Decoded sentence: Mussculseletal: Section for remaining musculoskeletal symptoms.\n",
      "\n",
      "-\n",
      "Input sentence: Acct ID\n",
      "GT sentence: Acct ID\n",
      "\n",
      "Decoded sentence: Acct ID\n",
      "\n",
      "-\n",
      "Input sentence: Idicates addiftional information s avilatble\n",
      "GT sentence: Indicates additional information is available.\n",
      "\n",
      "Decoded sentence: Idicates addifional Information s avilatiol \n",
      "\n",
      "-\n",
      "Input sentence: Group oPlyc# :\n",
      "GT sentence: Group Policy #:\n",
      "\n",
      "Decoded sentence: Group Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: PHYISCIAN INOF\n",
      "GT sentence: PHYSICIAN INFO\n",
      "\n",
      "Decoded sentence: PHYSICIAN INFORMAN ON ION PHYSICINF\n",
      "\n",
      "-\n",
      "Input sentence: State/Province:\n",
      "GT sentence: State/Province:\n",
      "\n",
      "Decoded sentence: State/Province:\n",
      "\n",
      "-\n",
      "Input sentence: Treating Physician Name\n",
      "GT sentence: Treating Physician Name\n",
      "\n",
      "Decoded sentence: Treating Physician Name\n",
      "\n",
      "-\n",
      "Input sentence: UNUM LIFE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "GT sentence: UNUM LIFE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "\n",
      "Decoded sentence: UNUM LIFE INSURANCE COMPANY OF AMERICA 2211 CONGRESS ST PORTLAND ME 04122-0002\n",
      "\n",
      "-\n",
      "Input sentence: Any overtime? - ono\n",
      "GT sentence: Any overtime? - no\n",
      "\n",
      "Decoded sentence: Any overtime? - no\n",
      "\n",
      "-\n",
      "Input sentence: Page 1\n",
      "GT sentence: Page 1\n",
      "\n",
      "Decoded sentence: Page 1\n",
      "\n",
      "-\n",
      "Input sentence: tTrnace number:\n",
      "GT sentence: Trace number:\n",
      "\n",
      "Decoded sentence: Trace number:\n",
      "\n",
      "-\n",
      "Input sentence: Custokmer oPlicy #:i\n",
      "GT sentence: Customer Policy #:\n",
      "\n",
      "Decoded sentence: Customer Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: Electronic. Survicr. 3.111111111111111 1-377.501-3335\n",
      "GT sentence: Electronic Service Requested\n",
      "\n",
      "Decoded sentence: Electronic. Survice. Survice. 111111-11-335011-3351\n",
      "\n",
      "-\n",
      "Input sentence: departmento  service:\n",
      "GT sentence: department of service:\n",
      "\n",
      "Decoded sentence: department of service:\n",
      "\n",
      "-\n",
      "Input sentence: Who Toe eorpte dEvdent HappenedTo: Employee/Poliyholder\n",
      "GT sentence: Who The Reported Event Happened To: Employee/Policyholder\n",
      "\n",
      "Decoded sentence: Who Tot erpente Event Happened To: Employee/Policyholder\n",
      "\n",
      "-\n",
      "Input sentence: 2. Metformin.\n",
      "GT sentence: 2. Metformin.\n",
      "\n",
      "Decoded sentence: 2. Metformin.\n",
      "\n",
      "-\n",
      "Input sentence: B. Com late this section for disablllt claims onl . \n",
      "GT sentence: B. Complete this section for disability claims only.\n",
      "\n",
      "Decoded sentence: B. Complete this section for disabillt claims only.\n",
      "\n",
      "-\n",
      "Input sentence: sDeinies aenmia, easybruising, enlarge dlymph ondkes, and chvrnic tero duse.\n",
      "GT sentence: Denies anemia, easy bruising, enlarged lymph nodes, and chronic steroid use.\n",
      "\n",
      "Decoded sentence: Dedinies and mia, prusing, enlarge dlymphone charges, and charnic ter ous.\n",
      "\n",
      "-\n",
      "Input sentence: Not Covered Amont\n",
      "GT sentence: Not Covered Amount\n",
      "\n",
      "Decoded sentence: Not Covered Amount\n",
      "\n",
      "-\n",
      "Input sentence: Servcie Arepa EH\n",
      "GT sentence: Service Area SEH\n",
      "\n",
      "Decoded sentence: Service Arepay Healt EH\n",
      "\n",
      "-\n",
      "Input sentence: Customer Policy #:\n",
      "GT sentence: Customer Policy #:\n",
      "\n",
      "Decoded sentence: Customer Policy #:\n",
      "\n",
      "-\n",
      "Input sentence: Regionla Health Inc\n",
      "GT sentence: Regional Health Inc\n",
      "\n",
      "Decoded sentence: Regional Health Inc\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: if this claim is related to normal pregnancy please provide the following.\n",
      "GT sentence: If this claim is related to normal pregnancy, please provide the following:\n",
      "\n",
      "Decoded sentence: If this claim is related to normal pregnancy pregnancy provide the following.\n",
      "\n",
      "-\n",
      "Input sentence: patella tendonitis\n",
      "GT sentence: patella tendonitis\n",
      "\n",
      "Decoded sentence: patella tendonitis\n",
      "\n",
      "-\n",
      "Input sentence: Surgical History\n",
      "GT sentence: Surgical History\n",
      "\n",
      "Decoded sentence: Surgical History\n",
      "\n",
      "-\n",
      "Input sentence: iDd You Know\n",
      "GT sentence: Did You Know\n",
      "\n",
      "Decoded sentence: Did You Know\n",
      "\n",
      "-\n",
      "Input sentence: Medication List\n",
      "GT sentence: Medication List\n",
      "\n",
      "Decoded sentence: Medication List\n",
      "\n",
      "-\n",
      "Input sentence: TWNI CITIES ORTHOPEDICS,w PA\n",
      "GT sentence: TWIN CITIES ORTHOPEDICS, PA\n",
      "\n",
      "Decoded sentence: TWIN CITIES ORTHOPEDICS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on separate tesseract corrected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain transfer from noisy spelling mistakes to OCR corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on noisy spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_transfer.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on OCR correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "# Keep test data from the corrected OCR, as this what we care about\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# Vectorize train data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "# Vectorize test data\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.001# Reduce the learning rate for fine tuning\n",
    "model.load_weights('best_model_transfer.hdf5')\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_transfer.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
