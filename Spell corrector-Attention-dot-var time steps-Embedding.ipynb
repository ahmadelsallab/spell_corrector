{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We tackle the problem of OCR post processing. In OCR, we map the image form of the document into the text domain. This is done first using an CNN+LSTM+CTC model, in our case based on tesseract. Since this output maps only image to text, we need something on top to validate and correct language semantics.\n",
    "\n",
    "The idea is to build a language model, that takes the OCRed text and corrects it based on language knowledge. The langauge model could be:\n",
    "- Char level: the aim is to capture the word morphology. In which case it's like a spelling correction system.\n",
    "- Word level: the aim is to capture the sentence semnatics. But such systems suffer from the OOV problem.\n",
    "- Fusion: to capture semantics and morphology language rules. The output has to be at char level, to avoid the OOV. However, the input can be char, word or both.\n",
    "\n",
    "The fusion model target is to learn:\n",
    "\n",
    "    p(char | char_context, word_context)\n",
    "\n",
    "In this workbook we use seq2seq vanilla Keras implementation, adapted from the lstm_seq2seq example on Eng-Fra translation task. The adaptation involves:\n",
    "\n",
    "- Adapt to spelling correction, on char level\n",
    "- Pre-train on a noisy, medical sentences\n",
    "- Fine tune a residual, to correct the mistakes of tesseract \n",
    "- Limit the input and output sequence lengths\n",
    "- Enusre teacher forcing auto regressive model in the decoder\n",
    "- Limit the padding per batch\n",
    "- Learning rate schedule\n",
    "- Bi-directional LSTM Encoder\n",
    "- Bi-directional GRU Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate, GRU, Dot, TimeDistributed, Activation, Embedding\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit gpu allocation. allow_growth, or gpu_fraction\n",
    "def gpu_alloc():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_alloc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER_sent(gt, pred):\n",
    "    '''\n",
    "    calculate_WER('calculating wer between two sentences', 'calculate wer between two sentences')\n",
    "    '''\n",
    "    gt_words = gt.lower().split(' ')\n",
    "    pred_words = pred.lower().split(' ')\n",
    "    d = np.zeros(((len(gt_words) + 1), (len(pred_words) + 1)), dtype=np.uint8)\n",
    "    # d = d.reshape((len(gt_words)+1, len(pred_words)+1))\n",
    "\n",
    "    # Initializing error matrix\n",
    "    for i in range(len(gt_words) + 1):\n",
    "        for j in range(len(pred_words) + 1):\n",
    "            if i == 0:\n",
    "                d[0][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][0] = i\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(gt_words) + 1):\n",
    "        for j in range(1, len(pred_words) + 1):\n",
    "            if gt_words[i - 1] == pred_words[j - 1]:\n",
    "                d[i][j] = d[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = d[i - 1][j - 1] + 1\n",
    "                insertion = d[i][j - 1] + 1\n",
    "                deletion = d[i - 1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    return d[len(gt_words)][len(pred_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WER(gt, pred):\n",
    "    '''\n",
    "\n",
    "    :param gt: list of sentences of the ground truth\n",
    "    :param pred: list of sentences of the predictions\n",
    "    both lists must have the same length\n",
    "    :return: accumulated WER\n",
    "    '''\n",
    "#    assert len(gt) == len(pred)\n",
    "    WER = 0\n",
    "    nb_w = 0\n",
    "    for i in range(len(gt)):\n",
    "        #print(gt[i])\n",
    "        #print(pred[i])\n",
    "        WER += calculate_WER_sent(gt[i], pred[i])\n",
    "        nb_w += len(gt[i])\n",
    "\n",
    "    return WER / nb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial noisy spelling mistakes\n",
    "def noise_maker(sentence, threshold):\n",
    "    '''Relocate, remove, or add characters to create spelling mistakes'''\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
    "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
    "    noisy_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        random = np.random.uniform(0, 1, 1)\n",
    "        # Most characters will be correct since the threshold value is high\n",
    "        if random < threshold:\n",
    "            noisy_sentence.append(sentence[i])\n",
    "        else:\n",
    "            new_random = np.random.uniform(0, 1, 1)\n",
    "            # ~33% chance characters will swap locations\n",
    "            if new_random > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    # If last character in sentence, it will not be typed\n",
    "                    continue\n",
    "                else:\n",
    "                    # if any other character, swap order with following character\n",
    "                    noisy_sentence.append(sentence[i + 1])\n",
    "                    noisy_sentence.append(sentence[i])\n",
    "                    i += 1\n",
    "            # ~33% chance an extra lower case letter will be added to the sentence\n",
    "            elif new_random < 0.33:\n",
    "                random_letter = np.random.choice(letters, 1)[0]\n",
    "                noisy_sentence.append(random_letter)\n",
    "                noisy_sentence.append(sentence[i])\n",
    "            # ~33% chance a character will not be typed\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "\n",
    "    return ''.join(noisy_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_gt(file_name, num_samples, max_sent_len, min_sent_len, delimiter='\\t', gt_index=1, prediction_index=0):\n",
    "    '''Load data from txt file, with each line has: <TXT><TAB><GT>. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    for row in open(file_name, encoding='utf8'):\n",
    "        if cnt < num_samples :\n",
    "            #print(row)\n",
    "            sents = row.split(delimiter)\n",
    "            input_text = sents[prediction_index]\n",
    "            \n",
    "            target_text = '\\t' + sents[gt_index] + '\\n'\n",
    "            if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                cnt += 1\n",
    "                \n",
    "                input_texts.append(input_text)\n",
    "                target_texts.append(target_text)\n",
    "                gt_texts.append(sents[gt_index])\n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_noise(file_name, num_samples, noise_threshold, max_sent_len, min_sent_len):\n",
    "    '''Load data from txt file, with each line has: <TXT>. The GT is just a noisy version of TXT. The  target to the decoder muxt have \\t as the start trigger and \\n as the stop trigger.'''\n",
    "    cnt = 0  \n",
    "    input_texts = []\n",
    "    gt_texts = []\n",
    "    target_texts = []\n",
    "    while cnt < num_samples :\n",
    "        for row in open(file_name, encoding='utf8'):\n",
    "        #for row in open(file_name):\n",
    "            if cnt < num_samples :\n",
    "                sents = row.split(\"\\t\")\n",
    "                input_text = noise_maker(sents[1], noise_threshold)\n",
    "                input_text = input_text[:-1]\n",
    "\n",
    "                target_text = '\\t' + sents[1] + '\\n'            \n",
    "                if len(input_text) > min_sent_len and len(input_text) < max_sent_len and len(target_text) > min_sent_len and len(target_text) < max_sent_len:\n",
    "                    cnt += 1\n",
    "                    input_texts.append(input_text)\n",
    "                    target_texts.append(target_text)\n",
    "                    gt_texts.append(target_text[1:-1])\n",
    "                    \n",
    "    return input_texts, target_texts, gt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_texts):\n",
    "    '''Build vocab dictionary to victorize chars into ints'''\n",
    "    vocab_to_int = {}\n",
    "    count = 0 # Start index for any char will be 1, as 0 is masked by the Embedding/Masking layer\n",
    "    codes = ['UNK', ' ', '\\t','\\n']# Start 'UNK' at the first entry, to keep its index=0 to be masked\n",
    "    for code in codes:\n",
    "        if code not in vocab_to_int:\n",
    "            vocab_to_int[code] = count\n",
    "            count += 1    \n",
    "    \n",
    "    for sentence in all_texts:\n",
    "        for char in sentence:\n",
    "            if char not in vocab_to_int:\n",
    "                vocab_to_int[char] = count\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    '''''Build inverse translation from int to char'''\n",
    "    int_to_vocab = {}\n",
    "    for character, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = character\n",
    "        \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(input_texts, target_texts, max_encoder_seq_length, num_encoder_tokens, vocab_to_int):\n",
    "    '''Prepares the input text and targets into the proper seq2seq numpy arrays'''\n",
    "    encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length),\n",
    "    dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            # c0..cn\n",
    "            encoder_input_data[i, t] = vocab_to_int[char]\n",
    "        for t, char in enumerate(target_text):\n",
    "            # c0'..cm'\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t] = vocab_to_int[char]\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, vocab_to_int[char]] = 1.\n",
    "                \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gt_sequence(input_seq, int_to_vocab):\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    for i in range(input_seq.shape[1]):\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = input_seq[0][i]\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_outputs, h, c  = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab_to_int['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    #print(input_seq)\n",
    "    attention_density = []\n",
    "    while not stop_condition:\n",
    "        #print(target_seq)\n",
    "        output_tokens, attention, h, c  = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs] + states_value)\n",
    "        #print(attention.shape)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        #print(sampled_token_index)\n",
    "        sampled_char = int_to_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_encoder_tokens, latent_dim):\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    encoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(encoder_inputs)    \n",
    "    #encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n",
    "    print(encoder_states)\n",
    "    \n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    decoder_inputs_ = Embedding(num_encoder_tokens, num_encoder_tokens,                           \n",
    "                            weights=[np.eye(num_encoder_tokens)],\n",
    "                            mask_zero=True, trainable=False)(decoder_inputs)    \n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n",
    "    \n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n",
    "\n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    print(decoder_outputs)\n",
    "    print(encoder_outputs)\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    #print('context', context)\n",
    "    att_context_concat = Concatenate()\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    #model = Model(decoder_inputs, decoder_outputs)\n",
    "    print('encoder-decoder  model:')\n",
    "    print(model.summary()) \n",
    "    \n",
    "    print(encoder_inputs)\n",
    "    print(encoder_outputs)\n",
    "    print(encoder_states)\n",
    "    #encoder_model = Model(encoder_inputs, [encoder_outputs, encoder_states])\n",
    "    encoder_model = Model(input=encoder_inputs, output=[encoder_outputs] + encoder_states)\n",
    "\n",
    "    #decoder_state_input_h = Input(shape=(latent_dim,))# LSTM\n",
    "    #decoder_state_input_c = Input(shape=(latent_dim,))# LSTM\n",
    "    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n",
    "    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n",
    "    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n",
    "    #decoder_state_input = Input(shape=(latent_dim*2,)) # Bi GRU\n",
    "\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    #decoder_states_inputs = [decoder_state_input] # Bi GRU\n",
    "\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n",
    "\n",
    "    #decoder_outputs, state = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    \n",
    "    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n",
    "    \n",
    "    attention = att_activation(attention)\n",
    "    #print('attention', attention)\n",
    "    \n",
    "    context = context_dot([attention, decoder_encoder_inputs])\n",
    "    #print('context', context)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "    #print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "    # Has another weight + tanh layer as described in equation (5) of the paper\n",
    "    #decoder_outputs = TimeDistributed(Dense(64, activation=\"tanh\"))(decoder_combined_context)\n",
    "    #decoder_outputs = TimeDistributed(Dense(num_encoder_tokens, activation=\"softmax\"))(decoder_outputs)\n",
    "    \n",
    "    #decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    #decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs, attention] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab):\n",
    "\n",
    "    encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')\n",
    "    \n",
    "    for t, char in enumerate(text):\n",
    "        # c0..cn\n",
    "        encoder_input_data[0, t] = vocab_to_int[char]\n",
    "\n",
    "    input_seq = encoder_input_data[0:1]\n",
    "\n",
    "    decoded_sentence, attention_density = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(28,12))\n",
    "    \n",
    "    ax = sns.heatmap(attention_density[:, : len(text) + 2],\n",
    "        xticklabels=[w for w in text],\n",
    "        yticklabels=[w for w in decoded_sentence])\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 1000000\n",
    "min_sent_len = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of lenghts\n",
    "lengths = []\n",
    "for text in input_texts:\n",
    "    lengths.append(len(text))\n",
    "    lengths.append(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5443b39b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = plt.hist(lengths, bins=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1180.,   940.,  1384.,  1322.,  1174.,   722.,   592.,   536.,\n",
       "         332.,   286.,   242.,   186.,   180.,   214.,   112.,   154.,\n",
       "          68.,    82.,    58.,    88.,    70.,    64.,    32.,    30.,\n",
       "          20.,    26.,    32.,    24.,    58.,    14.,    64.,     6.,\n",
       "          28.,    16.,    24.,    28.,     8.,    18.,    14.,    18.,\n",
       "          12.,    24.,    14.,    28.,    14.,     4.,    12.,    44.,\n",
       "           4.,     6.,     2.,     4.,     6.,     0.,    36.,     0.,\n",
       "           4.,     4.,     8.,     6.,    14.,     8.,     8.,     8.,\n",
       "           2.,     0.,     6.,     2.,     2.,     4.,    12.,    14.,\n",
       "           8.,    12.,     6.,     0.,     4.,     4.,     2.,     0.,\n",
       "           2.,     0.,     4.,     6.,     0.,     4.,    14.,    26.,\n",
       "           4.,     0.,     2.,     4.,     4.,     2.,     4.,     2.,\n",
       "           6.,     0.,     2.,     2.,     2.,     6.,     4.,     2.,\n",
       "          40.,     4.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,    20.,     8.,    18.,     4.,     0.,\n",
       "           0.,     0.,     0.,     6.,    24.,     2.,     0.,     0.,\n",
       "           0.,     2.,     2.,     0.,     0.,     2.,     2.,     4.,\n",
       "           2.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     4.,     4.,\n",
       "          16.,     0.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     4.,     0.,     0.,     0.,     2.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     2.,\n",
       "           0.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     2.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     2.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           2.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     2.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           0.,     0.,     0.,     2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.   ,     4.878,     9.756,    14.634,    19.512,    24.39 ,\n",
       "          29.268,    34.146,    39.024,    43.902,    48.78 ,    53.658,\n",
       "          58.536,    63.414,    68.292,    73.17 ,    78.048,    82.926,\n",
       "          87.804,    92.682,    97.56 ,   102.438,   107.316,   112.194,\n",
       "         117.072,   121.95 ,   126.828,   131.706,   136.584,   141.462,\n",
       "         146.34 ,   151.218,   156.096,   160.974,   165.852,   170.73 ,\n",
       "         175.608,   180.486,   185.364,   190.242,   195.12 ,   199.998,\n",
       "         204.876,   209.754,   214.632,   219.51 ,   224.388,   229.266,\n",
       "         234.144,   239.022,   243.9  ,   248.778,   253.656,   258.534,\n",
       "         263.412,   268.29 ,   273.168,   278.046,   282.924,   287.802,\n",
       "         292.68 ,   297.558,   302.436,   307.314,   312.192,   317.07 ,\n",
       "         321.948,   326.826,   331.704,   336.582,   341.46 ,   346.338,\n",
       "         351.216,   356.094,   360.972,   365.85 ,   370.728,   375.606,\n",
       "         380.484,   385.362,   390.24 ,   395.118,   399.996,   404.874,\n",
       "         409.752,   414.63 ,   419.508,   424.386,   429.264,   434.142,\n",
       "         439.02 ,   443.898,   448.776,   453.654,   458.532,   463.41 ,\n",
       "         468.288,   473.166,   478.044,   482.922,   487.8  ,   492.678,\n",
       "         497.556,   502.434,   507.312,   512.19 ,   517.068,   521.946,\n",
       "         526.824,   531.702,   536.58 ,   541.458,   546.336,   551.214,\n",
       "         556.092,   560.97 ,   565.848,   570.726,   575.604,   580.482,\n",
       "         585.36 ,   590.238,   595.116,   599.994,   604.872,   609.75 ,\n",
       "         614.628,   619.506,   624.384,   629.262,   634.14 ,   639.018,\n",
       "         643.896,   648.774,   653.652,   658.53 ,   663.408,   668.286,\n",
       "         673.164,   678.042,   682.92 ,   687.798,   692.676,   697.554,\n",
       "         702.432,   707.31 ,   712.188,   717.066,   721.944,   726.822,\n",
       "         731.7  ,   736.578,   741.456,   746.334,   751.212,   756.09 ,\n",
       "         760.968,   765.846,   770.724,   775.602,   780.48 ,   785.358,\n",
       "         790.236,   795.114,   799.992,   804.87 ,   809.748,   814.626,\n",
       "         819.504,   824.382,   829.26 ,   834.138,   839.016,   843.894,\n",
       "         848.772,   853.65 ,   858.528,   863.406,   868.284,   873.162,\n",
       "         878.04 ,   882.918,   887.796,   892.674,   897.552,   902.43 ,\n",
       "         907.308,   912.186,   917.064,   921.942,   926.82 ,   931.698,\n",
       "         936.576,   941.454,   946.332,   951.21 ,   956.088,   960.966,\n",
       "         965.844,   970.722,   975.6  ,   980.478,   985.356,   990.234,\n",
       "         995.112,   999.99 ,  1004.868,  1009.746,  1014.624,  1019.502,\n",
       "        1024.38 ,  1029.258,  1034.136,  1039.014,  1043.892,  1048.77 ,\n",
       "        1053.648,  1058.526,  1063.404,  1068.282,  1073.16 ,  1078.038,\n",
       "        1082.916,  1087.794,  1092.672,  1097.55 ,  1102.428,  1107.306,\n",
       "        1112.184,  1117.062,  1121.94 ,  1126.818,  1131.696,  1136.574,\n",
       "        1141.452,  1146.33 ,  1151.208,  1156.086,  1160.964,  1165.842,\n",
       "        1170.72 ,  1175.598,  1180.476,  1185.354,  1190.232,  1195.11 ,\n",
       "        1199.988,  1204.866,  1209.744,  1214.622,  1219.5  ,  1224.378,\n",
       "        1229.256,  1234.134,  1239.012,  1243.89 ,  1248.768,  1253.646,\n",
       "        1258.524,  1263.402,  1268.28 ,  1273.158,  1278.036,  1282.914,\n",
       "        1287.792,  1292.67 ,  1297.548,  1302.426,  1307.304,  1312.182,\n",
       "        1317.06 ,  1321.938,  1326.816,  1331.694,  1336.572,  1341.45 ,\n",
       "        1346.328,  1351.206,  1356.084,  1360.962,  1365.84 ,  1370.718,\n",
       "        1375.596,  1380.474,  1385.352,  1390.23 ,  1395.108,  1399.986,\n",
       "        1404.864,  1409.742,  1414.62 ,  1419.498,  1424.376,  1429.254,\n",
       "        1434.132,  1439.01 ,  1443.888,  1448.766,  1453.644,  1458.522,\n",
       "        1463.4  ,  1468.278,  1473.156,  1478.034,  1482.912,  1487.79 ,\n",
       "        1492.668,  1497.546,  1502.424,  1507.302,  1512.18 ,  1517.058,\n",
       "        1521.936,  1526.814,  1531.692,  1536.57 ,  1541.448,  1546.326,\n",
       "        1551.204,  1556.082,  1560.96 ,  1565.838,  1570.716,  1575.594,\n",
       "        1580.472,  1585.35 ,  1590.228,  1595.106,  1599.984,  1604.862,\n",
       "        1609.74 ,  1614.618,  1619.496,  1624.374,  1629.252,  1634.13 ,\n",
       "        1639.008,  1643.886,  1648.764,  1653.642,  1658.52 ,  1663.398,\n",
       "        1668.276,  1673.154,  1678.032,  1682.91 ,  1687.788,  1692.666,\n",
       "        1697.544,  1702.422,  1707.3  ,  1712.178,  1717.056,  1721.934,\n",
       "        1726.812,  1731.69 ,  1736.568,  1741.446,  1746.324,  1751.202,\n",
       "        1756.08 ,  1760.958,  1765.836,  1770.714,  1775.592,  1780.47 ,\n",
       "        1785.348,  1790.226,  1795.104,  1799.982,  1804.86 ,  1809.738,\n",
       "        1814.616,  1819.494,  1824.372,  1829.25 ,  1834.128,  1839.006,\n",
       "        1843.884,  1848.762,  1853.64 ,  1858.518,  1863.396,  1868.274,\n",
       "        1873.152,  1878.03 ,  1882.908,  1887.786,  1892.664,  1897.542,\n",
       "        1902.42 ,  1907.298,  1912.176,  1917.054,  1921.932,  1926.81 ,\n",
       "        1931.688,  1936.566,  1941.444,  1946.322,  1951.2  ,  1956.078,\n",
       "        1960.956,  1965.834,  1970.712,  1975.59 ,  1980.468,  1985.346,\n",
       "        1990.224,  1995.102,  1999.98 ,  2004.858,  2009.736,  2014.614,\n",
       "        2019.492,  2024.37 ,  2029.248,  2034.126,  2039.004,  2043.882,\n",
       "        2048.76 ,  2053.638,  2058.516,  2063.394,  2068.272,  2073.15 ,\n",
       "        2078.028,  2082.906,  2087.784,  2092.662,  2097.54 ,  2102.418,\n",
       "        2107.296,  2112.174,  2117.052,  2121.93 ,  2126.808,  2131.686,\n",
       "        2136.564,  2141.442,  2146.32 ,  2151.198,  2156.076,  2160.954,\n",
       "        2165.832,  2170.71 ,  2175.588,  2180.466,  2185.344,  2190.222,\n",
       "        2195.1  ,  2199.978,  2204.856,  2209.734,  2214.612,  2219.49 ,\n",
       "        2224.368,  2229.246,  2234.124,  2239.002,  2243.88 ,  2248.758,\n",
       "        2253.636,  2258.514,  2263.392,  2268.27 ,  2273.148,  2278.026,\n",
       "        2282.904,  2287.782,  2292.66 ,  2297.538,  2302.416,  2307.294,\n",
       "        2312.172,  2317.05 ,  2321.928,  2326.806,  2331.684,  2336.562,\n",
       "        2341.44 ,  2346.318,  2351.196,  2356.074,  2360.952,  2365.83 ,\n",
       "        2370.708,  2375.586,  2380.464,  2385.342,  2390.22 ,  2395.098,\n",
       "        2399.976,  2404.854,  2409.732,  2414.61 ,  2419.488,  2424.366,\n",
       "        2429.244,  2434.122,  2439.   ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  9.756\n",
      "Count of most probable lenght =  1384.0\n",
      "Min length =  4.878\n"
     ]
    }
   ],
   "source": [
    "max_sent_len =  h[1][np.argmax(h[0])]\n",
    "min_sent_len = h[1][1]\n",
    "print('Most probable length = ', max_sent_len)\n",
    "print('Count of most probable lenght = ', np.max(h[0]))\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len =  50#int(np.ceil(max_sent_len))\n",
    "min_sent_len = 4#int(np.floor(min_sent_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable length =  50\n",
      "Min length =  4\n"
     ]
    }
   ],
   "source": [
    "print('Most probable length = ', max_sent_len)\n",
    "print('Min length = ', min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "input_texts_OCR_tess, target_texts_tess, gt_tess = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "num_samples = 0\n",
    "OCR_data = os.path.join(data_path, 'output_handwritten.txt')\n",
    "input_texts_OCR_hand, target_texts_OCR_hand, gt_texts_OCR_hand = load_data_with_gt(OCR_data, num_samples, max_sent_len, min_sent_len, delimiter='|',gt_index=0, prediction_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texts_OCR\n",
    "#target_texts = target_texts_OCR\n",
    "input_texts_OCR = input_texts_OCR_tess + input_texts_OCR_hand\n",
    "target_texts_OCR = target_texts_tess + target_texts_OCR_hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3579"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnum_samples = 0\\nbig_data = os.path.join(data_path, 'big.txt')\\nthreshold = 0.9\\ninput_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \\n                                                                 num_samples=num_samples, \\n                                                                 noise_threshold=threshold, \\n                                                                 max_sent_len=max_sent_len, \\n                                                                 min_sent_len=min_sent_len)\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "num_samples = 0\n",
    "big_data = os.path.join(data_path, 'big.txt')\n",
    "threshold = 0.9\n",
    "input_texts_gen, target_texts_gen, gt_gen = load_data_with_noise(file_name=big_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)\n",
    "'''                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_texts = input_texs_gen\n",
    "#target_texts = target_texts_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on noisy tesseract corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'all_ocr_data_2.txt')\n",
    "threshold = 0.9\n",
    "input_texts_noisy_OCR, target_texts_noisy_OCR, gt_noisy_OCR = load_data_with_noise(file_name=tess_correction_data, \n",
    "                                                                 num_samples=num_samples, \n",
    "                                                                 noise_threshold=threshold, \n",
    "                                                                 max_sent_len=max_sent_len, \n",
    "                                                                 min_sent_len=min_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR\\ntarget_texts = target_texts_noisy_OCR\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on merge of tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_OCR + input_texts_gen\\ntarget_texts = input_texts_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_OCR + input_texts_gen\n",
    "target_texts = input_texts_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract correction + generic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_texts = input_texts_noisy_OCR + input_texts_gen\\ntarget_texts = input_texts_noisy_OCR + target_texts_gen\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "input_texts = input_texts_noisy_OCR + input_texts_gen\n",
    "target_texts = input_texts_noisy_OCR + target_texts_gen\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results noisy tesseract noisy + correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts_noisy_OCR + input_texts_OCR\n",
    "target_texts = target_texts_noisy_OCR + target_texts_OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of pre-training on generic and fine tuning on tesseract correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13579\n",
      "Cwlaim xType: VBe Acciden t-x Accidental Injur\n",
      " \n",
      " \tClaim Type: VB Accident - Accidental Injury\n",
      "\n",
      "\n",
      "Polichyolder/Owner Ifnormtion \n",
      " \tPolicyholder/Owner Information\n",
      "\n",
      "\n",
      "First ame: \n",
      " \tFirst Name:\n",
      "\n",
      "\n",
      "Middle oName/Initial\n",
      " \n",
      " \tMiddle Name/Initial:\n",
      "\n",
      "\n",
      "Last Name: \n",
      " \tLast Name:\n",
      "\n",
      "\n",
      "Social Security Nmuber: \n",
      " \tSocial Security Number:\n",
      "\n",
      "\n",
      "Birth Date: \n",
      " \tBirth Date:\n",
      "\n",
      "\n",
      "eGnde:r \n",
      " \tGender:\n",
      "\n",
      "\n",
      "Language reference: \n",
      " \tLanguage Preference:\n",
      "\n",
      "\n",
      "dderssa Lie 1i: \n",
      " \tAddress Line 1:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(len(input_texts))\n",
    "for i in range(10):\n",
    "    print(input_texts[i], '\\n', target_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = target_texts + input_texts\n",
    "vocab_to_int, int_to_vocab = build_vocab(all_texts)\n",
    "np.savez('vocab', vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, max_sent_len=max_sent_len, min_sent_len=min_sent_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(vocab_to_int))\n",
    "target_characters = sorted(list(vocab_to_int))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 13579\n",
      "Number of unique input tokens: 115\n",
      "Number of unique output tokens: 115\n",
      "Max sequence length for inputs: 49\n",
      "Max sequence length for outputs: 49\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t': 2,\n",
       " '\\n': 3,\n",
       " ' ': 1,\n",
       " '!': 104,\n",
       " '\"': 95,\n",
       " '#': 67,\n",
       " '$': 80,\n",
       " '%': 85,\n",
       " '&': 73,\n",
       " \"'\": 83,\n",
       " '(': 63,\n",
       " ')': 64,\n",
       " '*': 77,\n",
       " '+': 76,\n",
       " ',': 69,\n",
       " '-': 21,\n",
       " '.': 48,\n",
       " '/': 29,\n",
       " '0': 54,\n",
       " '1': 43,\n",
       " '2': 53,\n",
       " '3': 57,\n",
       " '4': 66,\n",
       " '5': 74,\n",
       " '6': 55,\n",
       " '7': 70,\n",
       " '8': 60,\n",
       " '9': 72,\n",
       " ':': 13,\n",
       " ';': 75,\n",
       " '<': 107,\n",
       " '=': 94,\n",
       " '?': 59,\n",
       " '@': 81,\n",
       " 'A': 16,\n",
       " 'B': 15,\n",
       " 'C': 4,\n",
       " 'D': 40,\n",
       " 'E': 45,\n",
       " 'F': 33,\n",
       " 'G': 41,\n",
       " 'H': 52,\n",
       " 'I': 22,\n",
       " 'J': 68,\n",
       " 'K': 50,\n",
       " 'L': 37,\n",
       " 'M': 36,\n",
       " 'N': 35,\n",
       " 'O': 30,\n",
       " 'P': 26,\n",
       " 'Q': 78,\n",
       " 'R': 46,\n",
       " 'S': 38,\n",
       " 'T': 9,\n",
       " 'U': 49,\n",
       " 'UNK': 0,\n",
       " 'V': 14,\n",
       " 'W': 51,\n",
       " 'X': 79,\n",
       " 'Y': 47,\n",
       " 'Z': 71,\n",
       " '[': 91,\n",
       " '\\\\': 97,\n",
       " ']': 92,\n",
       " '^': 86,\n",
       " '_': 105,\n",
       " 'a': 6,\n",
       " 'b': 39,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 12,\n",
       " 'f': 32,\n",
       " 'g': 42,\n",
       " 'h': 28,\n",
       " 'i': 7,\n",
       " 'j': 23,\n",
       " 'k': 58,\n",
       " 'l': 5,\n",
       " 'm': 8,\n",
       " 'n': 19,\n",
       " 'o': 27,\n",
       " 'p': 11,\n",
       " 'q': 56,\n",
       " 'r': 25,\n",
       " 's': 34,\n",
       " 't': 20,\n",
       " 'u': 24,\n",
       " 'v': 44,\n",
       " 'w': 31,\n",
       " 'x': 61,\n",
       " 'y': 10,\n",
       " 'z': 62,\n",
       " '{': 108,\n",
       " '|': 82,\n",
       " '}': 100,\n",
       " '~': 103,\n",
       " '£': 113,\n",
       " '§': 109,\n",
       " '«': 111,\n",
       " '®': 114,\n",
       " '°': 90,\n",
       " '»': 110,\n",
       " 'é': 106,\n",
       " '–': 93,\n",
       " '—': 101,\n",
       " '‘': 99,\n",
       " '’': 65,\n",
       " '“': 102,\n",
       " '”': 89,\n",
       " '•': 84,\n",
       " '€': 112,\n",
       " '●': 87,\n",
       " '✓': 96,\n",
       " 'ﬁ': 88,\n",
       " 'ﬂ': 98}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int # Some special chars need to be removed TODO: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: ' ',\n",
       " 2: '\\t',\n",
       " 3: '\\n',\n",
       " 4: 'C',\n",
       " 5: 'l',\n",
       " 6: 'a',\n",
       " 7: 'i',\n",
       " 8: 'm',\n",
       " 9: 'T',\n",
       " 10: 'y',\n",
       " 11: 'p',\n",
       " 12: 'e',\n",
       " 13: ':',\n",
       " 14: 'V',\n",
       " 15: 'B',\n",
       " 16: 'A',\n",
       " 17: 'c',\n",
       " 18: 'd',\n",
       " 19: 'n',\n",
       " 20: 't',\n",
       " 21: '-',\n",
       " 22: 'I',\n",
       " 23: 'j',\n",
       " 24: 'u',\n",
       " 25: 'r',\n",
       " 26: 'P',\n",
       " 27: 'o',\n",
       " 28: 'h',\n",
       " 29: '/',\n",
       " 30: 'O',\n",
       " 31: 'w',\n",
       " 32: 'f',\n",
       " 33: 'F',\n",
       " 34: 's',\n",
       " 35: 'N',\n",
       " 36: 'M',\n",
       " 37: 'L',\n",
       " 38: 'S',\n",
       " 39: 'b',\n",
       " 40: 'D',\n",
       " 41: 'G',\n",
       " 42: 'g',\n",
       " 43: '1',\n",
       " 44: 'v',\n",
       " 45: 'E',\n",
       " 46: 'R',\n",
       " 47: 'Y',\n",
       " 48: '.',\n",
       " 49: 'U',\n",
       " 50: 'K',\n",
       " 51: 'W',\n",
       " 52: 'H',\n",
       " 53: '2',\n",
       " 54: '0',\n",
       " 55: '6',\n",
       " 56: 'q',\n",
       " 57: '3',\n",
       " 58: 'k',\n",
       " 59: '?',\n",
       " 60: '8',\n",
       " 61: 'x',\n",
       " 62: 'z',\n",
       " 63: '(',\n",
       " 64: ')',\n",
       " 65: '’',\n",
       " 66: '4',\n",
       " 67: '#',\n",
       " 68: 'J',\n",
       " 69: ',',\n",
       " 70: '7',\n",
       " 71: 'Z',\n",
       " 72: '9',\n",
       " 73: '&',\n",
       " 74: '5',\n",
       " 75: ';',\n",
       " 76: '+',\n",
       " 77: '*',\n",
       " 78: 'Q',\n",
       " 79: 'X',\n",
       " 80: '$',\n",
       " 81: '@',\n",
       " 82: '|',\n",
       " 83: \"'\",\n",
       " 84: '•',\n",
       " 85: '%',\n",
       " 86: '^',\n",
       " 87: '●',\n",
       " 88: 'ﬁ',\n",
       " 89: '”',\n",
       " 90: '°',\n",
       " 91: '[',\n",
       " 92: ']',\n",
       " 93: '–',\n",
       " 94: '=',\n",
       " 95: '\"',\n",
       " 96: '✓',\n",
       " 97: '\\\\',\n",
       " 98: 'ﬂ',\n",
       " 99: '‘',\n",
       " 100: '}',\n",
       " 101: '—',\n",
       " 102: '“',\n",
       " 103: '~',\n",
       " 104: '!',\n",
       " 105: '_',\n",
       " 106: 'é',\n",
       " 107: '<',\n",
       " 108: '{',\n",
       " 109: '§',\n",
       " 110: '»',\n",
       " 111: '«',\n",
       " 112: '€',\n",
       " 113: '£',\n",
       " 114: '®'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sentences\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11542, 49)\n",
      "(11542, 49, 115)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n",
      "Tensor(\"lstm_2/transpose_2:0\", shape=(?, ?, 512), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "encoder-decoder  model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 115)    13225       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 512),  761856      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 115)    13225       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1286144     embedding_2[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, None, None)   0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 115)    117875      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,192,325\n",
      "Trainable params: 2,165,875\n",
      "Non-trainable params: 26,450\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Tensor(\"input_1:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
      "[<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  \n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * np.exp(-k*epoch)\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "#lr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks_list.append(lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11542 samples, validate on 2037 samples\n",
      "Epoch 1/50\n",
      "11542/11542 [==============================] - 72s 6ms/step - loss: 1.8836 - categorical_accuracy: 0.4784 - val_loss: 0.4452 - val_categorical_accuracy: 0.8462\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.84621, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:872: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'concatenate_1/concat:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'concatenate_2/concat:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "11542/11542 [==============================] - 68s 6ms/step - loss: 0.2985 - categorical_accuracy: 0.8834 - val_loss: 0.2376 - val_categorical_accuracy: 0.8983\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.84621 to 0.89832, saving model to best_model.hdf5\n",
      "Epoch 3/50\n",
      "11542/11542 [==============================] - 69s 6ms/step - loss: 0.1668 - categorical_accuracy: 0.9154 - val_loss: 0.1902 - val_categorical_accuracy: 0.9101\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.89832 to 0.91005, saving model to best_model.hdf5\n",
      "Epoch 4/50\n",
      "11542/11542 [==============================] - 70s 6ms/step - loss: 0.1123 - categorical_accuracy: 0.9285 - val_loss: 0.1772 - val_categorical_accuracy: 0.9134\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.91005 to 0.91337, saving model to best_model.hdf5\n",
      "Epoch 5/50\n",
      " 1472/11542 [==>...........................] - ETA: 55s - loss: 0.0833 - categorical_accuracy: 0.9355"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-362e39851e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0;31m#validation_split=0.2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('encoder_model.hdf5')\n",
    "decoder_model.save('decoder_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input_data[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode_gt_sequence(encoder_input_data[5:6], int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: •  :10 years\n",
      "GT sentence: • : 10 years\n",
      "\n",
      "Decoded sentence: • : 10 years\n",
      "\n",
      "-\n",
      "Input sentence: Contrac Adjustmen\n",
      "\n",
      "GT sentence: Contract Adjustment\n",
      "\n",
      "Decoded sentence: Contrac Adjustment\n",
      "\n",
      "-\n",
      "Input sentence: Shotr Term Dsiability Policy #\n",
      "GT sentence: Short Term Disability Policy #\n",
      "\n",
      "Decoded sentence: Shorter Term Disability Policy #\n",
      "\n",
      "-\n",
      "Input sentence: cProcedure: Cleaning ,xray,k bandage\n",
      "GT sentence: Procedure: Cleaning, xray, bandage\n",
      "\n",
      "Decoded sentence: Procedure: Cleaning, xray, bandage\n",
      "\n",
      "-\n",
      "Input sentence: TWINC ITIE ORTHOPEDICS\n",
      "GT sentence: TWIN CITIES ORTHOPEDICS\n",
      "\n",
      "Decoded sentence: TWIN CITIES ORTHOPEDICS\n",
      "\n",
      "-\n",
      "Input sentence: E-mai:la\n",
      "GT sentence: E-mail:\n",
      "\n",
      "Decoded sentence: E-mail:\n",
      "\n",
      "-\n",
      "Input sentence: Send Inquiries To\n",
      "GT sentence: Send Inquiries To:\n",
      "\n",
      "Decoded sentence: Send Inquires To\n",
      "\n",
      "-\n",
      "Input sentence: Busies Fax:\n",
      "GT sentence: Business Fax:\n",
      "\n",
      "Decoded sentence: Business Fax:\n",
      "\n",
      "-\n",
      "Input sentence: UNKNOWN\n",
      "GT sentence: UNKNOWN\n",
      "\n",
      "Decoded sentence: UNKNOWN\n",
      "\n",
      "-\n",
      "Input sentence: Addressl Line  1:\n",
      "GT sentence: Address Line 1 :\n",
      "\n",
      "Decoded sentence: Address Line 1:\n",
      "\n",
      "-\n",
      "Input sentence: Chronic arm pain\n",
      "GT sentence: Chronic arm pain\n",
      "\n",
      "Decoded sentence: Chronic arm pain\n",
      "\n",
      "-\n",
      "Input sentence: (mm/d/dyy) (mm/ddyy) (mm/dd/yy)\n",
      "GT sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: unumL\n",
      "GT sentence: unum\n",
      "\n",
      "Decoded sentence: unum\n",
      "\n",
      "-\n",
      "Input sentence: Therapy: 21Jan0218  oRecorded\n",
      "GT sentence: Therapy: 21Jan2018 to Recorded\n",
      "\n",
      "Decoded sentence: Therapy: 21Jan2018 to Recorded\n",
      "\n",
      "-\n",
      "Input sentence: The enfeitsC enter\n",
      "GT sentence: The Benefits Center\n",
      "\n",
      "Decoded sentence: The Benefits Center\n",
      "\n",
      "-\n",
      "Input sentence: CityState Ztip\n",
      "GT sentence: City State Zip\n",
      "\n",
      "Decoded sentence: City State Zip\n",
      "\n",
      "-\n",
      "Input sentence: My Sponuse:\n",
      "GT sentence: My Spouse:\n",
      "\n",
      "Decoded sentence: My Spouse:\n",
      "\n",
      "-\n",
      "Input sentence: Participant Nam:e\n",
      "GT sentence: Participant Name:\n",
      "\n",
      "Decoded sentence: Participant Name:\n",
      "\n",
      "-\n",
      "Input sentence: sSpouse On & Off-Job Accw April 1, 2017\n",
      "GT sentence: Spouse On & Off-Job Acc April 1, 2017\n",
      "\n",
      "Decoded sentence: Spouse On & Off-Job Acc April 1, 2017\n",
      "\n",
      "-\n",
      "Input sentence: Email Address:\n",
      "GT sentence: Email Address:\n",
      "\n",
      "Decoded sentence: Email Address:\n",
      "\n",
      "-\n",
      "Input sentence: Pfrovider No:\n",
      "GT sentence: Provider No:\n",
      "\n",
      "Decoded sentence: Provider No:\n",
      "\n",
      "-\n",
      "Input sentence: * Larkibn, John J, MD - Primary\n",
      "GT sentence: * Larkin, John J, MD - Primary\n",
      "\n",
      "Decoded sentence: * Larkin, John J, MD - Primary\n",
      "\n",
      "-\n",
      "Input sentence: Weabsitce: myFirstmChoice.fhn.com\n",
      "GT sentence: Website: myFirstChoice.fchn.com\n",
      "\n",
      "Decoded sentence: Website: myFirstChoice.fchn.com\n",
      "\n",
      "-\n",
      "Input sentence: Full shift last day - no\n",
      "GT sentence: Full shift last day - no\n",
      "\n",
      "Decoded sentence: Full shift last day - no\n",
      "\n",
      "-\n",
      "Input sentence: Tiem of Accidnt\n",
      "GT sentence: Time of Accident:\n",
      "\n",
      "Decoded sentence: Time of Accident\n",
      "\n",
      "-\n",
      "Input sentence: SS #o:\n",
      "GT sentence: SS #:\n",
      "\n",
      "Decoded sentence: SS #:\n",
      "\n",
      "-\n",
      "Input sentence: Intensity: 5-6\n",
      "GT sentence: Intensity: 5-6\n",
      "\n",
      "Decoded sentence: Intensity: 5-6\n",
      "\n",
      "-\n",
      "Input sentence: TIER 2 Family MOO? Max\n",
      "GT sentence: TIER 2 Family MOOP Max\n",
      "\n",
      "Decoded sentence: TIER 2 Family MOOP Max\n",
      "\n",
      "-\n",
      "Input sentence: Are you related ot thi ptaient?es No\n",
      "GT sentence: Are you related to this patient? Yes No\n",
      "\n",
      "Decoded sentence: Are you related to this patient? No\n",
      "\n",
      "-\n",
      "Input sentence: CIRCLE ONE\n",
      "GT sentence: CIRCLE ONE\n",
      "\n",
      "Decoded sentence: CIRCLE ONE\n",
      "\n",
      "-\n",
      "Input sentence: Hospital Service\n",
      "GT sentence: Hospital Service Surgery\n",
      "\n",
      "Decoded sentence: Hospital Service\n",
      "\n",
      "-\n",
      "Input sentence: Employee Name\n",
      "GT sentence: Employee Name\n",
      "\n",
      "Decoded sentence: Employee Name\n",
      "\n",
      "-\n",
      "Input sentence: Mediwca Provider Information Phyisian\n",
      "GT sentence: Medical Provider Information Physician\n",
      "\n",
      "Decoded sentence: Medical Provider Information Physician\n",
      "\n",
      "-\n",
      "Input sentence: BSA Caklculated:2u.33\n",
      "GT sentence: BSA Calculated: 2.33\n",
      "\n",
      "Decoded sentence: BSA Calculed/2u.33\n",
      "\n",
      "-\n",
      "Input sentence: Hopita lCode: 2026 \n",
      "GT sentence: Hospital Code: 2026 \n",
      "\n",
      "Decoded sentence: Hospital Code: 2026 \n",
      "\n",
      "-\n",
      "Input sentence: Gruop Name:\n",
      "GT sentence: Group Name:\n",
      "\n",
      "Decoded sentence: Group Name:\n",
      "\n",
      "-\n",
      "Input sentence: fCHECK CARD UING FOR PAYMENT\n",
      "GT sentence: CHECK CARD USING FOR PAYMENT\n",
      "\n",
      "Decoded sentence: CHECK CARAME UNG FOR PAYMENT\n",
      "\n",
      "-\n",
      "Input sentence: no pfain a telnd flxion\n",
      "GT sentence: no pain at end flexion\n",
      "\n",
      "Decoded sentence: no pain at Pelnones \n",
      "\n",
      "-\n",
      "Input sentence: 01-02-81 74177 1 rCT ABD & PELV Wf/COTNRAST\n",
      "GT sentence: 01-02-18 74177 1 CT ABD & PELV W/CONTRAST\n",
      "\n",
      "Decoded sentence: 01-02-18 74177 - CT ABD & PELV W/CONTRAST\n",
      "\n",
      "-\n",
      "Input sentence: mployde\n",
      "GT sentence: Employed\n",
      "\n",
      "Decoded sentence: Employed\n",
      "\n",
      "-\n",
      "Input sentence: Type:\n",
      "GT sentence: Type:\n",
      "\n",
      "Decoded sentence: Type:\n",
      "\n",
      "-\n",
      "Input sentence: DEDUCTIBLE OUT OF POKET\n",
      "GT sentence: DEDUCTIBLE OUT OF POCKET\n",
      "\n",
      "Decoded sentence: DEDUCTIBLE OUT OF POCKET\n",
      "\n",
      "-\n",
      "Input sentence: • Illumigene MYCO on\n",
      "GT sentence: • Illumigene MYCO on\n",
      "\n",
      "Decoded sentence: • Illumigene MYCO on\n",
      "\n",
      "-\n",
      "Input sentence: EE Name:\n",
      "GT sentence: EE Name:\n",
      "\n",
      "Decoded sentence: EE Name:\n",
      "\n",
      "-\n",
      "Input sentence: ITRE 2 Indivikdual MOOP Ma\n",
      "\n",
      "GT sentence: TIER 2 Individual MOOP Max\n",
      "\n",
      "Decoded sentence: TITE 2 Individual MOOP Max\n",
      "\n",
      "-\n",
      "Input sentence: Customer #:\n",
      "GT sentence: Customer #:\n",
      "\n",
      "Decoded sentence: Customer #:\n",
      "\n",
      "-\n",
      "Input sentence: OUTOFPOCKET\n",
      "GT sentence: DEDUCTIBLE OUT OF POCKET\n",
      "\n",
      "Decoded sentence: SUOUR POCKET\n",
      "\n",
      "-\n",
      "Input sentence: Dgateof Next Viysit:\n",
      "GT sentence: Date of Next Visit:\n",
      "\n",
      "Decoded sentence: Date of Next Visit:\n",
      "\n",
      "-\n",
      "Input sentence: (mm/dd/yy) mmi/dd/yy) (mm/dd/yy)\n",
      "GT sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: ECONDARY INSU:R\n",
      "GT sentence: SECONDARY INSUR:\n",
      "\n",
      "Decoded sentence: ECONDARY INSUR:\n",
      "\n",
      "-\n",
      "Input sentence: Dat eofBirth:\n",
      "GT sentence: Date of Birth:\n",
      "\n",
      "Decoded sentence: Date of Birth:\n",
      "\n",
      "-\n",
      "Input sentence: Product Type: Leave Mgmtvc\n",
      "GT sentence: Product Type: Leave Mgmt Svc\n",
      "\n",
      "Decoded sentence: Product Type: Leave Mgntac\n",
      "\n",
      "-\n",
      "Input sentence: Medical Proxitler Information — Physician\n",
      "GT sentence: Medical Provider Information - Physician\n",
      "\n",
      "Decoded sentence: Medical Provider Information - Physician\n",
      "\n",
      "-\n",
      "Input sentence: Total Patient Responsibility\n",
      "GT sentence: Total Patient Responsibility\n",
      "\n",
      "Decoded sentence: Total Patient Responsibility\n",
      "\n",
      "-\n",
      "Input sentence: Physician’sTaax IDNumbeer:\n",
      "GT sentence: Physician’s Tax ID Number:\n",
      "\n",
      "Decoded sentence: Physician’s Tax ID Number:\n",
      "\n",
      "-\n",
      "Input sentence: RX Given:\n",
      "GT sentence: RX Given:\n",
      "\n",
      "Decoded sentence: RX Given:\n",
      "\n",
      "-\n",
      "Input sentence: Dae First Misesd Wfork:\n",
      "GT sentence: Date First Missed Work:\n",
      "\n",
      "Decoded sentence: Date First Misesd Work:\n",
      "\n",
      "-\n",
      "Input sentence: Policy No\n",
      "GT sentence: Policy No.\n",
      "\n",
      "Decoded sentence: Policy No.\n",
      "\n",
      "-\n",
      "Input sentence: (mjm/dd/yy) (mmdd/yy )(mvm/drd/yy)\n",
      "GT sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "Decoded sentence: (mm/dd/yy) (mm/dd/yy) (mm/dd/yy)\n",
      "\n",
      "-\n",
      "Input sentence: Now Due\n",
      "GT sentence: Now Due\n",
      "\n",
      "Decoded sentence: Now Due\n",
      "\n",
      "-\n",
      "Input sentence: PAYMENTS\n",
      "GT sentence: PAYMENTS\n",
      "\n",
      "Decoded sentence: PAYMENTS\n",
      "\n",
      "-\n",
      "Input sentence: Cusbtomer Seyrvie Informaiton\n",
      "GT sentence: Customer Service Information\n",
      "\n",
      "Decoded sentence: Customer Service Information\n",
      "\n",
      "-\n",
      "Input sentence: 02/06/18 PYPATN PatientP ayment\n",
      "GT sentence: 02/06/18 PYPATN Patient Payment\n",
      "\n",
      "Decoded sentence: 02/06/18 PYPATN Patient Payment\n",
      "\n",
      "-\n",
      "Input sentence: Pincer rgip\n",
      "GT sentence: Pincer grip\n",
      "\n",
      "Decoded sentence: Pincer rrip\n",
      "\n",
      "-\n",
      "Input sentence: iOtheru Medical Codiions (Probelm List)\n",
      "GT sentence: Other Medical Conditions (Problem List)\n",
      "\n",
      "Decoded sentence: Other Medical Conditions (Problem List)\n",
      "\n",
      "-\n",
      "Input sentence: Sister: Factor V, seizures\n",
      "GT sentence: Sister: Factor V, seizures\n",
      "\n",
      "Decoded sentence: Sister: Factor V, seizures\n",
      "\n",
      "-\n",
      "Input sentence: EDGEWOOD\n",
      "GT sentence: EDGEWOOD\n",
      "\n",
      "Decoded sentence: EDGEWOOD\n",
      "\n",
      "-\n",
      "Input sentence: Right knee pain\n",
      "GT sentence: Right knee pain\n",
      "\n",
      "Decoded sentence: Right knee pain\n",
      "\n",
      "-\n",
      "Input sentence: Total mAountu Birlled by\n",
      "GT sentence: Total Amount Billed by\n",
      "\n",
      "Decoded sentence: Total Amount Billed by\n",
      "\n",
      "-\n",
      "Input sentence: bTotal Empoyee Montly Pdayrollt Deduction:\n",
      "GT sentence: Total Employee Montly Payroll Deduction:\n",
      "\n",
      "Decoded sentence: Total Employee Montly Payroll Deduction:\n",
      "\n",
      "-\n",
      "Input sentence: oNse: Nose normal.\n",
      "GT sentence: Nose: Nose normal.\n",
      "\n",
      "Decoded sentence: Nose: Nose normal.\n",
      "\n",
      "-\n",
      "Input sentence: jCL-103-UTH w(06/3l)\n",
      "GT sentence: CL-1023-AUTH (06/13)\n",
      "\n",
      "Decoded sentence: CL-1023-AUTH (06/13)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: dmit Tpe: Selewct.. AmitD ante\n",
      "GT sentence: Admit Type: Select... Admit Date:\n",
      "\n",
      "Decoded sentence: Amployer Type: Select... Amit Date\n",
      "\n",
      "-\n",
      "Input sentence: PT/Ox\n",
      "GT sentence: PT/OT\n",
      "\n",
      "Decoded sentence: PTO/O\n",
      "\n",
      "-\n",
      "Input sentence: URC/Non-Par Ajdustment\n",
      "GT sentence: URC/Non-Par Adjustment\n",
      "\n",
      "Decoded sentence: URC/Non-Par Adjustment\n",
      "\n",
      "-\n",
      "Input sentence: ACCIDENT CLAIMF ORM\n",
      "GT sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "Decoded sentence: ACCIDENT CLAIM FORM\n",
      "\n",
      "-\n",
      "Input sentence: AWENDING PHYSICIAN STATEMENT (PLEASE. PRINT)\n",
      "GT sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE PRINT)\n",
      "\n",
      "Decoded sentence: ATTENDING PHYSICIAN STATEMENT (PLEASE, PRINT)\n",
      "\n",
      "-\n",
      "Input sentence: Tetenone Num r\n",
      "GT sentence: Telephone Number\n",
      "\n",
      "Decoded sentence: Telephone Number\n",
      "\n",
      "-\n",
      "Input sentence: RETURN THIS RTIONW ITH YOUR PAYMENT\n",
      "GT sentence: RETURN THIS PORTION WITH YOUR PAYMENT\n",
      "\n",
      "Decoded sentence: RETURN THIS RATION WITH YOUR PAYMENT\n",
      "\n",
      "-\n",
      "Input sentence: eRporyt Request ID 49612087\n",
      "GT sentence: Report Request ID: 49612087\n",
      "\n",
      "Decoded sentence: Report Request ID: 49612087\n",
      "\n",
      "-\n",
      "Input sentence: Insured Coverague Type Coverage Effecttive Dacte\n",
      "GT sentence: Insured Coverage Type Coverage Effective Date\n",
      "\n",
      "Decoded sentence: Insured Coverage Type Coverage Effective Date\n",
      "\n",
      "-\n",
      "Input sentence: INSURED/PATIENT STATMEENT( PLEAES PRINT)\n",
      "GT sentence: INSURED/PATIENT STATEMENT (PLEASE PRINT)\n",
      "\n",
      "Decoded sentence: INSURED/PATIENT STATEMENT (PLEASE PRINT)\n",
      "\n",
      "-\n",
      "Input sentence: PATIENT EXCUSE\n",
      "GT sentence: PATIENT EXCUSE\n",
      "\n",
      "Decoded sentence: PATIENT EXCUSE\n",
      "\n",
      "-\n",
      "Input sentence: SERVICE LIE #\n",
      "GT sentence: SERVICE LINE #\n",
      "\n",
      "Decoded sentence: SERVICE LICE #\n",
      "\n",
      "-\n",
      "Input sentence: UGARANTOR IDt\n",
      "GT sentence: GUARANTOR ID\n",
      "\n",
      "Decoded sentence: GUARANTOR ID\n",
      "\n",
      "-\n",
      "Input sentence: kMdeical rPovidner Specialty: Podaitrist\n",
      "GT sentence: Medical Provider Specialty: Podiatrist\n",
      "\n",
      "Decoded sentence: Medical Provider Specialty: Podicarist\n",
      "\n",
      "-\n",
      "Input sentence: Assessment\n",
      "GT sentence: Assessment\n",
      "\n",
      "Decoded sentence: Assessment\n",
      "\n",
      "-\n",
      "Input sentence: Business Telephoe:\n",
      "GT sentence: Business Telephone:\n",
      "\n",
      "Decoded sentence: Business Telephone:\n",
      "\n",
      "-\n",
      "Input sentence: DICTATING DTAE/TIME\n",
      "\n",
      "GT sentence: DICTATING DATE/TIME:\n",
      "\n",
      "Decoded sentence: DICTATING DATE/TIME:\n",
      "\n",
      "-\n",
      "Input sentence: C ity.\n",
      "GT sentence: City:\n",
      "\n",
      "Decoded sentence: City:\n",
      "\n",
      "-\n",
      "Input sentence: Totl MnothlyP rmium:\n",
      "GT sentence: Total Monthly Premium:\n",
      "\n",
      "Decoded sentence: Total Monthly Prim:\n",
      "\n",
      "-\n",
      "Input sentence: Assessment:\n",
      "GT sentence: Assessment:\n",
      "\n",
      "Decoded sentence: Assessment:\n",
      "\n",
      "-\n",
      "Input sentence: Procedure Description\n",
      "GT sentence: Procedure Description\n",
      "\n",
      "Decoded sentence: Procedure Description\n",
      "\n",
      "-\n",
      "Input sentence: YES/NO\n",
      "GT sentence: YES/NO\n",
      "\n",
      "Decoded sentence: YES/NO\n",
      "\n",
      "-\n",
      "Input sentence: EMERGjENCY MEDICAL ASSOCIATES\n",
      "GT sentence: EMERGENCY MEDICAL ASSOCIATES\n",
      "\n",
      "Decoded sentence: EMERGENCY MEDICAL ASSOCIATES\n",
      "\n",
      "-\n",
      "Input sentence: DAKOTA RADIOLOGY\n",
      "GT sentence: DAKOTA RADIOLOGY\n",
      "\n",
      "Decoded sentence: DAKOTA RADIOLOGY\n",
      "\n",
      "-\n",
      "Input sentence: Degree\n",
      "GT sentence: Degree\n",
      "\n",
      "Decoded sentence: Degree\n",
      "\n",
      "-\n",
      "Input sentence: Histy provided by: dad.\n",
      "GT sentence: History provided by: dad.\n",
      "\n",
      "Decoded sentence: History provided by: dad.\n",
      "\n",
      "-\n",
      "Input sentence: _ ‘ all Othfrtangdages Contact\n",
      "GT sentence: All Other Languages Contact\n",
      "\n",
      "Decoded sentence: All Other Ortanges Contact\n",
      "\n",
      "-\n",
      "Input sentence: Address 2:\n",
      "GT sentence: Address 2:\n",
      "\n",
      "Decoded sentence: Address 2:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    target_seq = np.argmax(decoder_target_data[seq_index: seq_index + 1], axis=-1)\n",
    "    #print(target_seq)\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4262358d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff425dbc588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Shotr Term Dsiability Policy #\n",
      "GT sentence: Short Term Disability Policy #\n",
      "\n",
      "Decoded sentence: Shorter Term Disability Policy #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_index = 2\n",
    "target_text = target_texts[seq_index][1:-1]\n",
    "text = input_texts[seq_index]\n",
    "decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "print('-')\n",
    "print('Input sentence:', text)\n",
    "print('GT sentence:', target_text)\n",
    "print('Decoded sentence:', decoded_sentence)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "#print('WER_spell_correction |TRAIN= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_index = 150\n",
    "target_text = test_target_texts[seq_index][1:-1]\n",
    "text = test_input_texts[seq_index]\n",
    " \n",
    "decoded_sentence = visualize_attention(text, encoder_model, decoder_model, max_encoder_seq_length, num_decoder_tokens, vocab_to_int, int_to_vocab)\n",
    "print('-')\n",
    "print('Input sentence:', text)\n",
    "print('GT sentence:', target_text)\n",
    "print('Decoded sentence:', decoded_sentence)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on separate tesseract corrected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "tess_correction_data = os.path.join(data_path, 'new_trained_data.txt')\n",
    "input_texts_OCR, target_texts_OCR, gt_OCR = load_data_with_gt(tess_correction_data, num_samples, max_sent_len, min_sent_len)\n",
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "# Sample output from train data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "for seq_index in range(len(input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    \n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain transfer from noisy spelling mistakes to OCR corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, encoder_model, decoder_model = build_model(latent_dim=latent_dim, num_encoder_tokens=num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train on noisy spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_noisy_OCR\n",
    "target_texts = target_texts_noisy_OCR\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.01\n",
    "\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_categorical_accuracy:.2f}.hdf5\"\n",
    "filepath=\"best_model_transfer.hdf5\" # Save only the best model for inference step, as saving the epoch and metric might confuse the inference function which model to use\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "#callbacks_list = [checkpoint, tbCallBack, lrate]\n",
    "\n",
    "#model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          #validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on OCR correction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_texts = input_texts_OCR\n",
    "target_texts = target_texts_OCR\n",
    "\n",
    "# Keep test data from the corrected OCR, as this what we care about\n",
    "input_texts, test_input_texts, target_texts, test_target_texts  = train_test_split(input_texts, target_texts, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# Vectorize train data\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize_data(input_texts=input_texts,\n",
    "                                                                             target_texts=target_texts, \n",
    "                                                                             max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                             num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                             vocab_to_int=vocab_to_int)\n",
    "# Vectorize test data\n",
    "test_encoder_input_data, test_decoder_input_data, test_decoder_target_data = vectorize_data(input_texts=test_input_texts,\n",
    "                                                                                            target_texts=test_target_texts, \n",
    "                                                                                            max_encoder_seq_length=max_encoder_seq_length, \n",
    "                                                                                            num_encoder_tokens=num_encoder_tokens, \n",
    "                                                                                            vocab_to_int=vocab_to_int)\n",
    "\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "lr = 0.001# Reduce the learning rate for fine tuning\n",
    "model.load_weights('best_model_transfer.hdf5')\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          validation_data = ([test_encoder_input_data, test_decoder_input_data], test_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          #validation_split=0.2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_transfer.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample output from test data\n",
    "decoded_sentences = []\n",
    "target_texts_ =  []\n",
    "\n",
    "for seq_index in range(len(test_input_texts)):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "\n",
    "    input_seq = test_encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, _ = decode_sequence(input_seq, encoder_model, decoder_model, num_decoder_tokens, int_to_vocab)\n",
    "    target_text = test_target_texts[seq_index][1:-1]\n",
    "    print('-')\n",
    "    print('Input sentence:', test_input_texts[seq_index])\n",
    "    print('GT sentence:', target_text)\n",
    "    print('Decoded sentence:', decoded_sentence)   \n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    target_texts_.append(target_text)\n",
    "    \n",
    "WER_spell_correction = calculate_WER(target_texts_, decoded_sentences)\n",
    "print('WER_spell_correction |TEST= ', WER_spell_correction)\n",
    "\n",
    "WER_OCR = calculate_WER(target_texts_, test_input_texts)\n",
    "print('WER_OCR |TEST= ', WER_OCR)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- Add attention\n",
    "- Full attention\n",
    "- Condition the Encoder on word embeddings of the context (Bi-directional LSTM)\n",
    "- Condition the Decoder on word embeddings of the context (Bi-directional LSTM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.107"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
